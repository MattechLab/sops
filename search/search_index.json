{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mat-TechLab's Standard Operating Procedures","text":"<p>Welcome to MattchLab's SOPs website. These SOPs refers to different protocols which data are collected simultaneously and that show very similar or overlapping procedures. These refers to several different protocols: </p> <ul> <li>visual checkerboard fMRI protocol</li> <li>audio fMRI protocol </li> <li>resting state audio-visual fMRI protocol. </li> </ul> <p>In the Data Collection section, the different protocols are extensively descripted. </p>"},{"location":"#summary","title":"Summary","text":"<p>Understanding how the brain's structure influences its distributed functions and processing dynamics holds great potential for revolutionizing neuroscience and clinical applications. Functional magnetic resonance imaging (fMRI) has proven to be a valuable, non-invasive tool for examining both the architecture and activity of the brain in vivo. Despite significant mathematical and computational advances in reconstructing resonance signals, most studies rely on the blood oxygen level-dependent (BOLD) signal using echo-planar imaging (EPI). This method employs Cartesian sampling and image reconstruction with a direct one-to-one correspondence between the number of acquired volumes and reconstructed images. However, EPI techniques face trade-offs between spatial and temporal resolutions. In this project, we aim to overcome these limitations by measuring the BOLD signal using a gradient recalled echo (GRE) with a 3D radial-spiral phyllotaxis trajectory at a high sampling rate. This approach allows the reconstruction of 3D signal time courses with whole-brain coverage, achieving simultaneously higher spatial (1 mm\u00b3) and temporal (up to 250 ms) resolutions compared to optimized EPI schemes. Additionally, artifacts are corrected before image reconstruction, and the desired temporal resolution can be selected post-scan without assumptions about the hemodynamic response shape. In total, each protocol involves MRI/fMRI acquisition from 20 healthy adult human subjects using three different 3.0 Tesla (T) MRI scanners available at CHUV.</p>"},{"location":"#impact","title":"Impact","text":"<p>Overall, this project will introduce a new framework for acquiring and analyzing fMRI data, addressing many of the current limitations. This method represents a significant advancement in functional BOLD imaging and has the potential to fundamentally redefine the research questions fMRI can address regarding cognitive processes. The project will publicly release valuable datasets that are essential for enhancing the acquisition and analysis of functional resonance data. In conclusion, this study is poised to be a turning point for MRI research in both neuroscientific and clinical applications.</p>"},{"location":"data-collection/","title":"Data collection","text":""},{"location":"data-collection/#example-1","title":"Example 1","text":"<p>The following is an example checklist for setting up a participant in the MRI to avoid motion-related or foreign object-related artifacts:</p> <p>BEFORE SCAN DATE</p> <ul> <li> Confirm that participant does not have any MRI contraindications</li> <li> Remind participant that any jewelry should be removed prior to the scan </li> <li> If allowed to wear street clothes, remind participant to avoid clothing with metal or that would uncomfortable to lie in for the duration of the scan</li> <li> If participant has indicated nervousness or history of claustrophobia, utilize mock scanner </li> </ul> <p>DAY OF SCAN, PRIOR TO PARTICIPANT ARRIVAL</p> <ul> <li> Prepare consent documents and MRI safety screener </li> <li> Prepare scrubs and MR-compatible glasses if applicable</li> <li> Setup scanner bed with proper headcoil, under-knee padding, neck padding, and any other padding necessary for safety and comfort </li> <li> Check stimulus display and response device </li> </ul> <p>SCAN TIME</p> <ul> <li> Have participant fill out consent documents and MRI safety screener, and verbally confirm responses, paying attention to frequently forgotten devices and implants, like orthodontia</li> <li> Have participant empty their pockets or change into scrubs, and remove all jewelry/hair accessories and check for any missed metallic objects with the scan center\u2019s preferred method</li> <li> Instruct participant on staying still and encourage them to request breaks if necessary </li> <li> Solicit feedback on participant\u2019s comfort while positioning them on the scanner bed and suggest ergonomic positioning of arms to avoid discomfort</li> <li> Follow the Scan console checklist.</li> </ul> <p>DURING SCAN</p> <ul> <li> Check in with participant frequently</li> <li> Watch for motion if you can see the participant, or use motion monitoring equipment</li> </ul> <p>AFTER SCAN</p> <ul> <li> Solicit more feedback on participant\u2019s comfort for future sessions</li> <li> Run MRIQC to evaluate data</li> </ul>"},{"location":"data-collection/#example-2","title":"Example 2","text":"<p>BEFORE THE SCAN DATE (1-2 days)</p> <ul> <li> Call or email the participant before the appointment to remind them of the details and times, and confirm that they can still make it</li> <li> Register their confirmation in STRATA or reschedule the appointment (see scheduling SOPs).</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/","title":"DEBI protocol","text":"Note for writers <ul> <li>Precisely point out what is connected to what by which kind of cables.</li> <li>Need double check with JB</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#overall-experimental-setting","title":"Overall experimental setting","text":"<p>The experimental setting is to obtain anatomical MRI scan with synchronized eye tracking recording (the right eye gaze trajectories and eye movement events including blinking, saccades and fixation).</p> <p>Here is the illustration of the overall experiment:</p> <p></p> <p>The graph above can be divided into the following components: - Syncbox: A NordicLabs Syncbox sends TTL (transistor-transistor logic) triggers to the scanner and forward the signal converted to the keyboard signal \"s\" to the PsychoPy laptop. </p> <p></p> <p>Trigger to PsychoPy PC</p> <p></p> <p>Trigger to Scanner</p> <ul> <li>Scanner: 3T clinical scanner (MAGNETOM PrismaFit, Siemens Healthineers) with a 64-channel head-neck coil with an attached mirror.</li> <li>Eye tracker: We use the EyeLink 1000 Plus (SR Research Ltd.) for eye tracking.   (i) The eye tracker consists of an infrared lens and camera sensor on one side, along with an infrared lamp to illuminate the subject's right eye. It is positioned inside the scanner bore.   (ii) An infrared mirror is mounted on the head coil.</li> <li>The Eye Tracker PC is bi-directionally connected to both the eye tracker and the PsychoPy laptop. It receives eye tracking data from the tracker, processes the eye movement trajectories and images, calculates pupil sizes by segmenting the pupil area, and classifies eye movement events based on predefined thresholds. The ET PC also receives trigger and task event messages from the PsychoPy laptop, logs data, and sends both the eye tracking data and logs back to the PsychoPy laptop.</li> <li>Stimuli laptop (PsychoPy Laptop): The PsychoPy laptop runs the PsychoPy software, which is used to execute the task programs. These programs coordinate various hardware components, including the ET PC, eye tracker, the screen, and syncbox. In our DEBI experiment, the 'MR-Eye' protocol displays a central dot that changes color at a frequency of 10Hz. In the 'MREye-Track' protocol, a gray dot sequentially appears at the top, bottom, left, and right sides of the screen. The laptop also stores the data and logs generated by the Eye Tracker PC after the experiment.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#session-preparation","title":"Session Preparation","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#configure-the-ip-address","title":"Configure the IP address","text":"<p>If you are connecting your PC to the eye tracker (ET) for the first time, you need to reconfigure the IP address. On windows: - Go to <code>Control Panel</code> -&gt; <code>Network and Internet</code> -&gt; <code>Network Connections</code> - Double-click on the <code>Ethernet</code> connection. - Select <code>Internet Protocol Version 4 (TCP/IPv4)</code> and click <code>Properties</code>. - Update the IP address to <code>100.1.1.2</code> and the subnet mask <code>255.255.255.0</code>. (Photos from Helene's sop)</p> <p> </p>"},{"location":"data-collection/data-collection_DEBI_protocol/#prepare-et-set-up-outside-the-scanner-room","title":"Prepare ET set up outside the scanner room","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#setting-up-the-projector","title":"Setting up the projector","text":"<p>Turn on the Sony projector located in the back room of the scanner room. </p> <p>Ensure the projector beam is directed into the scanner room.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#set-up-the-psychopy-laptop-on-the-table","title":"Set up the Psychopy laptop on the table","text":"<ul> <li> <p>Insert the hdmi into the Psychopy laptop to monitor the visual stimuli on the screen. The hdmi should be from the 3-cable bundle (which connects PC, projector and the Psychopy laptop). </p> </li> <li> <p>If the monitor does not automatically switch the screen source, use the button below to manually change it.  </p> </li> <li> <p>Ensure that the PC beneath the monitor remains turned on. </p> </li> <li> <p>Connect the USB cable from the Syncbox to the PsychoPy laptop.  </p> </li> <li> <p>Plug the Ethernet from the ET computer to the psychopy laptop. </p> </li> </ul> <p>Make sure the IP address is reset if the laptop is connected to the ET PC for the first time!</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#set-up-the-syncbox","title":"Set up the syncbox","text":"<ul> <li>Ensure the SyncBox cable (RJ45) already plugged onto the interface to the scanner room. </li> <li>Plug the other end of the cable into the SyncBox. </li> <li>Turn on the syncbox</li> <li>Go to the <code>Simulation</code>, and we can see the <code>Start Session</code> on the page.  </li> <li>Configure the TR time to 2500 ms according to our sequence. The TR determines the interval between two triggers. </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#set-up-the-et-workstation","title":"Set up the ET workstation","text":"<ul> <li>Make sure the ET PC is charged </li> <li>Before turning on the ET computer, ensure it is connected to the PsychoPy laptop (this should have been done in the previous step).</li> <li>Turn on the ET PC.</li> <li>Initialize the ET software from the ET work station here by typing \"elcl.exe\" in the terminal </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#prepare-the-et","title":"Prepare the ET","text":"<ul> <li>All the lenses, mirrors, and other equipment are in the box in JB's office. </li> <li>Install the 50mm lens onto the eye tracker (the compatible lens has a silver screw on it) (photos from Oscar's SOP)</li> </ul> <p> what value is the default position of the screw for convenient focus ?</p> <p>The default position of the screw for convenient focus is typically set to <code>1</code>. However, the exact value can vary depending on the specific participant and setup for each time. It's recommended to start with the screw at value <code>1</code> and adjust from there for optimal focus during setup. If you're following a specific SOP, it might provide additional details for your equipment.</p> <p></p>"},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-infrared-mirror-onto-the-head-coil","title":"Place the infrared-mirror onto the head coil","text":"<ul> <li>Detach the standard mirror's frame from the head coil, if it is placed there. </li> <li>Take the infrared mirror out of the \u00abfMRI\u00a0usage\u00bb box. It should be always protected by a mask unless in use.</li> </ul> <p>This infrared mirror is the most delicate part, because the morror cannot be replaced nor cleaned. This mirror is\u00a0EXTREMELY EXPENSIVE.</p> <p></p> <ul> <li>Get two gloves (e.g., from the box hanging at the entrance of the scanner room)</li> <li>Put the gloves on, and\u00a0DON'T TOUCH ANYTHING. You must have the standard mirror dismounted and ahead of this step.\u00a0</li> <li>WITH THE GLOVES\u00a0proceed to extract the infra-red mirror from its box, being extremely careful.\u00a0YOU CAN ONLY TOUCH THE MIRROR WITH GLOVES, because it cannot be cleaned up. Watch out for FINGERPRINTS\u00a0and once taken out of the protection mask,\u00a0IMMEDIATELY AND CAREFULLY ATTACH IT to the\u00a0head coil. [A photo of the head coil with the mirror correctly installed is missing]</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#preparation-in-the-scanner-room","title":"Preparation in the scanner room","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#connect-three-external-cables-to-et-and-scanner","title":"Connect three external cables to ET and scanner","text":"<p> - Two plugs for the black and one plug for the orange to ET   - Connect the external cable from the syncbox to the scanner. No photo here due to magnetic field</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-eye-tracker-and-screen","title":"Place the eye tracker and screen","text":"<ul> <li>Place the glass plate (stored in JB's office) on the scanner</li> <li>Position the ET on the glass plate according to the stickers on the plate.</li> <li>Place the half-circle one-direction screen, which is on the table behind the scanner, onto the glass plate. Position it between the projector and the eye tracker to reflect the projector's image. </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-participant","title":"Place the participant","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-subject-on-the-bed","title":"Place the subject on the bed","text":"<ul> <li>Apply a blanket, ear plugs and sand bags to the participant.</li> <li>Adjust the head coil and the mirror. If necessary, apply some pads to adjust the participant's head position to ensure the forehead tightly positioned against the head coil.</li> <li>Ensure the participant does not cross their legs.</li> <li>Provide the participant with the emergency button and explain that they can press it in case of an emergency.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#adjust-the-scanner-before-send-the-subject-inside","title":"Adjust the scanner before send the subject inside","text":"<ul> <li>Twist the knob to adjust the height of the bed and\u00a0 wait for it to stop.</li> <li>Gently move the participant with the manual controls. Stop when the head is under the head-localizer. Instruct the participant to close his eyes.</li> <li>Turn on the red light to localize the head. Put one hand on the head coil, then turn the knob left or right to align the red light with the mark on the head coil. Once aligned, turn off the red light and instruct the participant to open their eyes.</li> <li>Turn off the ventilation and set the scanner light to the minimum level.</li> <li>SSend the participant into the scanner, then proceed to the console at the back of scanner.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#adjust-the-lens-of-eye-tracker","title":"Adjust the lens of eye tracker","text":"<ul> <li>Point the lens of eye tracker towards the participant's right eye.</li> <li>[Add the default position of the lens for convenient adjustment]</li> <li>Rotate the lens until the pupil is in focus on the screen during camera mode. Adjust until the image is sharp, with both the pupil and eyelashes well defined.  ask for Mauro's consent</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#et-calibration","title":"ET Calibration","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#inform-the-participant","title":"Inform the participant","text":"<p>Inform the participant that you are leaving the room and will shortly come back for a final preparation.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#open-psychopy-in-the-psychopy-laptop","title":"Open psychopy in the psychopy laptop.","text":"<p>If you are connecting the eye tracker to the experimental laptop for the first time, you will need to configure it (see Section 0).</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#run-the-experiment-on-psychopy","title":"Run the experiment on psychopy","text":"<p>Click the <code>Run Experiment</code> button, or run the experiment from the terminal by typing: <code>python experiment.py</code></p> <p>Make sure that once the experiment start after the calibration, the data are being stored to the xx.EDF file. There should be a message about that displayed at the ET\u2019s PC screen. (According to Helene's, but we did not notice there was such message popping out before)</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#run-the-calibration","title":"Run the calibration","text":"<p>Once the stimulation begins, follow the messages on the screen to run the calibration, make sure the following options are selected correctly</p> <ul> <li>Calibration Type: <code>5 points calibration</code></li> <li>Sampling rate: <code>1k</code></li> <li>Tracking mode: <code>Pupil-CR</code></li> <li>Pupil Tracking: <code>Ellipse</code></li> <li>Camera Position: <code>Right</code>\"</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#apply-threshold-to-find-the-pupil","title":"Apply threshold to find the pupil","text":"<ul> <li>On the ET PC, click <code>Apply Threshold</code> (top left corner, as shown in the figure below). Ensure that the pupil is detected and that you see the blue cross on the eye. If you encounter issues, check the lighting inside the scanner (ensure it's not too bright or too dim) and verify the participant's position inside the coil. Once the calibration starts, accept the calibration points when they turn green by clicking <code>Accept Fixation</code>.</li> <li>If the calibration was successful, you will see the sentence <code>calibration successful</code> at the bottom in green. Check the stability of the accepted points and overall score of the calibration. </li> </ul> <p>If the calibration points form a cross, it is the perfect calibration.</p> <p></p>"},{"location":"data-collection/data-collection_DEBI_protocol/#follow-up-with-the-validation","title":"Follow up with the validation.","text":"<p>What you should see in an ideal situation is: the reference dot on the center of the screen and another dot that corresponds to the pupil calibration. The calibration dot is more or less stable moving a little around the reference dot.</p> <p>If the calibration dot is unstable and is moving around far from the reference dot, the experimenter should go back clicking the restart button, adjust the contrast and redo the calibration. Once the calibration dot is quite stable, proceed with the validation clicking the accept fixation.</p> <p></p>"},{"location":"data-collection/data-collection_DEBI_protocol/#go-into-the-scanner-room-and-inform-the-participant","title":"Go into the scanner room and inform the participant","text":"<p>Inform the participant that you are leaving the room and will now close the door to start. Let them also know that you are going to communicate with them very shortly to check that communications through the speaker are functioning.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#exit-the-scanning-room","title":"Exit the Scanning Room.","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#close-the-scanning-room-door","title":"Close the Scanning Room door.","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#running-the-scanning-session","title":"Running the scanning session","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#run-the-experiment","title":"Run the Experiment","text":"<ul> <li>At the end of the ET calibration we are ready to run the experiment.</li> <li>Wait for the sentence \u201cIn this task you will see a color dot. Please keep your eyes on the fixation point. The program is ready for the scanner trigger. Press s to proceed manually.\u201d</li> </ul> <p>Due to the upgrade of the scanner, it cannot immediately start the acquisition after receives the trigger signal from the syncbox. Thus, we need to extract the temporal information of the scanner and ET respectively. In order to make such post-processing easier, we need to first start the acquisition, and then the eye tracker. Thus, the first trigger recorded in the pmu in the raw data will be exactly the same trigger that starts the eye tracker.</p> <p>The order: start of scanner -&gt; press the button of syncbox is important and cannot be exchanged.</p> <ul> <li>Now two people need to get ready beside the syncbox and the scanner.</li> <li>One person first start the scanner acquisition.</li> <li>Then, another person press  <code>start session</code> on the sync box clicking the round button. </li> <li>The stimulation will start with the ET recording. </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#acquire-a-high-resolution-anatomical-image","title":"Acquire a high-resolution anatomical image","text":"<p>wip</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#acquire-t1w-libre-image-and-t1w-vibe-image","title":"Acquire T1w-LIBRE image and T1w-VIBE image","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#acquire-t2w-libre-image-and-t2w-tse-image","title":"Acquire T2w-LIBRE image and T2w-TSE image","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#session-completed","title":"Session Completed","text":"<ul> <li>At the end of the stimulation, click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> <li>The exam is over, inform the participant that the session has concluded.</li> <li>You can proceed with the tear-down protocol.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#session-tear-down","title":"Session Tear-Down","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#showing-the-participant-out","title":"Showing the Participant Out","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in a few seconds.</li> <li>Extract the participant by pressing the extraction button and then gently rolling the central knob. Alternatively, you can just press the Home button.  </li> <li>Remove the upper side of the head coil:</li> <li>Unplug the head coil from the bed connector.</li> <li>Lift the lever that releases the upper part of the coil and put it aside (e.g., inside the bore or on a chair next to the scanner).</li> <li>Assist the participant to remove the headphones.</li> <li>Help the participant sit down.</li> <li>Help the participant step down and accompany them out to the control room.</li> <li>Help the participant recover their personal belongings and change clothes if necessary.</li> <li>Give the participant the corresponding compensation for the participation and transportation.</li> <li>Ask the participant to sign the receipt of the amount of the financial compensation.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#et-setting","title":"ET setting","text":"<ul> <li>Place the half-circle screen back to the table behind the scanner.</li> <li>Unplug the two cables (signal and power) connected to the ET arm.</li> <li>Roll the two ET cables and put them in the cupboard inside the Scanning room.</li> <li>Remove the mirror frame from its rails mounted on the head coil and lay it on the bed.</li> <li>Put the gloves on and cover the infrared mirror with a mask for storage.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#clearing-up-the-scanner","title":"Clearing up the Scanner","text":"<ul> <li>Unplug the cable on the scanner from syncbox and roll it back to the shelf.</li> <li>Remove used blankets and bed-sheets ONE-BY-ONE: extend them to let any forgotten items fall on the floor before you fold it; and dispose of them in the adequate bin (soiled linen bag if they are fabric and trash if they are disposable).</li> <li>Dispose of all single-use sanitary protections (padding covers, earplugs, etc.).</li> <li>Put the pillows back in their designated storage places.</li> <li>Remove the head coil and put it in the scanner's bore.</li> <li>Remove the back padding elements and put them back in their designated storage.</li> <li>Reinstall the spine coil.</li> <li>Wipe the bed and the head coil (bottom and upper parts).</li> <li>Lock the head coil back with its bottom part without plugging the connectors.</li> <li>Put the head coil away with the other head-coils on the shelf next to the scanner.</li> <li>Return the bed to its Home position by pressing the button (more info).</li> <li>Take the ET arm, the infrared mirror and the plexiglass panel outside to the control room and store them in the ET/fMRI box.</li> <li>Exit and close the external door.</li> </ul> <p>Everything that is removed for the experiment needs to be put back in place at the end of the experiment, i.e., position of the bed, coil, emergency button, ears cover.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#collect-et-data","title":"Collect ET data","text":"<ul> <li>Copy data from the subfolder of PsychoPy program into the hard drive. !!! tip \"The subfolder should include the files for one session with the formats like  <code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.EDF</code> <code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.csv</code> <code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.log</code> where '**.EDF' files are eye tracking records, <code>.csv</code> and <code>.log</code> files logs the task messages and the corresponding timestamps.\"</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#collect-mri-raw-data","title":"Collect MRI raw data","text":"<p>Export Twix raw data - Twix: Username: medadmin, Password: adm\\(pwd\\)4\\(med\\). - Press <code>Ctrl+Esc</code> to open the IDE terminal. - In the IDE terminal: ideacmdtool -&gt; type <code>4</code> -&gt; type <code>6</code> // - Type <code>Twix</code> and the Twix data browser opens. - Select the data you want to copy, right click on the mouse -&gt; Copy Total RAID file -&gt; select destination (your hard drive). -  The only useful thing is the physio [select the flag on External Signal] need more explanation </p> <p>Export the DICOM data (directly reconstructed images from the scanner) - Login as SuperUser by pressing <code>Tab</code> + <code>Delete</code> + <code>(Bottone a DX - 9)</code> to enter the advance mode. - Export DICOM: select the patient, go to export // File System // Browse -&gt; select <code>HD</code>. - Select the \u201cEnhanced\u201d option (1 DICOM  / volume) instead of Interoperability (1 DICOM / slice).</p> <p>The default option is <code>Interoperability</code>! So we have to change it manually!</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#cleaning-up-the-control-room","title":"Cleaning up the Control Room","text":"<ul> <li>Plug back the SyncBox and the VGA projector where they were. Make sure you leave it connected exactly as you found it.</li> <li>Cover the eye tracker lens with the lid.</li> <li>Unscrew the 50mm lens from the eye tracker and place it back to the bag with the tag <code>50mm Lens for MRI use</code> on it. </li> <li>Make sure the infrared mirror covered with a mask and everything stored safely in the ET/MRI box.</li> <li>Store the ET/MRI box back to the office.</li> <li>Switch off ET PC. </li> <li>Switch off the projector.</li> </ul>"},{"location":"data-collection/data-collection_audio/","title":"Audio fMRI protocol","text":""},{"location":"data-collection/data-collection_audio/#data-collection-audio-fmri-protocol","title":"Data Collection audio fMRI protocol","text":""},{"location":"data-collection/data-collection_audio/#overall-experimental-setting","title":"Overall experimental setting","text":"<p>The experimental setup includes:</p> <ul> <li>Syncbox: A NordicLabs Syncbox receives TT (transistor-transistor logic) triggers from the scanner. This box can forward the triggers converted into other formats and/or manipulate them (e.g., filter, generate, etc.).</li> <li>Experimental laptop: It is the laptop where the Psychopy software is installed and with it the task programs are executed. This laptop also stores the data recorded by the ET at the end of the experiment.</li> </ul> <p>ONE DAY BEFORE SCAN DATE</p> <ul> <li>Verify that the Psychopy experiment runs correctly.</li> <li>Verify that all the tools necessary for the data collection run correctly.</li> <li>Print the informed consent form.</li> <li>Print the MRI safety screener (EN|FR).</li> <li>Print a receipt form for each participant that will get scanned.</li> </ul>"},{"location":"data-collection/data-collection_audio/#session-preparation","title":"Session preparation","text":"<p>The following section describes how to prepare the session on the day of the scan BEFORE the participant arrives. Try to arrive at the Control Room at least 30 min ahead of the session start time.</p>"},{"location":"data-collection/data-collection_audio/#setup-preparation-inside-the-scanner-room","title":"Setup Preparation Inside the Scanner room","text":"<ul> <li>Memorize where the other tools for the recordings are to put those back in place at the end (coil, emergency button, ears cover).</li> </ul>"},{"location":"data-collection/data-collection_audio/#setting-up-the-projector","title":"Setting up the projector","text":"<ul> <li>Before entering the scanner room, go to the room where the projector is installed.</li> <li>Switch the projector ON by hitting the power button located on its right side.</li> <li>Verify the aim of the projector's beam by looking through the tube into the Scanning Room.</li> <li>Verify the projection corresponds to the Psychopy laptop screen.</li> <li>Go to the scanner room and take the half-circle one-direction screen from the table behind the scanner and put it on the back of the scanner.</li> </ul>"},{"location":"data-collection/data-collection_audio/#setting-up-cables-and-headphones","title":"Setting up cables and headphones","text":"<ul> <li>Open the door of the cable section between the control room and the scanner room.</li> <li>Connect the scanner machine to the sharing system.</li> <li>Prepare the headphones to connect to the scanner machine. The type of headphone was choose based on the comfortability for the participant. Here are the two possible models:</li> </ul>"},{"location":"data-collection/data-collection_audio/#setting-up-the-coils","title":"Setting up the coils","text":"<ul> <li> <p>If any head coil from the last exam is still plugged, remove it:  <ul> <li>If it is the 64-channel coil, you can just temporarily move it into the scanner's bore.</li> <li>Otherwise, store it on the shelf where the other coils are and bring the 64-channel one in the proximity of the bed (e.g., inside the scanner's bore). Make sure to remove other coil's fitting elements.</li> </ul></p> </li> <li> <p>Remove the spine coil by lifting the corresponding latch, then sliding it toward the head of the bed, lift it from the bed and place it on the floor ensuring it is not obstructing any passage or unstable.</p> </li> <li>Place the two back padding elements filling the spine coil socket.</li> <li>Place the 64-channel head-and-neck coil into its socket at the head end of the bed.</li> </ul>"},{"location":"data-collection/data-collection_audio/#final-setting-inside-the-scanning-room","title":"Final setting inside the scanning room","text":"<ul> <li>Cover the MRI bed with a clean sheet.</li> <li> <p>Prepare padding: under-knee padding, neck-and-head padding, under-elbows padding, head-sides padding, top-head wedge padding.  <ul> <li>Wrap a sanitary cover around each padding.</li> <li>Place a double neck-and-head padding inside the coil, to ensure the eyes are close to the coil's windows.</li> </ul></p> </li> <li> <p>Prepare a blanket to cover the participant.</p> </li> </ul>"},{"location":"data-collection/data-collection_audio/#setup-preparation-inside-the-control-room","title":"Setup Preparation Inside the Control Room","text":""},{"location":"data-collection/data-collection_audio/#setting-up-experiment-instruments","title":"Setting up experiment instruments","text":"<ul> <li>Arrive to the Control Room at least 30 min ahead the session start time.</li> <li>Place the experimental laptop on the designed desk and connect all the as showed in the following picture (in this case Jack audio is not necessary). Specifically, connect the experimental laptop to:  <ul> <li>Plug the power adaptor to the laptop, and the adaptor to the power outlet on the wall.</li> <li>the screen switch box with the corresponding HDMI cable ( This should project your screen on the screen of CHUV's tower).</li> <li>the audio switch box with the corresponding jack cable ( This should share your audio on the audio of CHUV's tower).</li> <li>Connect the SyncBox to the laptop with the USB cable. It is normally plugged into CHUV's stimuli workstation, it must be re-plugged in there after the session.   </li> </ul> <ul> <li>Switch the laptop on and open the psychopy code.</li> <li>Click the switch button to share your PC.</li> <li>Switch the SyncBox on using the button on the right side.</li> <li>Change the SyncBox correctly to send the triggers (Corresponding to push the key-button \u201cS\u201d from keyboard). Take the SyncBox and go on \u201cSimulation\u201d mode.</li> <li>Then, change the parameters in the main menu modifying the pulse length at 100ms and the TR time at 650ms. </li> <li> <p>Push the enter button  and the syncbox will be now waiting for the scanner's trigger signal to forward it.  </p> </li> <li> <p>Open the door of the cable wardrobe between the recording room and the scanner room, and connect the sync box in the following way:</p> </li> </ul> <p> </p>"},{"location":"data-collection/data-collection_audio/#setup-preparation-at-the-technician-position","title":"Setup Preparation at the technician position","text":"<ul> <li>Changes the parameters for the audio system at the box and at the pc:</li> </ul> <ul> <li>Switch audio to the scanner room clicking \u201cinput\u201d button:</li> </ul>"},{"location":"data-collection/data-collection_audio/#participant-preparation","title":"Participant Preparation","text":""},{"location":"data-collection/data-collection_audio/#participant-reception","title":"Participant reception","text":"<ul> <li>Meet the participant at an easily locatable place (e.g., the reception desk of the Radiology Unit) and show them the way into the control room. Allow sufficient time before the experiment for the preparation.</li> <li>Show the participant the scanning room and explain to them how the device is controlled from outside.</li> <li>Ask to the participant to fill out the consent form and MRI safety screener, and verbally confirm responses and discuss further doubts, paying attention to frequently forgotten devices and implants, like orthodontia.</li> <li>Remind the participant to use the bathroom at this moment if they need.</li> <li>Describe the participant how the session will develop and explain clearly the task. Let them interrupt you to ask for clarifications and answer all the questions that may arise.</li> </ul> Script for the session <p>\u201cWe are going to acquire two types of images. The first type is anatomical imaging that we use to study the morphology of the brain. The second type is a functional MRI, which we use to understand how the brain activates as a response to stimuli we will present to you. During the whole duration of the exam, please do not create closed loops by crossing your legs or holding your hands together. It is possible that your peripheral nerves get stimulated at some points, so you will feel twitching of muscles, for instance, of your pectorals. Do not panic, it is okay, but if it feels too uncomfortable, please squeeze the alarm button.For the anatomical MRI we just ask you to relax and try to stay as still and comfortable as possible. Like a photographic camera, the largest problem making analyses difficult is motion. As opposed to a photo camera, the imaging of the brain happens very slow so there is a lot of opportunity for involuntary movements (e.g., when you blink or you take a deeper breath) or semi-voluntary (e.g., you need to swallow).   During the functional MRI you will hear different sounds in the headphones. The audio stimulation consists of short sounds of objects of everyday life (es for example, the sound of vacuum cleaner, the crying of a child, a guitar song, a bell rings, etc.) or animal noises. The experimental protocol consists of 15s audio stimulation using the above tracks followed by 25s of pauses in which we provided a pink noise. For the entire period of the experiment you should takes the eye open on a fixation point (a red cross) at the center of a gray environment. The experiment has a duration of 40 minutes more and less.</p> <p>*Is everything clear to you? Do you have any questions?\u201d</p> <ul> <li>Offer the participant a box to deposit everything they have in their pockets and all jewelry/hair accessories, and indicate the before to continue, we need to make sure we do not introduce any dangerous object in the magnet room. Therefore, it is important to inform the participant to remove every metallic accessory. </li> <li>Ask to remove shoes at the entrance of the scanning room.</li> </ul>"},{"location":"data-collection/data-collection_audio/#participant-preparation-in-the-scanning-room","title":"Participant preparation in the scanning room","text":"<ul> <li>Instruct the participant to lay on the MRI bed and adjust the participant inside.</li> <li>Put the headphones on the ears.</li> <li>Give to the participant the emergency button. Make the participant try it, so they can see it works. To switch off the alarm, there\u2019s a button on the scanner (circular, both on the left and on the right of the hole) </li> <li>Once the previous part is insured, the participant is ready. If the participant is cold, put a blanket on top of him. </li> <li>Connect the coil's cable to the corresponding socket on the table.</li> <li>Gently move the participant with the manual regulation. Stop when the head is under the head-localizer. Ask the participant to close the eyes, localize the head with the infrareds.</li> <li>Switch off the infrareds, now the participant can open the eyes. You can move the participant (always gently as before) inside the scanner, until the mm counter marks \u201cIsometric\u201d.</li> <li>If everything is ok, you can move forward and record.</li> </ul>"},{"location":"data-collection/data-collection_audio/#participant-preparation-in-the-scanning-room_1","title":"Participant preparation in the scanning room","text":"<ul> <li>Inform the participant that you are leaving the room and will now close the door to start. Let them also know that you are going to communicate with them very shortly to check that communications through the speaker are functioning.</li> <li>Exit the Scanning Room.</li> <li>Close the Scanning Room door.</li> </ul>"},{"location":"data-collection/data-collection_audio/#running-the-scanning-session","title":"Running the scanning session","text":"<p>You MUST know the security procedures in case of problem and keep yourself updated with changes</p>"},{"location":"data-collection/data-collection_audio/#before-initiating-the-session-run-the-experiment","title":"Before initiating the session: Run the experiment","text":"<ul> <li>Double click on the psychopy file of the audio protocol to open it.</li> <li>Run the experiment on psychopy clicking the \u201crun experiment\u201d button selected in the red square.  </li> <li>Wait for the sentence \u201cIn this task you will hear different sounds in the headphones. Please keep your eyes on the fixation point. The program is ready for the scanner trigger. Press s to proceed manually.\u201d</li> <li>Then click \u201cstart session\u201d on the sync box clicking the round button. </li> </ul> <ul> <li>The stimulation will start with the scanning. At the end of the experiment click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> </ul>"},{"location":"data-collection/data-collection_audio/#during-the-session","title":"During the session","text":"<ul> <li>Check in with the participant.</li> <li>Check in that everything is correctly working (sounds are played and heart from the participant, volume is loud enough, triggers correctly works, etc.)</li> </ul>"},{"location":"data-collection/data-collection_audio/#acquire-a-localizer","title":"Acquire a localizer","text":"<ul> <li>Indicate the participant that the scanning will soon start.</li> <li>Wait for the participant confirmation and set the speaker off afterward.</li> <li>Launch the AAhead_scout_64ch-head-coil protocol by pressing Continue.</li> <li>Once the localizer is concluded, click on the image stack icon with left click and drag the image with a 1 onto the image viewer. That will open the interpolated localizer on the viewer. </li> <li>If the quality looks good, check the box stating Localizer looked ok. If not, re-acquire the localizer. </li> </ul>"},{"location":"data-collection/data-collection_audio/#acquire-high-resolution-anatomical-image","title":"Acquire high-resolution anatomical image","text":"<ul> <li>Run the wip19_mprage_1iso_cs4p2 protocol by pressing *Continue.</li> </ul> While you are still running the MPRAGE sequence: <pre><code>    Open the parameters of the sequence and ensure that:\n\n    - under Sequence/Part1, the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.\n    - under Routine, TR and TE should be set at the minimum value. the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.\n    - under Contrast/common, TR and TE should be set at the minimum value. The fat-water should be set as standard and the flip angle should be set to 5 degree.\n    - under Contrast/filter, click on the three dots and tick the \u201cunfiltered images\u201d.\n    - under System/coils, select the coils HC3, HC5, HC4, HC6, HC7.\n    - under System/miscellaneous, Put the coil selection as manual.\n    - under Physio, make sure that RECONSTRUCTION is off.\n\n    At the end, click copy and go button\n</code></pre>"},{"location":"data-collection/data-collection_audio/#acquire-functional-image","title":"Acquire functional image","text":"<ul> <li>Inform the participant that we will start with the fMRI block, therefore the participant will start hearing sounds.</li> <li>Run the BEAT_LIBREoff_BOLD_audio_bis protocol by pressing *Continue.</li> </ul>"},{"location":"data-collection/data-collection_audio/#session-completed","title":"Session completed","text":"<ul> <li>The exam is over, inform the participant that the session has concluded.</li> <li>At the end of the stimulation click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> <li>You can proceed with the tear-down protocol.</li> </ul>"},{"location":"data-collection/data-collection_audio/#session-tear-down","title":"Session tear-down","text":""},{"location":"data-collection/data-collection_audio/#showing-the-participant-out","title":"Showing the participant out","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in few seconds.</li> <li> <p>Extract the participant by pressing the extraction button and then gently rolling the central knob. Alternatively, you can just press the Home button.  <li>Remove the upper side of the head coil:     <ul> <li>Unplug the head coil from the bed connector.</li> <li>Lift the lever that releases the upper part of the coil and put it aside (e.g., inside the bore or on a chair next to the scanner).</li> </ul> </li></p> </li> <li> <p>Assist the participant to the headphones.</p> </li> <li>Help the participant sit down.</li> <li>Help the participant step down and accompany them out to the control room.</li> <li>Help the participant recover their personal belongings and change clothes if necessary.</li> <li>Give the participant the corresponding compensation for the participation and transportation.</li> <li>Ask the participant to sign the receipt of the amount of the financial compensation.</li> </ul>"},{"location":"data-collection/data-collection_audio/#cleaning-up-the-scanning-room","title":"Cleaning up the Scanning Room","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in few seconds.</li> <li>Remove used blankets and bed-sheets ONE-BY-ONE: extend them to let any forgotten items fall on the floor before you fold it; and dispose of them in the adequate bin (soiled linen bag if they are fabric and trash if they are disposable).</li> <li>Dispose of all single-use sanitary protections (padding covers, earplugs, etc.).</li> <li>Put the pillows back in their designated storage places.</li> <li>Remove the head coil and put it in the scanner's bore.</li> <li>Remove the back padding elements and put them back in their designated storage.</li> <li>Reinstall the spine coil.</li> <li>Wipe the bed and the head coil (bottom and upper parts).</li> <li>Lock the head coil back with its bottom part without plugging the connectors.</li> <li>Put the head coil away with the other head-coils on the shelf next to the scanner.</li> <li>Return the bed to its Home position by pressing the  button (more info).</li> <li>Exit and close the external door.</li> </ul> <p>Everything that is removed for the experiment should be put back in place and the end of the experiment</p>"},{"location":"data-collection/data-collection_audio/#cleaning-up-the-control-room","title":"Cleaning up the Control room","text":"<ul> <li>Switch off laptop. Plug back the SyncBox and the VGA projector where they were. Make sure you leave it connected exactly as you found it.</li> <li>Switch off the projector.</li> </ul>"},{"location":"data-collection/data-collection_rs/","title":"Resting state fMRI protocol","text":"<p>Recruitment</p> <p>The recruitment section should clarify all aspects regarding how participants will be engaged within the study.</p> <ul> <li> Describe the location (e.g., university campus, hospital, community, city, zipcode area, etc.) of recruitment.</li> <li> Update this document describing the strategy for participant recruitment. Please beware that local ethical review boards may have some guidelines regarding the conditions that apply to recruitment tools and procedures. Some example strategies of recruitment are:<ul> <li>Consecutive ongoing recruitment through principal investigator in daily clinical practice.</li> <li>Participant recruitment through referring physician.</li> <li>Advertisement/flyer (in this case, add the corresponding files under <code>assets/</code> and link them from the body of this document).</li> </ul> </li> <li> Describe whether participants' information will be pulled from some existing database, or collected into a newly created database.   If a database or will be used, describe how to gain access to it and how to operate it.</li> <li> Describe the mechanisms to contact participants:<ul> <li>If contact if over telephone, write down a script for the call</li> <li>If contact is over e-mail or postal mail, write down the communication template</li> </ul> </li> </ul> <p>Screening</p> <p>Every new candidate participant must be screened to assess they meet all the inclusion criteria of your study, as well as do not meet any of the exclusion criteria.</p> <p>Your SOPs should clearly state:</p> <ul> <li> Refer to the inclusion and exclusion criteria of the study protocol, highlighting important aspects of the study (e.g., we are only investigating twins).</li> <li> Describe how the screening must be conducted.</li> <li> Prepare a checklist with all the elements that the candidate participant must be walked through if they accept to participate, for example:</li> <li> Confirm that participant does not have any contraindications<ul> <li> List of obvious implants and medical devices (e.g., peacemaker in an MRI experiment)</li> <li> List of less-obvious circumstances that may disqualify participation, such as non-removable orthodontia, or home-made tattoos.       Please note that minor implants (such as metal braces, retainers and some palatal expanders) that are sufficiently secured to the body may be safe on clinical MR scanners at 1.5T and 3T (tesla). However, these implants may originate artifacts on the generated images if they cause large-enough discontinuity of magnetic properties.</li> </ul> </li> <li> Confirm that participant will accept the participation conditions described in the informed consent form, for example:<ul> <li> If they will be required to abstain from eating, consuming coffee or smoking within a window of time prior to the exam.</li> <li> If their data will be openly shared after proper privacy protection measures.</li> <li> If they want to be informed about relevant incidental findings (and whether they can give up on their right to learn about them).</li> <li> Report life events that are relevant to the experiment and may be cause of stopping the participation for safety and health reasons (e.g., pregnancy) or other reasons (i.e., incompatibility with the study).</li> <li> Remind participant of any incompatibilities with the experiment that may apply (e.g., jewelry, removable orthodontics, some hairstyling products, etc.).</li> <li> If allowed to wear street clothes, remind participant to avoid clothing with metal or that would uncomfortable to lie in for the duration of the scan</li> <li> If participant has indicated nervousness or history of claustrophobia, or they belong to a population at risk (e.g., infants), anticipate the scheduling of a preparation session on a mock scanner prior to the first acquisition.</li> </ul> </li> </ul> <p>Scheduling</p> <p>Participants will likely need to visit your facilities for the development of the study in one or more occasions.</p> <ul> <li> State the number and nature of each of the sessions.</li> <li> Indicate how the scheduling should be conducted with each participant.</li> <li> Establish a system to store and share the information on how the research team will be made aware of the schedule and its changes</li> <li> Clearly explain how to book shared resources utilized by the study (e.g., MRI scanner).</li> </ul>"},{"location":"data-collection/data-collection_visuo/","title":"Visual checkerboard fMRI protocol","text":""},{"location":"data-collection/data-collection_visuo/#data-collection-visual-checkerboard-fmri-protocol","title":"Data Collection Visual checkerboard fMRI protocol","text":""},{"location":"data-collection/data-collection_visuo/#overall-experimental-setting","title":"Overall experimental setting","text":"<p>The experimental setup includes:</p> <ul> <li>Syncbox: A NordicLabs Syncbox receives TTL (transistor-transistor logic) triggers from the scanner. This box can forward the triggers converted into other formats and/or manipulate them (e.g., filter, generate, etc.).</li> <li> <p>Eye tracker (ET): We use the EyeLink 1000 Plus (SR Research Ltd., Ottawa, Canada). Our particular variant \u201cLong Range\u201d is composed of three main elements:   <ul> <li>inside the scanner's bore, we place an arm that holds an infrared lens and camera sensor on one side and an infrared lamp that illuminates the right eye of the subject through</li> <li>a special mirror to reflect the infrared spectrum;</li> <li>a PC tower that receives the camera recordings, post-processes the images, and calculates the final parameters of interest (position of the eye, pupil size, etc.).</li>        The ET is also connected to the experimental laptop.     </ul> </p> </li> <li> <p>Experimental laptop: It is the laptop where the Psychopy software is installed and with it the task programs are executed. This laptop also stores the data recorded by the ET at the end of the experiment.</p> </li> </ul> <p>ONE DAY BEFORE SCAN DATE</p> <ul> <li>Verify that all Psychopy experiments run correctly.</li> <li>Verify that all the tools necessary for the data collection run correctly \u2192 check that the eyelink machine is correctly working.</li> </ul> Configuration <p>If you connect the eye tracker to the experimental laptop for the first time, you need to configure it. - Go to the experimental laptop. - Double click on the ethernet connection inside the control panel.</p> <p></p> <ul> <li>Change the IP address \u2192 double click on the internet protocol version and change IP address with IP 100.1.1.2 and sub with 255.255.255.0.</li> </ul> <ul> <li>Print the informed consent form.</li> <li>Print the MRI safety screener (EN|FR).</li> <li>Print a receipt form for each participant that will get scanned.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#session-preparation","title":"Session preparation","text":"<p>The following section describes how to prepare the session on the day of the scan BEFORE the participant arrives. Try to arrive at the Control Room at least 30 min ahead of the session start time.</p>"},{"location":"data-collection/data-collection_visuo/#setup-preparation-inside-the-scanner-room","title":"Setup Preparation Inside the Scanner room","text":"<ul> <li>Memorize where the other tools for the recordings are to put those back in place at the end (coil, emergency button, ears cover).</li> </ul>"},{"location":"data-collection/data-collection_visuo/#setting-up-the-projector","title":"Setting up the projector","text":"<ul> <li>Before entering the scanner room, go to the room where the projector is installed.</li> <li>Switch the projector ON by hitting the power button located on its right side.</li> <li>Verify the aim of the projector's beam by looking through the tube into the Scanning Room.</li> <li>Verify the projection corresponds to the Psychopy laptop screen.</li> <li>Before exiting the projector room, grab the plexiglass panel where the ET arm will be placed inside the scanner.</li> <li>Take the panel to the Control Room.</li> <li>Go to the scanner room and take the half-circle one-direction screen from the table behind the scanner and put it on the back of the scanner.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#setting-up-the-eye-tracker","title":"Setting up the eye-tracker","text":"<ul> <li>Check out Eye Tracker set-up guidelin (ET_setup_3T_EyeLink_1000Plus).</li> <li>After accurately preparing the ET arm following the ET_setup_3T_EyeLink_1000Plus, bring the plexiglass panel inside the scanning room and place it at the end of the scanner's bore. A sign indicates the top side that MUST face up. The plastic feet must face down to avoid the panel from sliding. To ensure the repeatable positioning of the ET, place the end of the plexiglass such that its edge aligns with the edge of the illuminated MRI rails.</li> <li>Exit the Scanning Room and fetch the ET arm.</li> <li>Enter the Scanning Room and place the ET arm on top of the plexiglass panel with the two posterior feet of the ET arm aligned within the two corner markers made of scotch tape.</li> </ul> <p>Hold the ET arm FIRMLY because the magnetic field imposes some resistance</p> <ul> <li> <p>Unroll and connect the cables (two plugs for the black, one plug for the orange). </p> </li> <li> <p>Take the half-circle one-direction screen from the table behind the scanner and put it on the back of the scanner behind the ET system (don't push the plexiglass yet).</p> </li> </ul> <p></p>"},{"location":"data-collection/data-collection_visuo/#setting-up-the-coils","title":"Setting up the coils","text":"<ul> <li> <p>If any head coil from the last exam is still plugged, remove it:  <ul> <li>If it is the 64-channel coil, you can just temporarily move it into the scanner's bore.</li> <li>Otherwise, store it on the shelf where the other coils are and bring the 64-channel one in the proximity of the bed (e.g., inside the scanner's bore). Make sure to remove other coil's fitting elements.</li> </ul></p> </li> <li> <p>Remove the spine coil by lifting the corresponding latch, then sliding it toward the head of the bed, lift it from the bed and place it on the floor ensuring it is not obstructing any passage or unstable.</p> </li> <li>Place the two back padding elements filling the spine coil socket.</li> <li>Place the 64-channel head-and-neck coil into its socket at the head end of the bed.</li> <li>Attach the dedicated infrared mirror to the coil (see ET_setup_3T_EyeLink_1000Plus):  <ul> <li>Exit the Scanning Room.</li> <li>Fit in a pair of new latex gloves.</li> <li>Extract the dedicated infrared mirror from the ET box CAREFULLY.</li> <li>Remove the mirror protection EXTRA-CAREFULLY.</li> <li>Take the mirror, enter the Scanning Room and lock the mirror onto the frame of the head-coil.</li> </ul> </li> </ul>"},{"location":"data-collection/data-collection_visuo/#final-setting-inside-the-scanning-room","title":"Final setting inside the scanning room","text":"<ul> <li>Cover the MRI bed with a clean sheet.</li> <li> <p>Prepare padding: under-knee padding, neck-and-head padding, under-elbows padding, head-sides padding, top-head wedge padding.  <ul> <li>Wrap a sanitary cover around each padding.</li> <li>Place a double neck-and-head padding inside the coil, to ensure the eyes are close to the coil's windows.</li> </ul></p> </li> <li> <p>Prepare a blanket to cover the participant.</p> </li> <li>Prepare a new pair of earplugs.</li> <li>Completely disable the light inside of the scanner and the ventilation of the scanning room to facilitate the best performance of the ET.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#setup-preparation-inside-the-control-room","title":"Setup Preparation Inside the Control Room","text":""},{"location":"data-collection/data-collection_visuo/#setting-up-experiment-instruments","title":"Setting up experiment instruments","text":"<ul> <li>Arrive to the Control Room at least 30 min ahead the session start time.</li> <li>Place the experimental laptop on the designed desk and connect all the as showed in the following picture (in this case Jack audio is not necessary). Specifically, connect the experimental laptop to:  <ul> <li>Plug the power adaptor to the laptop, and the adaptor to the power outlet on the wall.</li> <li>The screen switch box with the corresponding HDMI cable (This should project your screen on the screen of CHUV's tower).</li> <li>The RJ-45/Ethernet cable from the ET computer into the RJ-45 socket of the experimental laptop.</li> <li>Connect the SyncBox to the laptop with the USB cable. It is normally plugged into CHUV's stimuli workstation, it must be re-plugged in there after the session.</li> </ul> </li> </ul> <ul> <li>Switch the laptop on and open the psychopy code.</li> <li>Click the switch button to share your PC.</li> <li>Switch the SyncBox on using the button on the right side.</li> <li>Change the SyncBox correctly to send the triggers (Corresponding to push the key-button \u201cS\u201d from keyboard). Take the SyncBox and go on \u201cSimulation\u201d mode.</li> <li>Then, change the parameters in the main menu modifying the pulse length at 100ms and the TR time at 650ms. </li> <li> <p>Push the enter button  and the syncbox will be now waiting for the scanner's trigger signal to forward it.  </p> </li> <li> <p>Open the door of the cable wardrobe between the recording room and the scanner room, and connect the sync box in the following way:</p> </li> </ul> <p> </p> <ul> <li>Switch on the ET\u2019s PC using the power-om button at the front</li> <li>Select \"Eyelink\" when given the option of which operating system to launch.  <li>Verify the IP address assigned to the Ethernet interface of the experimental laptop is correct:     <ul> <li>Check the output of the following command and verify that IP/mask is 100.1.1.2/24, and the protocol is IP version 4.</li> <li>Check whether the link is properly established.</li> </ul> </li> </li> </ul>"},{"location":"data-collection/data-collection_visuo/#when-the-participant-arrives","title":"When the Participant Arrives","text":""},{"location":"data-collection/data-collection_visuo/#participant-preparation","title":"Participant Preparation","text":""},{"location":"data-collection/data-collection_visuo/#participant-reception","title":"Participant Reception","text":"<ul> <li>Meet the participant at an easily locatable place (e.g., the reception desk of the Radiology Unit) and show them the way into the control room. Allow sufficient time before the experiment for the preparation.</li> <li>Show the participant the scanning room and explain to them how the device is controlled from outside.</li> <li>Ask the participant to fill out the consent form and MRI safety screener, and verbally confirm responses and discuss further doubts, paying attention to frequently forgotten devices and implants, like orthodontia.</li> <li>Remind the participant to use the bathroom at this moment if they need.</li> <li>Describe to the participant how the session will develop and explain clearly the task. Let them interrupt you to ask for clarifications and answer all the questions that may arise.</li> </ul> Script for the session <p>\u201cWe are going to acquire two types of images. The first type is anatomical imaging that we use to study the morphology of the brain. The second type is a functional MRI, which we use to understand how the brain activates as a response to stimuli we will present to you. During the whole duration of the exam, please do not create closed loops by crossing your legs or holding your hands together. It is possible that your peripheral nerves get stimulated at some points, so you will feel twitching of muscles, for instance, of your pectorals. Do not panic, it is okay, but if it feels too uncomfortable, please squeeze the alarm button.During the functional MRI you will see a point at the center of the screen that will change the colors. For the entire period of the experiment you should takes the eye open on this fixation point at the center of a gray environment. The experiment has a duration of 30 minutes more and less. Before to start with scanning we will need to calibrate the eyetracker, therefore we will ask you to fix different points on the screen.</p> <p>Is everything clear to you? Do you have any questions?\u201d</p> <ul> <li>Offer the participant a box to deposit everything they have in their pockets and all jewelry/hair accessories, and indicate that before continuing, we need to make sure we do not introduce any dangerous objects into the magnet room. Therefore, it is important to inform the participant to remove every metallic accessory.</li> <li>Ask to remove shoes at the entrance of the scanning room.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#participant-preparation-in-the-scanning-room","title":"Participant Preparation in the Scanning Room","text":"<ul> <li>Instruct the participant to lay on the MRI bed and adjust the participant inside. With the paddings, their head position MUST be adjusted and elevated so that the nose and the forehead of the participant are both close to the upper coil. This procedure ensures the ET has the clearest possible view of the eye.</li> <li>Give them the ear-plugs to protect their hearing during acquisition, allow time for them to place them.</li> <li>Give the participant the emergency button. Make the participant try it, so they can see it works. To switch off the alarm, there\u2019s a button on the scanner (circular, both on the left and on the right of the hole).</li> <li>Once the previous part is ensured, the participant is ready. If the participant is cold, put a blanket on top of him.</li> <li>Connect the coil's cable to the corresponding socket on the table.</li> <li>Gently move the participant with the manual regulation. Stop when the head is under the head-localizer. Ask the participant to close the eyes, localize the head with the infrareds.</li> <li>Switch off the infrareds, now the participant can open the eyes. You can move the participant (always gently as before) inside the scanner, until the mm counter marks \u201cIsometric\u201d.</li> <li>If everything is okay, you can move forward and record.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#final-preparations","title":"Final Preparations","text":"<ul> <li>Inform the participant that you are leaving the room and will shortly come back for a final preparation.</li> <li>Proceed with the ET positioning (see ET_setup_3T_EyeLink_1000Plus) and calibration.</li> </ul> ET Calibration <ul> <li>Open psychopy in the experimental laptop.If you connect the eye tracker to the experimental laptop for the first time, you need to configure it.</li> <li>Double click on the psychopy file of the rs protocol to open it.</li> <li>Run the experiment on psychopy clicking the \u201crun experiment\u201d button. IMPORTANT: make sure that once the experiment start after the calibration, the data are being stored to the xx.EDF file. There should be a message about that displayed at the ET\u2019s PC screen.</li> <li>Once the stimulation begins, you follow the messages on the screen to run the calibration (make sure the 5 points calibration has been selected).</li> <li>ET PC \u2013 apply threshold (left upper corner, as on the left figure), make sure that the pupil was found, and you see the blue cross on the eye. In case of troubles \u2013 check if there is enough light inside the scanner and not too much, check the position of the participant inside the coil. Once the calibration starts accept calibration points when green (Accept fixation).</li> </ul> <ul> <li>If the calibration was successful, you will see the sentence \u2018calibration successful\u2019 at the bottom in green. Check the stability of the accepted points and overall score o the calibration.</li> <li>Follow up with the validation. What you should see in an ideal situation is: the reference dot on the center of the screen and another dot, which correspond to the pupil calibration, that it is more and less stable moving a little around the reference dot. If the calibration dot is unstable and is moving around far from the reference dot the experimenter should go back clicking the restart button, adjust the contrast and redo the calibration. Once the calibration dot is quite stable proceed with the validation clicking the accept fixation. </li> </ul> <ul> <li>Inform the participant that you are leaving the room and will now close the door to start. Let them also know that you are going to communicate with them very shortly to check that communications through the speaker are functioning.</li> <li>Exit the Scanning Room.</li> <li>Close the Scanning Room door.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#running-the-scanning-session","title":"Running the Scanning Session","text":"<p>You MUST know the security procedures in case of problem and keep yourself updated with changes</p>"},{"location":"data-collection/data-collection_visuo/#before-initiating-the-session-run-the-experiment","title":"Before Initiating the Session: Run the Experiment","text":"<ul> <li>At the end of the ET calibration we are ready to run the experiment.</li> <li>Wait for the sentence \u201cIn this task you will see a color dot. Please keep your eyes on the fixation point. The program is ready for the scanner trigger. Press s to proceed manually.\u201d</li> <li>Then click \u201cstart session\u201d on the sync box clicking the round button. </li> </ul> <ul> <li>The stimulation will start with the scanning. At the end of the experiment click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#during-the-session","title":"During the Session","text":"<ul> <li>Check in with the participant.</li> <li>Check in that everything is correctly working (e.g., The pupil is correctly detected).</li> <li>Watch for motion and arousal state using the ET's camera. If you detect motion or the participant falls asleep at inadequate points, use the speaker to inform them.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#acquire-a-localizer","title":"Acquire a Localizer","text":"<ul> <li>Indicate the participant that the scanning will soon start.</li> <li>Wait for the participant confirmation and set the speaker off afterward.</li> <li>Launch the AAhead_scout_64ch-head-coil protocol by pressing Continue.</li> <li>Once the localizer is concluded, click on the image stack icon with left click and drag the image with a 1 onto the image viewer. That will open the interpolated localizer on the viewer.</li> <li>If the quality looks good, check the box stating Localizer looked ok. If not, re-acquire the localizer.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#acquire-a-high-resolution-anatomical-image","title":"Acquire a High-Resolution Anatomical Image","text":"<ul> <li>Run the wip19_mprage_1iso_cs4p2 protocol by pressing Continue.</li> </ul> Anatomical image Acquisition <ul> <li>While you are still running the MPRAGE sequence open the parameters of the sequence and ensure that:</li> <li>under Sequence \u2937 Part1, the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.</li> <li>under Routine, TR and TE should be set at the minimum value. the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.</li> <li>under Contrast \u2937 common, TR and TE should be set at the minimum value. The fat-water should be set as standard and the flip angle should be set to 5 degree.</li> <li>under Contrast \u2937 filter, click on the three dots and tick the \u201cunfiltered images\u201d.</li> <li>under System \u2937 coils, select the coils HC3, HC5, HC4, HC6, HC7.</li> <li>under System \u2937 miscellaneous, Put the coil selection as manual.</li> <li>under Physio, make sure that RECONSTRUCTION is off. </li> <li>Finally, click copy and go button</li> </ul>"},{"location":"data-collection/data-collection_visuo/#acquire-functional-image","title":"Acquire Functional Image","text":"<ul> <li>Inform the participant that we will start with the fMRI block, therefore the participant will start hearing sounds.</li> <li>Run the BEAT_LIBREoff_BOLD_audio_bis protocol by pressing Continue.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#session-completed","title":"Session Completed","text":"<ul> <li>The exam is over, inform the participant that the session has concluded.</li> <li>At the end of the stimulation click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> <li>You can proceed with the tear-down protocol.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#session-tear-down","title":"Session Tear-Down","text":""},{"location":"data-collection/data-collection_visuo/#showing-the-participant-out","title":"Showing the Participant Out","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in a few seconds.</li> <li> <p>Extract the participant by pressing the extraction button and then gently rolling the central knob. Alternatively, you can just press the Home button.  <li>Remove the upper side of the head coil:     <ul> <li>Unplug the head coil from the bed connector.</li> <li>Lift the lever that releases the upper part of the coil and put it aside (e.g., inside the bore or on a chair next to the scanner).</li> </ul> </li></p> </li> <li> <p>Assist the participant to the headphones.</p> </li> <li>Help the participant sit down.</li> <li>Help the participant step down and accompany them out to the control room.</li> <li>Help the participant recover their personal belongings and change clothes if necessary.</li> <li>Give the participant the corresponding compensation for the participation and transportation.</li> <li>Ask the participant to sign the receipt of the amount of the financial compensation.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#cleaning-up-the-scanning-room","title":"Cleaning up the Scanning Room","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in a few seconds.</li> <li>Remove used blankets and bed-sheets ONE-BY-ONE: extend them to let any forgotten items fall on the floor before you fold it; and dispose of them in the adequate bin (soiled linen bag if they are fabric and trash if they are disposable).</li> <li>Dispose of all single-use sanitary protections (padding covers, earplugs, etc.).</li> <li>Put the pillows back in their designated storage places.</li> <li>Remove the head coil and put it in the scanner's bore.</li> <li>Remove the back padding elements and put them back in their designated storage.</li> <li>Reinstall the spine coil.</li> <li>Wipe the bed and the head coil (bottom and upper parts).</li> <li>Lock the head coil back with its bottom part without plugging the connectors.</li> <li>Put the head coil away with the other head-coils on the shelf next to the scanner.</li> <li>Return the bed to its Home position by pressing the button (more info).</li> <li>Exit and close the external door.</li> </ul> <p>Everything that is removed for the experiment needs to be put back in place at the end of the experiment, i.e., position of the bed, coil, emergency button, ears cover.</p>"},{"location":"data-collection/data-collection_visuo/#cleaning-up-the-control-room","title":"Cleaning up the Control Room","text":"<ul> <li>Switch off the laptop and ET PC Tower. Plug back the SyncBox and the VGA projector where they were. Make sure you leave it connected exactly as you found it.</li> <li>Switch off the projector.</li> </ul>"},{"location":"data-management/0_overall-data-workflow/","title":"Overall data workflow","text":"<p>The data workflow can be divided into two parts: (figure is editing) https://docs.google.com/drawings/d/1J4RUVk647YXuAsq7B-6wf6nKiEy5rKnjl5h4MKooOrw/edit</p> <ul> <li>MRI data. The raw MRI data should be first stored on the HES-SO server. <ul> <li>We recommend to convert the raw MRI data to BIDS. Since from there, the BIDS compliant data can be quality controlled and pre-processed using the corresponding packages (e.g. MRIQC, fMRIPrep, dMRIPrep) to allow computation of analysis-grade derivatives (e.g. functional or structural connectivity). </li> <li> The code snippets for converting MRI raw data into BIDS</li> </ul> </li> <li> <p>Eye tracking data. The eye tracking data is automatically generated and store in the PsychoPy laptop. We should store the edf data in the folder of the MRI dataset.</p> </li> <li> <p>Twix physio data. If we need to extract the physio information from the raw data, we should also store the physio data in the subfolder <code>Twix</code> inside the dataset folder.</p> </li> <li> <p>Example folder structure should be followed as below:</p> </li> </ul> <pre><code>MR-Eye/\n    \u251c\u2500\u2500 code/\n    \u2502   \u251c\u2500\u2500 git_repo_specific_for_this_project/\n    \u2502   \u2502   \u251c\u2500\u2500 README.md  # Add a detailed description of the repository here\n    \u2502   \u2502   \u251c\u2500\u2500 [Your code files for this specific project go here]\n    \u2502   \u2502   \u2514\u2500\u2500 .gitignore  # Include entries to exclude commonly-used packages like Monalisa\n    \u2502\n    \u251c\u2500\u2500 data/\n    \u2502   \u251c\u2500\u2500 update_protocol_description.md\n    \u2502   \u251c\u2500\u2500 subject_table.xlsx\n    \u2502   \u251c\u2500\u2500 final_fixed_protocol.md\n    \u2502   \u251c\u2500\u2500 pilot_/\n    \u2502   \u2502   \u251c\u2500\u2500 year-month-date/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 subject00x/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 [Relevant pilot subject data files]\n    \u2502   \u2502   \u2514\u2500\u2500 [Additional subjects as needed]\n    \u2502   \u251c\u2500\u2500 final_protocol/\n    \u2502   \u2502   \u251c\u2500\u2500 subject001/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 mri/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dicom/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 raw_data/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ISMRMD/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 bids/  # (TBD: Decide on the structure)\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 physio/\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 others/\n    \u2502   \u2502   \u2514\u2500\u2500 [Additional subjects as needed]\n    \u2502\n    \u251c\u2500\u2500 output/\n    \u2502   \u2514\u2500\u2500 [Generated output files go here]\n    \u2502\n    \u251c\u2500\u2500 .gitignore  # Add patterns to exclude unnecessary files globally, like logs or temp files\n    \u2514\u2500\u2500 README.md  # Main project description with an overview, usage, and other details.\n</code></pre>"},{"location":"data-management/0_software_install/","title":"Software installation","text":""},{"location":"data-management/0_software_install/#1-eyelink-installation","title":"1 EyeLink Installation","text":"<ul> <li>The EyeLink MUST be installed before PyEDFRead and Pychopy.</li> <li>Register a new account for downloading the EyeLink Developers Kits. Link for installation.</li> <li>The activation of the new account might take up for 24 hours.</li> <li>Install the EyeLink Developer Kit and EyeLink Data Viewer following the instructions.   </li> </ul>"},{"location":"data-management/0_software_install/#2-additional-steps","title":"2 Additional Steps","text":""},{"location":"data-management/0_software_install/#21-for-windows-platform","title":"2.1 For Windows Platform","text":"<ul> <li>Ensure you have installed the Microsoft C++ Build Tools.</li> <li>You can download them from the Visual Studio website.</li> <li>Make sure to select the \"Desktop development with C++\" workload during the installation.</li> <li>Properly set up the environment variable:<ul> <li>For example on Windows system <pre><code>D:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.40.33807\\bin\\Hostx64\\x64\nD:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.40.33807\\include\n</code></pre></li> </ul> </li> <li>Modify the files in the directory of EyeLink for supporting Windows</li> <li>Edit edftypes.h in the dir: C:\\Program Files (x86)\\SR Research\\EyeLink\\Includes\\eyelink   </li> </ul>"},{"location":"data-management/0_software_install/#22-for-mac-platform","title":"2.2 For Mac Platform","text":"<p>Install the EyeLink Developer Kit and EyeLink Data Viewer following the instructions on the website.</p>"},{"location":"data-management/0_software_install/#23-for-linux-platform","title":"2.3 For Linux Platform","text":"<p>This part is derived from HCPH SOP. Since we did not work on eye tracking data with Linux system, we refer to the HCPH SOP for completeness.</p> <ul> <li>Enable Canonical's universe repository with the following command:   <pre><code>sudo add-apt-repository universe\nsudo apt update\n</code></pre></li> <li>Install and update the ca-certificates package:   <pre><code>sudo apt update\nsudo apt install ca-certificates\n</code></pre></li> <li>Add the SR Research Software Repository signing key:   <pre><code>curl -sS https://apt.sr-research.com/SRResearch_key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/sr-research.gpg\n</code></pre></li> <li>Add the SR Research Software Repository as an Aptitude source:     <pre><code>sudo apt install eyelink-display-software\n</code></pre></li> <li>Install the EyeLink Data Viewer:   <pre><code>sudo apt install eyelink-dataviewer\n</code></pre></li> </ul>"},{"location":"data-management/0_software_install/#3-virtual-environment-preparation","title":"3 Virtual Environment Preparation","text":"<ul> <li> <p>Prepare conda virtual env, with python version above 3.7. Some unexpected issues will occur if the python version=3.6 or below.     <pre><code>$ conda create -n edfenv python=3.8\n$ conda activate edfenv\n$ pip install cython\n$ pip install pandas\n$ pip install h5py\n</code></pre></p> </li> <li> <p>Install pyedfread from the original repo:   <pre><code>$ pip install git+https://github.com/s-ccs/pyedfread\n</code></pre></p> </li> </ul>"},{"location":"data-management/1_data_export/","title":"Data export","text":""},{"location":"data-management/1_data_export/#collect-et-data","title":"Collect ET data","text":"<p>Copy data from the subfolder of PsychoPy program into the hard drive.</p> <p>Tip</p> <p>The subfolder should include the files for one session with the formats like </p> <ul> <li><code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.EDF</code> </li> <li><code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.csv</code> </li> <li><code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.log</code></li> </ul> <p>where '**.EDF' files are eye tracking records, <code>.csv</code> and <code>.log</code> files logs the task messages and the corresponding timestamps.\"</p>"},{"location":"data-management/1_data_export/#collect-mri-raw-data","title":"Collect MRI raw data","text":"<p>Export Twix raw data - Twix: Username: medadmin, Password: adm\\(pwd\\)4\\(med\\). - Press <code>Ctrl+Esc</code> to open the IDE terminal. - In the IDE terminal: ideacmdtool -&gt; type <code>4</code> -&gt; type <code>6</code> // - Type <code>Twix</code> and the Twix data browser opens. - Select the data you want to copy, right click on the mouse -&gt; Copy Total RAID file -&gt; select destination (your hard drive). -  The only useful thing is the physio [select the flag on External Signal] need more explanation </p> <p>Export the DICOM data (directly reconstructed images from the scanner) - Login as SuperUser by pressing <code>Tab</code> + <code>Delete</code> + <code>(Bottone a DX - 9)</code> to enter the advance mode. - Export DICOM: select the patient, go to export // File System // Browse -&gt; select <code>HD</code>. - Select the \u201cEnhanced\u201d option (1 DICOM  / volume) instead of Interoperability (1 DICOM / slice).</p> <p>The default option is <code>Interoperability</code>! So we have to change it manually!</p>"},{"location":"data-management/2_et1_edf_to_bids/","title":"EDF conversion","text":"<p>Derived from edf_to_bids and convert.py Hsop: Converting eye-tracking into BIDS - Standard Operating Procedures of the HCPh project (axonlab.org) TheAxonLab/hcph-sops (github.com)</p> <p>EyeLink eye tracking system produces EDF recording files. In this step we need to first convert the raw edf files to BIDS format for enforcing a standardized structure, naming convention and metadata description. It makes researchers easier to understand and use data from different sources.</p>"},{"location":"data-management/2_et1_edf_to_bids/#0-package-preparation","title":"0 Package preparation","text":"<ul> <li> <p>To use pyEDFRead python package, we must properly install the open-source software EyeLink on our device/laptop in the very first step. You can find the instructions here.</p> </li> <li> <p>Import the environments</p> </li> </ul> <pre><code>from __future__ import annotations \nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom pyedfread import read_edf\nfrom collections import defaultdict\nfrom itertools import product, groupby\nfrom warnings import warn\nimport re\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#1-load-raw-et-data","title":"1 Load raw ET data","text":"<ul> <li>Use <code>pyedfread</code> package to open edf and </li> </ul> <p><pre><code>DATA_PATH = Path(\"/Path/to/EDF folder\")\nedf_name = f\"file1.EDF\"\n\nfile_path = str(DATA_PATH / edf_name)\nprint(file_path)\nori_recording, ori_events, ori_messages = read_edf(file_path)\n</code></pre> The edf file will generate Pandas dataframes:</p> <ul> <li><code>ori_recording</code>: The ET recordings with trajectory information, pupil area and other information.</li> <li><code>ori_events</code>: contains information of task events.</li> <li><code>ori_messages</code>: log messages including ET calibration, validation and user-defined task messages sent from Psychopy program to the device.</li> </ul> <pre><code>ori_messages = ori_messages.rename(\n    columns={\n        # Normalize weird header names generated by pyedfread\n        \"message\": \"trialid\",\n        \"trial\": \"trial\",\n        # Convert some BIDS columns\n        \"time\": \"timestamp\",\n    }\n)\n\nrecording = ori_recording\nmessages = ori_messages\nevents = ori_events\nprint(f'\\nThe entire info of `message`: \\n{messages}')\nrecording.columns\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#2-parsing-the-messages","title":"2 Parsing the messages","text":"<ul> <li>Drop the duplicated codes. <pre><code>messages = messages.rename(\n    columns={c: c.strip() for c in messages.columns.values}\n).drop_duplicates()\n</code></pre></li> <li>Check the information about calibration of ET data.</li> </ul> <pre><code># Extract calibration headers\n_cal_hdr = ori_messages.trialid.str.startswith(\"!CAL\")\ncalibration = ori_messages[_cal_hdr]\n# messages = messages.drop(messages.index[_cal_hdr])\nprint(calibration)\n</code></pre> <ul> <li> <p>Extracting the start time and stop time from metadata. If no information is extracted from metadata, we will keep them as <code>None</code>. <pre><code># Extracting the StartTime and StopTime metadata.\nmessage_first_trigger = 'MODE RECORD'\nmessage_last_trigger = 'end'\nmetadata = {\n    'StopTime': None,\n    'StartTime': None\n}\n\n# Find Start time\nstart_rows = messages.trialid.str.contains(\n    message_first_trigger, case=False, regex=True\n)\nstop_rows = messages.trialid.str.contains(\n    message_last_trigger, case=False, regex=True\n)\n\n\n# Extract calibration headers\n_cal_hdr = messages.trialid.str.startswith(\"!CAL\")\ncalibration = messages[_cal_hdr]\nmessages = messages.drop(messages.index[_cal_hdr])\n\n# Pick the LAST of the start messages\nmetadata[\"StartTime\"] = (\n    int(messages[start_rows].timestamp.values[-1])\n    if start_rows.any()\n    else None\n)\n\n# Pick the FIRST of the stop messages\nmetadata[\"StopTime\"] = (\n    int(messages[stop_rows].timestamp.values[0])\n    if stop_rows.any()\n    else None\n)\n\n# Drop start and stop messages from messages dataframe\nmessages = messages.loc[~start_rows &amp; ~stop_rows, :]\n</code></pre></p> </li> <li> <p>Extracting basic metadata <pre><code># Extracting basic metadata.\n# !MODE RECORD CR 1000 2 0 R\n\nmode_record = messages.trialid.str.startswith(\"!MODE RECORD\")\n\nmeta_record = {\n    \"freq\": DEFAULT_FREQUENCY,\n    \"mode\": DEFAULT_MODE,\n    \"eye\": DEFAULT_EYE,\n}\n\nif mode_record.any():\n    try:\n        meta_record = re.match(\n            r\"\\!MODE RECORD (?P&lt;mode&gt;\\w+) (?P&lt;freq&gt;\\d+) \\d \\d (?P&lt;eye&gt;[RL]+)\",\n            messages[mode_record].trialid.iloc[-1].strip(),\n        ).groupdict()\n\n        meta_record[\"eye\"] = EYE_CODE_MAP[meta_record[\"eye\"]]\n        meta_record[\"mode\"] = (\n            \"P-CR\" if meta_record[\"mode\"] == \"CR\" else meta_record[\"mode\"]\n        )\n    except AttributeError:\n        warn(\n            \"Error extracting !MODE RECORD message, \"\n            \"using default frequency, mode, and eye\"\n        )\n    finally:\n        messages = messages.loc[~mode_record]\n\neye = (\n    (\"right\", \"left\") if meta_record[\"eye\"] == \"both\" else (meta_record[\"eye\"],)\n)\n\nmetadata[\"SamplingFrequency\"] = int(meta_record[\"freq\"])\nmetadata[\"EyeTrackingMethod\"] = meta_record[\"mode\"]\nmetadata[\"RecordedEye\"] = meta_record[\"eye\"]\n</code></pre></p> </li> <li> <p>Extracting screen parameters <pre><code># Extracting screen parameters.\n# GAZE_COORDS 0.00 0.00 800.00 600.00\n\n# Extract GAZE_COORDS message signaling start of recording\ngaze_msg = messages.trialid.str.startswith(\"GAZE_COORDS\")\n\nmetadata[\"ScreenAOIDefinition\"] = [\n    \"square\",\n    DEFAULT_SCREEN,\n]\nif gaze_msg.any():\n    try:\n        gaze_record = re.match(\n            r\"GAZE_COORDS (\\d+\\.\\d+) (\\d+\\.\\d+) (\\d+\\.\\d+) (\\d+\\.\\d+)\",\n            messages[gaze_msg].trialid.iloc[-1].strip(),\n        ).groups()\n        metadata[\"ScreenAOIDefinition\"][1] = [\n            int(round(float(gaze_record[0]))),\n            int(round(float(gaze_record[2]))),\n            int(round(float(gaze_record[1]))),\n            int(round(float(gaze_record[3]))),\n        ]\n    except AttributeError:\n        warn(\"Error extracting GAZE_COORDS\")\n    finally:\n        messages = messages.loc[~gaze_msg]\n\nprint(metadata)\n</code></pre></p> </li> <li> <p>Extracting parameters of the pupil fit model.</p> </li> </ul> <pre><code># Extracting parameters of the pupil fit model.\n# ELCL_PROC ELLIPSE (5)\n# ELCL_EFIT_PARAMS 1.01 4.00  0.15 0.05  0.65 0.65  0.00 0.00 0.30\n# Extract ELCL_PROC AND ELCL_EFIT_PARAMS to extract pupil fit method\npupilfit_msg = messages.trialid.str.startswith(\"ELCL_PROC\")\n\nif pupilfit_msg.any():\n    try:\n        pupilfit_method = [\n            val\n            for val in messages[pupilfit_msg]\n            .trialid.iloc[-1]\n            .strip()\n            .split(\" \")[1:]\n            if val\n        ]\n        metadata[\"PupilFitMethod\"] = pupilfit_method[0].lower()\n        metadata[\"PupilFitMethodNumberOfParameters\"] = int(\n            pupilfit_method[1].strip(\"(\").strip(\")\")\n        )\n    except AttributeError:\n        warn(\"Error extracting ELCL_PROC (pupil fitting method)\")\n    finally:\n        messages = messages.loc[~pupilfit_msg]\n\npupilfit_msg_params = messages.trialid.str.startswith(\"ELCL_EFIT_PARAMS\")\nif pupilfit_msg_params.any():\n    rows = messages[pupilfit_msg_params]\n    row = rows.trialid.values[-1].strip().split(\" \")[1:]\n    try:\n        metadata[\"PupilFitParameters\"] = [\n            tuple(float(val) for val in vals)\n            for k, vals in groupby(row, key=bool)\n            if k\n        ]\n    except AttributeError:\n        warn(\"Error extracting ELCL_EFIT_PARAMS (pupil fitting parameters)\")\n    finally:\n        messages = messages.loc[~pupilfit_msg_params]     \n</code></pre> <ul> <li>Parsing validation messages</li> </ul> <pre><code># Calibration validation.\n# VALIDATE R 4POINT 4 RIGHT at 752,300 OFFSET 0.35 deg. -8.7,-3.8 pix.\n# Extract VALIDATE messages for a calibration validation\nvalidation_msg = messages.trialid.str.startswith(\"VALIDATE\")\n\nif validation_msg.any():\n    metadata[\"ValidationPosition\"] = []\n    metadata[\"ValidationErrors\"] = []\n\nfor i_row, validate_row in enumerate(messages[validation_msg].trialid.values):\n    prefix, suffix = validate_row.split(\"OFFSET\")\n    validation_eye = (\n        f\"eye{eye.index('right') + 1}\"\n        if \"RIGHT\" in prefix\n        else f\"eye{eye.index('left') + 1}\"\n    )\n    validation_coords = [\n        int(val.strip())\n        for val in prefix.rsplit(\"at\", 1)[-1].split(\",\")\n        if val.strip()\n    ]\n    metadata[\"ValidationPosition\"].append(\n        [validation_eye, validation_coords]\n    )\n\n    validate_values = [\n        float(val)\n        for val in re.match(\n            r\"(-?\\d+\\.\\d+) deg\\.\\s+(-?\\d+\\.\\d+),(-?\\d+\\.\\d+) pix\\.\",\n            suffix.strip(),\n        ).groups()\n    ]\n\n    metadata[\"ValidationErrors\"].append(\n        (validation_eye, validate_values[0], tuple(validate_values[1:]))\n    )\nmessages = messages.loc[~validation_msg]\n\nprint(messages)\nprint(metadata)\n</code></pre> <ul> <li>Extract final bits of metadata and THRESHOLDS messages prior to the recording. <pre><code>thresholds_msg = messages.trialid.str.startswith(\"THRESHOLDS\")\nif thresholds_msg.any():\n    metadata[\"PupilThreshold\"] = [None] * len(eye)\n    metadata[\"CornealReflectionThreshold\"] = [None] * len(eye)\n    thresholds_chunks = (\n        messages[thresholds_msg].trialid.iloc[-1].strip().split(\" \")[1:]\n    )\n    eye_index = eye.index(EYE_CODE_MAP[thresholds_chunks[0]])\n    metadata[\"PupilThreshold\"][eye_index] = int(thresholds_chunks[-2])\n    metadata[\"CornealReflectionThreshold\"][eye_index] = int(\n        thresholds_chunks[-1]\n    )\nmessages = messages.loc[~thresholds_msg]\nprint(messages)\nprint(metadata)\n</code></pre></li> <li>Consume the remaining messages <pre><code>if not messages.empty:\n    metadata[\"LoggedMessages\"] = [\n        (int(msg_timestamp), msg.strip())\n        for msg_timestamp, msg in messages[[\"timestamp\", \"trialid\"]].values\n    ]\n\nprint(messages)\nprint(metadata)\n</code></pre></li> </ul>"},{"location":"data-management/2_et1_edf_to_bids/#3-parsing-the-recording-dataframe","title":"3 Parsing the recording dataframe","text":"<pre><code>recording = ori_recording\n</code></pre> <ul> <li>Curation of the input dataframe</li> </ul> <pre><code># Normalize timestamps (should be int and strictly positive)\nrecording = recording.astype({\"time\": int})\nrecording = recording[recording[\"time\"] &gt; 0]\nraw_recording_len = len(recording)\nprint(f'raw_recording length: {raw_recording_len}')\n\nrecording = recording.rename(\n    columns={\n#         # Fix buggy header names generated by pyedfread\n#         \"fhxyvel\": \"fhxvel\",\n#         \"frxyvel\": \"frxvel\",\n        # Normalize weird header names generated by pyedfread\n        \"rx\": \"screen_ppdeg_x_coordinate\",\n        \"ry\": \"screen_ppdeg_y_coordinate\",\n        # Convert some BIDS columns\n        \"time\": \"timestamp\",\n    }\n)\n\n# Split extra columns from the dataframe\nextra = recording[[\"flags\", \"input\", \"htype\"]]\nrecording = recording.drop(columns=[\"flags\", \"input\", \"htype\"])\nprint(len(recording))\n\n# Remove columns that are always very close to zero\nrecording = recording.loc[:, (recording.abs() &gt; 1e-8).any(axis=0)]\n# Remove columns that are always 1e8 or more\nrecording = recording.loc[:, (recording.abs() &lt; 1e8).any(axis=0)]\n# Replace unreasonably high values with NaNs\nrecording = recording.replace({1e8: np.nan})\n\nassert len(recording) == raw_recording_len\n</code></pre> <ul> <li>Clean-up pupil size and gaze position</li> </ul> <pre><code># These are the parameters we most likely we care for, so special curation is applied:\nscreen_resolution = [800, 600]\n\nfor eyenum, eyename in enumerate(eye):\n    # Clean-up implausible values for pupil area (pa)\n    recording.loc[\n        recording[f\"pa_{eyename}\"] &lt; 1, f\"pa_{eyename}\"\n    ] = np.nan\n    recording = recording.rename(\n        columns={f\"pa_{eyename}\": f\"eye{eyenum + 1}_pupil_size\"}\n    )\n    print(f\"pa_{eyename} renamed as: eye{eyenum + 1}_pupil_size\")\n    # Clean-up implausible values for gaze x position\n    recording.loc[\n        (recording[f\"gx_{eyename}\"] &lt; 0)\n        | (recording[f\"gx_{eyename}\"] &gt; screen_resolution[0]),\n        f\"gx_{eyename}\",\n    ] = np.nan\n    # Clean-up implausible values for gaze y position\n    recording.loc[\n        (recording[f\"gy_{eyename}\"] &lt;= 0)\n        | (recording[f\"gy_{eyename}\"] &gt; screen_resolution[1]),\n        f\"gy_{eyename}\",\n    ] = np.nan\n\nprint(recording)\nassert len(recording) == raw_recording_len\n</code></pre> <ul> <li>Shape the columns to comply with BIDS format.</li> </ul> <pre><code># Munging columns to comply with BIDS. \n# At this point, the dataframe is almost ready for writing out as BIDS.\n# Interpolate BIDS column names\ncolumns = list(\n    set(recording.columns)\n    - set(\n        (\n            \"timestamp\",\n            \"screen_ppdeg_x_coordinate\",\n            \"screen_ppdeg_y_coordinate\",\n            \"eye1_pupil_size\",#pa\n            \"eye2_pupil_size\",#pa\n        )\n    )\n)\nbids_columns = []\nfor eyenum, eyename in enumerate(eye):\n    for name in columns:\n        colprefix = f\"eye{eyenum + 1}\" if name.endswith(f\"_{eyename}\") else \"\"\n        _newname = name.split(\"_\")[0]\n        _newname = re.sub(r\"([xy])$\", r\"_\\1_coordinate\", _newname)\n        _newname = re.sub(r\"([xy])vel$\", r\"_\\1_velocity\", _newname)\n        _newname = _newname.split(\"_\", 1)\n        _newname[0] = EDF2BIDS_COLUMNS[_newname[0]]\n        _newname.insert(0, colprefix)\n        bids_columns.append(\"_\".join((_n for _n in _newname if _n)))\n\n# Rename columns to be BIDS-compliant\nrecording = recording.rename(columns=dict(zip(columns, bids_columns)))\n\n# Reorder columns to render nicely (tracking first, pupil size after)\ncolumns = sorted(\n    set(recording.columns.values).intersection(BIDS_COLUMNS_ORDER),\n    key=lambda entry: BIDS_COLUMNS_ORDER.index(entry),\n)\ncolumns += [c for c in recording.columns.values if c not in columns]\nrecording = recording.reindex(columns=columns)\n\nprint(recording)\nassert len(recording) == raw_recording_len\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#4-parsing-the-calibration-messages","title":"4 Parsing the calibration messages","text":"<pre><code># Parse calibration metadata\nmetadata[\"CalibrationCount\"] = 0\nif not calibration.empty:\n    warn(\"Calibration of more than one eye is not implemented\")\n    calibration.trialid = calibration.trialid.str.replace(\"!CAL\", \"\")\n    calibration.trialid = calibration.trialid.str.strip()\n\n    metadata[\"CalibrationLog\"] = list(\n        zip(\n            calibration.timestamp.values.astype(int),\n            calibration.trialid.values,\n        )\n    )\n\n    calibrations_msg = calibration.trialid.str.startswith(\n        \"VALIDATION\"\n    ) &amp; calibration.trialid.str.contains(\"ERROR\")\n    metadata[\"CalibrationCount\"] = calibrations_msg.sum()\n\n    calibration_last = calibration.index[calibrations_msg][-1]\n    try:\n        meta_calib = re.match(\n            r\"VALIDATION (?P&lt;ctype&gt;[\\w\\d]+) (?P&lt;eyeid&gt;[RL]+) (?P&lt;eye&gt;RIGHT|LEFT) \"\n            r\"(?P&lt;result&gt;\\w+) ERROR (?P&lt;avg&gt;-?\\d+\\.\\d+) avg\\. (?P&lt;max&gt;-?\\d+\\.\\d+) max\\s+\"\n            r\"OFFSET (?P&lt;offsetdeg&gt;-?\\d+\\.\\d+) deg\\. \"\n            r\"(?P&lt;offsetxpix&gt;-?\\d+\\.\\d+),(?P&lt;offsetypix&gt;-?\\d+\\.\\d+) pix\\.\",\n            calibration.loc[calibration_last, \"trialid\"].strip(),\n        ).groupdict()\n\n        metadata[\"CalibrationType\"] = meta_calib[\"ctype\"]\n        metadata[\"AverageCalibrationError\"] = [float(meta_calib[\"avg\"])]\n        metadata[\"MaximalCalibrationError\"] = [float(meta_calib[\"max\"])]\n        metadata[\"CalibrationResultQuality\"] = [meta_calib[\"result\"]]\n        metadata[\"CalibrationResultOffset\"] = [\n            float(meta_calib[\"offsetdeg\"]),\n            (float(meta_calib[\"offsetxpix\"]), float(meta_calib[\"offsetypix\"])),\n        ]\n        metadata[\"CalibrationResultOffsetUnits\"] = [\"deg\", \"pixels\"]\n    except AttributeError:\n        warn(\"Calibration data found but unsuccessfully parsed for results\")\n\n\nprint(calibration)\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#5-parsing-the-events-dataframe","title":"5 Parsing the events dataframe","text":"<p>There are three types of eye movements:</p> <ul> <li>fixation </li> <li>saccade</li> <li>blinks</li> </ul> <p>The mask for each event is recorded, with the mask value indicating whether an event occurred at a specific timestamp. A mask value of 1 at a given timestamp signifies that the event was detected at that moment.</p> <pre><code># print(events)\nprint(recording)\n\n# Process events: first generate empty columns\nrecording[\"eye1_fixation\"] = 0\nrecording[\"eye1_saccade\"] = 0\nrecording[\"eye1_blink\"] = 0\n\n# Add fixations\nfor _, fixation_event in events[\n    events[\"type\"] == \"fixation\"\n].iterrows():\n    recording.loc[\n        (recording[\"timestamp\"] &gt;= fixation_event[\"start\"])\n        &amp; (recording[\"timestamp\"] &lt;= fixation_event[\"end\"]),\n        \"eye1_fixation\",\n    ] = 1\n\n# Add saccades, and blinks, which are a sub-event of saccades\nfor _, saccade_event in events[\n    events[\"type\"] == \"saccade\"\n].iterrows():\n    recording.loc[\n        (recording[\"timestamp\"] &gt;= saccade_event[\"start\"])\n        &amp; (recording[\"timestamp\"] &lt;= saccade_event[\"end\"]),\n        \"eye1_saccade\",\n    ] = 1\n\n    if saccade_event[\"contains_blink\"] == 1: #Note here some version is \"blink\", depends on the item name\n        recording.loc[\n            (recording[\"timestamp\"] &gt;= saccade_event[\"start\"])\n            &amp; (recording[\"timestamp\"] &lt;= saccade_event[\"end\"]),\n            \"eye1_blink\",\n        ] = 1\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#6-write-the-data-into-bids-structure","title":"6 Write the data into BIDS structure","text":"<p><pre><code>from copy import deepcopy\n\nmetadata['Columns'] = recording.columns.tolist()\nprint(metadata)\nsave_metadata = deepcopy(metadata)\n# metadata.pop('CalibrationLog', None)\n# print(metadata)\n</code></pre> We need to convert the 'CalibrationCount' into <code>int</code> type before the conversion.</p> <pre><code>def convert_to_int(metadata):\n    if 'CalibrationCount' in metadata:\n        metadata['CalibrationCount'] = int(metadata['CalibrationCount']) if isinstance(metadata['CalibrationCount'], (np.int32, np.int64, int)) else metadata['CalibrationCount']\n    if \"CalibrationLog\" in metadata:\n        metadata[\"CalibrationLog\"] = [(int(x[0]),x[1]) if isinstance(x[0], (np.int32, np.int64, int)) else x for x in metadata['CalibrationLog']]\n    return metadata\n\n\nconvert_metadata = convert_to_int(metadata)\n</code></pre> <p>Write the <code>dataframe</code> into <code>bids</code> <pre><code>out_dir = DATA_PATH\nedf_extension = 'EDF'\nedf_name = edf_name\nfilename = edf_name.split('.')[0]\nprint(f'bid filename: {filename}')\n\ndef write_bids_from_df(\n    recording, metadata,\n    out_dir,\n    filename,\n    # exp_run: str | Path,\n) -&gt; List[str]:\n    \"\"\"\n    Directly save the eye-tracking recording/metadata into a  BIDS structure.\n\n    Parameters\n    ----------\n    recording : dataframe\n        The recording data extracted from the EDF file.\n    metadata : dict\n        The metadata extracted from the EDF file.\n    out_dir : obj:`os.pathlike`\n        The path of EDF file. Refers to the folder (not the EDF file).\n    filename: str\n        The filename of the EDF file. The file name without the suffix, eg: \"Subject001\"\n\n    Returns\n    -------\n    List[str]\n        A list of generated files.\n\n    \"\"\"\n\n    out_json = out_dir / (filename + \".json\")\n    out_json.write_text(\n        json.dumps(metadata, sort_keys=True, indent=2)\n    )\n\n    # Write out data\n    out_tsvgz = out_dir / (filename + \".tsv.gz\")\n\n    recording.to_csv(\n        out_tsvgz,\n        sep=\"\\t\",\n        index=True,\n        header=True,\n        compression=\"gzip\",\n        na_rep=\"n/a\",\n    )\n\n    return str(out_tsvgz), str(out_json)\n\n\nwrite_bids_from_df(\n    recording, convert_metadata,\n    out_dir,\n    filename,\n)\n</code></pre></p> <p>Now the BIDS files are generated: EDF Path</p> <ul> <li>&lt;filename&gt;.json</li> <li>&lt;filename&gt;.tsv.gz</li> </ul>"},{"location":"data-management/3_data_storage/","title":"Data storage","text":"<p>To effectively manage and analyze MRI data, including prescan files, raw data, DICOM files, and associated physiological recordings and codes for analysis, we recommend organizing the collected dataset into the following structure:</p> <pre><code>MR-Eye/\n    \u251c\u2500\u2500 code/\n    \u2502   \u251c\u2500\u2500 git_repo_specific_for_this_project/\n    \u2502   \u2502   \u251c\u2500\u2500 README.md  # Add a detailed description of the repository here\n    \u2502   \u2502   \u251c\u2500\u2500 [Your code files for this specific project go here]\n    \u2502   \u2502   \u2514\u2500\u2500 .gitignore  # Include entries to exclude commonly-used packages like Monalisa\n    \u2502\n    \u251c\u2500\u2500 data/\n    \u2502   \u251c\u2500\u2500 update_protocol_description.md\n    \u2502   \u251c\u2500\u2500 final_fixed_protocol.md\n    \u2502   \u251c\u2500\u2500 subject_table.xlsx\n    \u2502   \u251c\u2500\u2500 pilot_/\n    \u2502   \u2502   \u251c\u2500\u2500 year-month-date/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 subject00x/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 [Relevant pilot subject data files]\n    \u2502   \u2502   \u2514\u2500\u2500 [More dated folders as needed]\n    \u2502   \u251c\u2500\u2500 final_protocol/\n    \u2502   \u2502   \u251c\u2500\u2500 subject001/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 mri/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dicom/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 raw_data/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ismrmd/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 bids/  # (TBD: Decide on the structure)\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 physio/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ET/\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **.edf\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **.tsv.gz\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 **.json\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 twix/**.mat \n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 others/\n    \u2502   \u2502   \u2514\u2500\u2500 [Additional subjects as needed]\n    \u2502\n    \u251c\u2500\u2500 output/\n    \u2502   \u2514\u2500\u2500 [Generated output files go here]\n    \u2502\n    \u2514\u2500\u2500 README.md  # Main project description with an overview, usage, and other details\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/","title":"MR-Eye 2.0","text":"<p>Derived from: https://www.axonlab.org/hcph-sops/data-management/eyetrack-qc/</p> <p>Author: Yiwei Jia</p> <p>This notebook focuses on the analysis of eye-tracking data within the MR-Eye 2.0 project, where visual stimuli are presented at the center of the screen. The analysis includes examining eye-tracking signals, identifying regions of interest (ROIs), and creating eye-tracking (ET) masks based on specific criteria\u2014in this case, selecting areas corresponding to eye rotation displacements within 1/3 voxel size. Furthermore, we investigate eye movement characteristics extracted from the ET data and compare them across four subjects to assess and standardize the data quality.</p> <p>For demonstration purposes, this notebook illustrates only the method for selecting the region but does not include functions for saving the masks to a local device. Readers are encouraged to implement their own save functions as needed.</p> <pre><code># Derived from:\n# https://www.axonlab.org/hcph-sops/data-management/eyetrack-qc\n# Load the autoreload extension\n%load_ext autoreload\n# Set autoreload to update the modules every time before executing a new line of code\n%autoreload 2\n\n%matplotlib inline\nfrom pathlib import Path\nimport json\nimport ppjson\nfrom importlib import reload  # For debugging purposes\n\nimport numpy as np\nimport pandas as pd\n\nimport eyetrackingrun as et\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport copy\nfrom write_bids_yiwei import EyeTrackingRun, write_bids, write_bids_from_df\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#step-1-inspecting-eye-tracking-data","title":"Step 1: Inspecting Eye-Tracking Data","text":"<p>In this step, we\u2019ll begin by parsing the metadata of our eye-tracking data. This will help us understand the structure of the data, including the recording parameters, subject details, and any relevant events or labels. Parsing the metadata provides insight into the conditions and settings during data collection, which is essential for accurate analysis.</p> <pre><code># Specify the subject and modality you want to inspect\n\nsubject_idx = 1\nT_idx = 1\n\nif T_idx == 1:\n    mode = 'T1'\nelse:\n    mode = 'T2'\n\n\nBIDS_PATH = Path(\"./data/\")  # file within a subdirectory\nFILE_NAME = f\"sub00{subject_idx}_T{T_idx}\"    \n\n# session = \"001\" #can be a string to be defined when required\ntsv_name = f\"{FILE_NAME}.tsv.gz\"\n\nrecording_file = BIDS_PATH / tsv_name\nprint(f'recording_file: {recording_file}')\n\nrecording = pd.read_csv(\n    recording_file,\n#     sep=r\"\\s+\",\n    sep=\"\\t\",\n    na_values=\"n/a\",\n)\n</code></pre> <pre><code>recording_file: data/sub001_T1.tsv.gz\n</code></pre> <p>This command shows the first few rows of our eye-tracking dataset,  helping us understand the organization, key columns, and the format of the data.  This step is useful to verify that the data loaded correctly and to identify any initial patterns or anomalies.</p> <pre><code>print(f'The length of the recording: {len(recording)}')\n</code></pre> <pre><code>The length of the recording: 754932\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#step-2-understanding-metadata","title":"Step 2: Understanding Metadata","text":"<p>The metadata provides crucial information about the setup and parameters used during eye-tracking data collection. By examining the metadata, we can gain insights into the recording conditions, data quality, and calibration settings.</p> <pre><code>metadata = json.loads((\n    recording_file.parent\n    / recording_file.name.replace(\".tsv.gz\", \".json\")\n).read_text())\n\nmetadata\n</code></pre> <pre><code>{'AverageCalibrationError': [0.5],\n 'CalibrationCount': 1,\n 'CalibrationLog': [[1047679,\n   '&gt;&gt;&gt;&gt;&gt;&gt;&gt; CALIBRATION (HV5,P-CR) FOR RIGHT: &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;'],\n  [1047679, 'Calibration points:'],\n  [1047680, '-10.3, -50.9         0,      0'],\n  [1047680, '-8.9, -71.9         0,  -2457'],\n  [1047680, '-9.3, -29.2         0,   2457'],\n  [1047680, '-41.2, -50.5     -3474,      0'],\n  [1047680, '24.2, -49.8      3474,      0'],\n  [1047680, 'eye check box: (L,R,T,B)\\n\\t  -48    31   -76   -25'],\n  [1047680, 'href cal range: (L,R,T,B)\\n\\t-5211  5211 -3686  3686'],\n  [1047680,\n   'Cal coeff:(X=a+bx+cy+dxx+eyy,Y=f+gx+goaly+ixx+jyy)\\n   5.882e-05  107.02  1.2434 -0.18295 -0.27969 \\n   4.4115e-05 -0.84952  115.02 -0.085003 -0.090966'],\n  [1047680, 'Prenormalize: offx, offy = -10.326 -50.929'],\n  [1047680, 'Gains: cx:102.441 lx:116.794 rx:88.586'],\n  [1047680, 'Gains: cy:109.181 ty:118.022 by:110.039'],\n  [1047680, 'Resolution (upd) at screen center: X=2.6, Y=2.4'],\n  [1047680, 'Gain Change Proportion: X: 0.318 Y: 0.073'],\n  [1047680, 'Gain Ratio (Gy/Gx) = 1.066'],\n  [1047680, 'Cross-Gain Ratios: X=0.011, Y=0.008'],\n  [1047680, 'PCR gain ratio(x,y) = 2.478, 2.261'],\n  [1047680, 'CR gain match(x,y) = 1.020, 1.020'],\n  [1047680, 'Slip rotation correction OFF'],\n  [1047682, 'CALIBRATION HV5 R RIGHT   GOOD'],\n  [1058027,\n   'VALIDATION HV5 R RIGHT GOOD ERROR 0.50 avg. 0.79 max  OFFSET 0.43 deg. -11.0,-3.5 pix.']],\n 'CalibrationResultOffset': [0.43, [-11.0, -3.5]],\n 'CalibrationResultOffsetUnits': ['deg', 'pixels'],\n 'CalibrationResultQuality': ['GOOD'],\n 'CalibrationType': 'HV5',\n 'Columns': ['eye1_x_coordinate',\n  'eye1_y_coordinate',\n  'eye1_pupil_size',\n  'eye1_pupil_x_coordinate',\n  'eye1_pupil_y_coordinate',\n  'eye1_href_x_coordinate',\n  'eye1_href_y_coordinate',\n  'fast_raw_x_velocity',\n  'screen_ppdeg_x_coordinate',\n  'screen_ppdeg_y_coordinate',\n  'timestamp',\n  'eye1_fixation',\n  'eye1_saccade',\n  'eye1_blink'],\n 'CornealReflectionThreshold': [179],\n 'EyeTrackingMethod': 'P-CR',\n 'LoggedMessages': [[1066471,\n   'NO Reply is disabled for function eyelink_cal_result'],\n  [1084633, 'RECCFG CR 1000 2 0 R'],\n  [1084633, 'ELCLCFG TOWER'],\n  [1084633, 'ELCL_PCR_PARAM 5 3.0']],\n 'MaximalCalibrationError': [0.79],\n 'PupilFitMethod': 'centroid',\n 'PupilFitMethodNumberOfParameters': 3,\n 'PupilThreshold': [68],\n 'RecordedEye': 'right',\n 'SamplingFrequency': 1000,\n 'ScreenAOIDefinition': ['square', [0, 800, 0, 600]],\n 'StartTime': None,\n 'StopTime': None,\n 'ValidationErrors': [['eye1', 0.45, [-12.1, 1.8]],\n  ['eye1', 0.21, [-5.6, 0.7]],\n  ['eye1', 0.41, [-11.1, 1.8]],\n  ['eye1', 0.5, [-12.7, -4.4]],\n  ['eye1', 0.79, [-10.8, -18.3]]],\n 'ValidationPosition': [['eye1', [400, 300]],\n  ['eye1', [400, 51]],\n  ['eye1', [400, 549]],\n  ['eye1', [48, 300]],\n  ['eye1', [752, 300]]]}\n</code></pre> <p>For instance: - RecordedEye: This specifies which eye was tracked. Here, the data corresponds to the right eye. - SamplingFrequency: The sampling rate is 1000 Hz (1 kHz), meaning the data captures 1000 samples per second. - CalibrationDetails: We can see the calibration method used (<code>HV5</code>), calibration quality (<code>GOOD</code>), and errors.</p> <p>Understanding these parameters helps us ensure data reliability and assess the quality of eye-tracking measurements before analysis.</p> <pre><code># Prepare the time axis based on the sampling frequency\n\nt_axis = (\n    recording.timestamp.values - recording.timestamp[0]\n) / metadata[\"SamplingFrequency\"]\nprint(f\"The end of the timestamp: {t_axis[-1]}\")\n</code></pre> <pre><code>The end of the timestamp: 754.931\n</code></pre> <pre><code># Change the naming to make it more readable\nrecording = recording.rename(\n        columns={\n            f\"eye1_pupil_size\": f\"pupil_size\",\n            f\"eye1_fixation\": f\"fixation\",\n            f\"eye1_saccade\": f\"saccade\",\n            f\"eye1_blink\": f\"blink\",\n            f\"eye1_x_coordinate\": f\"x_coordinate\", \n            f\"eye1_y_coordinate\": f\"y_coordinate\"         \n        }\n    )\n\nmetadata[\"Columns\"] = recording.columns.tolist()\n</code></pre> <pre><code>metadata[\"Columns\"]\n</code></pre> <pre><code>['Unnamed: 0',\n 'x_coordinate',\n 'y_coordinate',\n 'pupil_size',\n 'eye1_pupil_x_coordinate',\n 'eye1_pupil_y_coordinate',\n 'eye1_href_x_coordinate',\n 'eye1_href_y_coordinate',\n 'fast_raw_x_velocity',\n 'screen_ppdeg_x_coordinate',\n 'screen_ppdeg_y_coordinate',\n 'timestamp',\n 'fixation',\n 'saccade',\n 'blink']\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#step-3-inspecting-signals-event-masks-and-correlation","title":"Step 3: Inspecting Signals, Event Masks and Correlation","text":"<p>In this step we first visualize all the signals in the recordings to examine the overall shapes (trend, variability, or periodicity) and the scale of its values (range and units).</p> <p>Besides, eye-tracking data includes event masks, which label segments of the data corresponding to specific events such as blinks, saccades, or fixations. These masks are typically binary arrays or categorical labels indicating the presence of a specific event at each timestamp.</p> <pre><code>check_all = True\nif check_all:\n    fig,axes = plt.subplots(6, 1, figsize=(10, 36), sharex=False)\n\n     # Plot x coordinate\n    axes[0].plot(t_axis, recording[\"x_coordinate\"].values, label='X Coordinate', color='blue')\n    axes[0].set_title(\"X coordinate\")\n    axes[0].legend()\n    axes[0].set_xlabel(\"time [s]\")\n\n    # Plot y coordinate\n    axes[1].plot(t_axis, recording[\"y_coordinate\"].values, label='Y Coordinate', color='green')\n    axes[1].set_title(\"Y coordinate\")\n    axes[1].legend()\n    axes[1].set_xlabel(\"time [s]\")\n\n    # Plot pupil x coordinate\n    axes[2].plot(t_axis, recording[\"eye1_pupil_x_coordinate\"].values, label='Pupil X Coordinate', color='blue')\n    axes[2].set_title(\"Pupil X\")\n    axes[2].legend()\n    axes[2].set_xlabel(\"time [s]\")\n\n    # Plot pupil y coordinate\n    axes[3].plot(t_axis, recording[\"eye1_pupil_y_coordinate\"].values, label='Pupil Y Coordinate', color='green')\n    axes[3].set_title(\"Pupil Y\")\n    axes[3].legend()\n    axes[3].set_xlabel(\"time [s]\")\n\n    # Plot href x coordinate\n    axes[4].plot(t_axis, recording[\"eye1_href_x_coordinate\"].values, label='Href X Coordinate', color='orange')\n    axes[4].set_title(\"Href X\")\n    axes[4].legend()\n    axes[4].set_xlabel(\"time [s]\")\n\n    # Plot screen ppdeg x coordinate\n    axes[5].plot(t_axis, recording[\"screen_ppdeg_x_coordinate\"].values, label='Screen PPdeg X Coordinate', color='red')\n    axes[5].set_title(\"Screen X (ppdeg)\")\n    axes[5].legend()\n    axes[5].set_xlabel(\"time [s]\")\n\n    plt.ylabel(\" \");\n</code></pre> <p></p> <p>Here we check the pupil area</p> <pre><code>check_pupil = True\nif check_pupil:\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"pupil_size\"].values,\n    )\n\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"pupil area [a.u.]\");\n\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"pupil_size\"].values,\n    )\n    plt.title(\"Zoom In - Pupil Area\")\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"pupil area [a.u.]\")\n    # JB1 (100, 150)\n    plt.xlim((100, 150))\n</code></pre> <p></p> <p></p> <p>Check the eye events:</p> <ul> <li> <p>Blink</p> </li> <li> <p>Saccade</p> </li> <li> <p>Fixation</p> </li> </ul> <p>We want to first check the masks of each event and want to inspect the correlation with the signals we checked above.</p> <pre><code># Check blinking\ncheck_blink = True\nif check_blink:\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"blink\"].values,\n    )\n    plt.title(\"Blink Event\")\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"eyes closed\");\n\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"blink\"].values * 10000,\n    )\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"eyes closed\")\n    plt.title(\"Zoom in - Blink Event\")\n    plt.xlim((100, 150))\n</code></pre> <p></p> <p></p> <pre><code>plot_pupil_blink = True\nif plot_pupil_blink:\n    fig = plt.figure(figsize=(16, 2))\n\n    plt.plot(\n        t_axis,\n        recording[\"pupil_size\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"blink\"].values * 5000,\n    )\n    # screen_coor = recording[\"screen_ppdeg_x_coordinate\"].values\n    # plt.plot(\n    #     t_axis,\n    #     (screen_coor-np.min(screen_coor))/(np.max(screen_coor)-np.min(screen_coor)) * 7000\n    # )\n\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"pupil area [a.u.]\")\n    plt.xlim((260, 290))\n    plt.title('Pupil area vs. blinking')\n</code></pre> <p></p> <p>Check saccading</p> <pre><code>Check_saccading=True\nif Check_saccading:\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"saccade\"].values,\n    )\n    plt.title(\"Saccade Event\")\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"eyes closed\");\n\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"saccade\"].values * 10000,\n    )\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"eyes closed\")\n    plt.title(\"Zoom in - Saccade Event\")\n    plt.xlim((100, 150))\n</code></pre> <p></p> <p></p> <pre><code>plot_pupil_saccade = True\n\nif plot_pupil_saccade:\n    fig = plt.figure(figsize=(16, 2))\n\n    plt.plot(\n        t_axis,\n        recording[\"x_coordinate\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"saccade\"].values * 500,\n    )\n\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"x coord [pixel]\")\n    plt.xlim((206, 210))\n    plt.ylim((300, 600))\n    plt.title('x coordinates vs. saccading')\n</code></pre> <p></p> <p>Check fixation</p> <pre><code>plot_fixation = True\nxlim = (206, 210)\nylim = (200, 600)\n\nif plot_fixation:\n    # Fixation\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"x_coordinate\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"fixation\"].values * 500,\n    )\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.title('X coordinate of gazing with fixation')\n\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"y_coordinate\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"fixation\"].values * 500,\n    )\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.title('Y coordinate of gazing with fixation')\n</code></pre> <p></p> <p></p> <p>Did you find the correlation between x/y coordinates, pupil area and saccade or fixation events?</p>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#step-4-data-cleaning","title":"Step 4: Data Cleaning","text":"<ul> <li> <p>Extract the signals we need </p> </li> <li> <p>Eliminate the blinking and non-fixation area of ET data.</p> </li> <li>Design some criteria for preserving the useful data<ul> <li>Visual angle</li> <li>Heat maps</li> <li>Fixation algorithm (literature review)</li> </ul> </li> </ul>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#41-extract-the-signals-we-need","title":"4.1: Extract the signals we need","text":"<p>In this exercise, we are focusing on a subset of the eye-tracking (ET) data. Specifically, we will process the first portion of the ET data, which corresponds to the duration of the sequence <code>T1_LIBRE</code>.</p> <p>We will extract the first portion of the data with a length matching <code>T1_LIBRE</code>. Inspect the extracted subset to understand its structure and characteristics, such as gaze coordinates and event markers.</p> <pre><code># Here is the given information: time duration (in sec) according to the prior information.\n\nif  subject_idx == 1:\n    T1_LIBRE = 374.558\nelif subject_idx == 2:\n    T1_LIBRE = 374.380\nelif subject_idx == 3:\n    T1_LIBRE = 334.237\nelse:\n    T1_LIBRE = 374.565\n\n\n\nprint(f\"The length of T1_LIBRE Subject{subject_idx} should be: {T1_LIBRE}\")\n</code></pre> <pre><code>The length of T1_LIBRE Subject1 should be: 374.558\n</code></pre> <pre><code># Size of screen\nsize = (\n    metadata[\"ScreenAOIDefinition\"][1][1],\n    metadata[\"ScreenAOIDefinition\"][1][3],\n)\n\n# For easier processing the entire recording and only the coordinate\n# We stored the (x,y) coordinate in the coor_data \n# And stored the entire recording in the coor_recording\ncoor_data = recording[[\"x_coordinate\", \"y_coordinate\"]]\nprint(f\"Extract X Y coordinates from recording: {len(coor_data)}\")\n\ncoor_recording = recording\n\n\ncoor_data.head()\ncoor_recording.head()\n</code></pre> <pre><code>Extract X Y coordinates from recording: 754932\n</code></pre> <pre><code># start_margin determines where to begin extracting the useful portion of the eye-tracking (ET) data.\n# In our case, it is set as 0.\n\nstart_margin = int(0.0*metadata['SamplingFrequency']) \n\nprint(f'mode: {mode}')\nprint(f\"set start margin as {start_margin}\")\n\n\nT1_LIBRE_sample = T1_LIBRE*metadata['SamplingFrequency']\n\n\n\nt_axis_xy = (\n    coor_data.index \n) / metadata[\"SamplingFrequency\"]\n\n# x coordinate\nfig = plt.figure(figsize=(16, 2))\n\nplt.plot(\n    t_axis_xy,\n    coor_data[\"x_coordinate\"].values,\n)\n\n\nplt.axvline(x=start_margin/metadata[\"SamplingFrequency\"], color='r', linestyle='--', label='LIBRE starts')\nplt.axvline(x=(T1_LIBRE_sample+start_margin)/metadata[\"SamplingFrequency\"], color='b', linestyle='--', label='LIBRE ends')\n\n\nplt.title(\"x_coordinate along time\")\nplt.xlabel(\"time [s]\")\nplt.ylabel(\"x coordinate [px]\")\nplt.legend()\n\n# y coordinate\nfig = plt.figure(figsize=(16, 2))\n\nplt.plot(\n    t_axis_xy,\n    coor_data[\"y_coordinate\"].values,\n)\n\nplt.axvline(x=start_margin/metadata[\"SamplingFrequency\"], color='r', linestyle='--', label='LIBRE starts')\nplt.axvline(x=(T1_LIBRE_sample+start_margin)/ metadata[\"SamplingFrequency\"], color='b', linestyle='--', label='LIBER ends')\n\n\n\nplt.title(\"y_coordinate along time\")\nplt.xlabel(\"time [s]\")\nplt.ylabel(\"y coordinate [px]\")\nplt.legend()\n\ncoor_data_LIBRE = coor_data[start_margin:int(T1_LIBRE_sample)] \ncoor_recording_LIBRE = coor_recording[start_margin:int(T1_LIBRE_sample)] \n\ncoor_data_LIBRE_raw = copy.deepcopy(coor_data_LIBRE)\nprint(len(coor_data_LIBRE))\n</code></pre> <pre><code>mode: T1\nset start margin as 0\n374558\n</code></pre> <p></p> <p></p>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#42-eliminating-blinking","title":"4.2 Eliminating blinking","text":"<p>Based on the blink mask explored earlier, we now process the data to handle timestamps where blinks occurred. During a blink, gaze data is unreliable or missing, so we mark these periods by assigning NaN (Not a Number) to the gaze coordinates.</p> <pre><code>use_eliminate_blink = True\n\ndef eliminate_blink(coor_data, coor_recording, seq_name):\n# Note: instead of filtering them out, I assign nan to the invalid elements to preserve the time information\n    print(f'The data will be eliminated: {len(coor_data[coor_recording.blink &gt; 0])}')\n    coor_data.loc[coor_recording.blink &gt; 0,  ['x_coordinate', 'y_coordinate']] = np.nan\n    coor_recording.loc[coor_recording.blink &gt; 0,  ['x_coordinate', 'y_coordinate']] = np.nan\n    print(f'After eliminating blinking: \\nlen of coor_data_{seq_name} {len(coor_data)} \\\n      should be equal to len of coor_recording {len(coor_recording)}')\n    return coor_data, coor_recording\n\n\nif use_eliminate_blink:\n    coor_data_LIBRE,  coor_recording_LIBRE = eliminate_blink(coor_data_LIBRE, coor_recording_LIBRE, 'LIBRE')\n# coor_data_STANDARD,  coor_recording_STANDARD = eliminate_blink(coor_data_STANDARD, \n#                                                                coor_recording_STANDARD, 'STANDARD')\nplot_heatmap = False\nif plot_heatmap:\n    import plot\n    plot.plot_heatmap_coordinate(coor_data_LIBRE, density=False, screen_size=size, \n                                 title='LIBRE: The gaze from the beginning to the end')\n    plot.plot_heatmap_coordinate(coor_data_STANDARD, density=False, screen_size=size, \n                                 title='STANDARD: The gaze from the beginning to the end')\n</code></pre> <pre><code>The data will be eliminated: 9513\nAfter eliminating blinking: \nlen of coor_data_LIBRE 374558       should be equal to len of coor_recording 374558\n\n\n/var/folders/x4/yl1kbpks5sxc3345y3ttk6gm0000gn/T/ipykernel_28918/3369306811.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  coor_data.loc[coor_recording.blink &gt; 0,  ['x_coordinate', 'y_coordinate']] = np.nan\n</code></pre> <pre><code>\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#43-preserving-fixation","title":"4.3 Preserving fixation","text":"<p>Similarly, we will preserve the timestamps where fixation events are detected.</p> <pre><code>use_preserve_fixation = True\n\ndef preserve_fixation(coor_data, coor_recording, seq_name):\n#     coor_data = coor_data[coor_recording.fixation &gt; 0]\n#     coor_recording = coor_recording[coor_recording.fixation &gt; 0]\n    print(f'The data will be eliminated: {len(coor_data[coor_recording.fixation &lt; 1])}')\n    coor_data.loc[coor_recording.fixation &lt; 1, ['x_coordinate', 'y_coordinate']] = np.nan\n    coor_recording.loc[coor_recording.fixation &lt; 1, ['x_coordinate', 'y_coordinate']] = np.nan\n\n    return coor_data, coor_recording\n\n\nif use_preserve_fixation:\n    coor_data_LIBRE,  coor_recording_LIBRE = preserve_fixation(coor_data_LIBRE, coor_recording_LIBRE, 'LIBRE')\n\nplot_heatmap = False\nif plot_heatmap:\n    import plot\n    plot.plot_heatmap_coordinate(coor_data_LIBRE, density=False, screen_size=size, \n                                 title='LIBRE: The gaze from the beginning to the end')\n</code></pre> <pre><code>The data will be eliminated: 21482\n\n\n/var/folders/x4/yl1kbpks5sxc3345y3ttk6gm0000gn/T/ipykernel_28918/3299493500.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  coor_data.loc[coor_recording.fixation &lt; 1, ['x_coordinate', 'y_coordinate']] = np.nan\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#44-plot-the-gazing-dots","title":"4.4 Plot the gazing dots","text":"<p>To visualize the gaze points before/after the data cleaning</p> <pre><code>from matplotlib.font_manager import FontProperties\ntitle_font = FontProperties(family='Times New Roman', size=20, weight='bold')\naxis_font = FontProperties(family='Times New Roman', size=20)\n# ============================================================\n# Example data (replace with your actual data)\nX_coord = coor_data_LIBRE_raw['x_coordinate']\nY_coord = coor_data_LIBRE_raw['y_coordinate']\nfig, ax= plt.subplots(figsize=(8, 6))\n# Plot the data, flipping X coordinates and using dots as markers\nplt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\nplt.xlim((0, 800))\nplt.ylim((0, 600))\n# Set plot title\nplt.title('LIBRE: Before filtering', fontproperties=title_font)\n\nfor label in plt.gca().get_xticklabels():\n    label.set_fontproperties(axis_font)\n\nfor label in plt.gca().get_yticklabels():\n    label.set_fontproperties(axis_font)\n# Reverse the direction of the Y-axis\nplt.gca().invert_yaxis()\nplt.gca().invert_xaxis()\n\n# ============================================================\n# Example data (replace with your actual data)\nX_coord = coor_data_LIBRE['x_coordinate']\nY_coord = coor_data_LIBRE['y_coordinate']\nfig, ax= plt.subplots(figsize=(8, 6))\n# Plot the data, flipping X coordinates and using dots as markers\nplt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\nplt.xlim((0, 800))\nplt.ylim((0, 600))\n# Set plot title\nplt.title('LIBRE: After Filtering', fontproperties=title_font)\n\nfor label in plt.gca().get_xticklabels():\n    label.set_fontproperties(axis_font)\n\nfor label in plt.gca().get_yticklabels():\n    label.set_fontproperties(axis_font)\n# Reverse the direction of the Y-axis\nplt.gca().invert_yaxis()\nplt.gca().invert_xaxis()\n# ============================================================\n</code></pre> <p></p> <p></p>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#45-analyzing-gaze-point-distribution-before-and-after-filtering","title":"4.5 Analyzing Gaze Point Distribution Before and After Filtering","text":"<p>After filtering out blink-related timestamps by assigning NaN to gaze coordinates, it's important to inspect how this operation has affected the distribution of gaze points along each dimension (e.g., x_coordinate and y_coordinate).</p> <pre><code>import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set up the figure\nfig, axes = plt.subplots(2, 1, figsize=(16, 10))\n\n# Plot KDE for eye1_x_coordinate\nsns.kdeplot(coor_data_LIBRE_raw['x_coordinate'].dropna(), ax=axes[0], color='blue', fill=True, label='raw_x_coordinate')\nsns.kdeplot(coor_data_LIBRE['x_coordinate'].dropna(), ax=axes[0], color='orange', fill=True, label='filtered_x_coordinate')\naxes[0].set_title(\"X Coordinate: Raw vs. Filtered\")\naxes[0].set_xlabel(\"X Coordinate (px)\")\naxes[0].set_ylabel(\"Density\")\naxes[0].set_xlim((250,550))\naxes[0].legend()\n\n# Plot KDE for y coordinate\nsns.kdeplot(coor_data_LIBRE_raw[\"y_coordinate\"].dropna(), ax=axes[1], color='teal', fill=True, label='raw_y_coordinate')\nsns.kdeplot(coor_data_LIBRE[\"y_coordinate\"].dropna(), ax=axes[1], color='coral', fill=True, label='filtered_y_coordinate')\naxes[1].set_title(\"Y Coordinate: Raw vs. Filtered\")\naxes[1].set_xlabel(\"Y Coordinate (px)\")\naxes[1].set_ylabel(\"Density\")\naxes[1].set_xlim((250,350))\naxes[1].legend()\n\n# Add a common title\nfig.suptitle(\"Distribution of Eye-Tracking Data Along X Dimension\", fontsize=16)\n\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n</code></pre> <p></p>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#step-5-calculate-the-mask-statistics","title":"Step 5: Calculate the mask, statistics","text":"<p>You can calculate the mask based on your user case. </p> <p>In the following step, I will present my method to generate the mask according to the project I introduced in the course.</p> <pre><code>from mask_gen import cal_mask, plot_x_y_coord, filter_XY_coord\n</code></pre> <pre><code>def cal_stats(coor_data):\n    stats_dict = {}\n    X_coord = coor_data[\"x_coordinate\"].values\n    Y_coord = coor_data[\"y_coordinate\"].values\n\n    stats_dict['x_mean'] = np.nanmean(X_coord)\n    stats_dict['x_median'] = np.nanmedian(X_coord)      # Median, ignoring NaNs\n    stats_dict['x_std'] = np.nanstd(X_coord)        # Standard deviation\n    stats_dict['x_min'] = np.nanmin(X_coord)      # Minimum value\n    stats_dict['x_max'] = np.nanmax(X_coord)      # Maximum value\n    # Calculate 25th, 50th (median), and 75th percentiles while ignoring NaN values\n    stats_dict['x_q25'] = np.nanpercentile(X_coord, 25)  # 25th percentile\n    stats_dict['x_q75'] = np.nanpercentile(X_coord, 75)  # 75th percentile\n\n    stats_dict['y_mean'] = np.nanmean(Y_coord)\n    stats_dict['y_median'] = np.nanmedian(Y_coord)      # Median, ignoring NaNs\n    stats_dict['y_std'] = np.nanstd(Y_coord)        # Standard deviation\n    stats_dict['y_min'] = np.nanmin(Y_coord)      # Minimum value\n    stats_dict['y_max'] = np.nanmax(Y_coord)      # Maximum value\n    stats_dict['y_q25'] = np.nanpercentile(Y_coord, 25)  # 25th percentile\n    stats_dict['y_q75'] = np.nanpercentile(Y_coord, 75)  # 75th percentile\n\n    return stats_dict\n</code></pre> <pre><code>\n</code></pre> <pre><code>coor_data_LIBRE_1 = copy.deepcopy(coor_data_LIBRE)\nstats_dict = cal_stats(coor_data_LIBRE_1)\nprint(stats_dict)\n\n# You can play with the upper and lower boundary to filter the data as you need\nstand_x_upper=stats_dict['x_q75']-stats_dict['x_median']\nstand_x_lower=stats_dict['x_q25']-stats_dict['x_median']\nstand_y_upper=stats_dict['y_q75']-stats_dict['y_median']\nstand_y_lower=stats_dict['y_q25']-stats_dict['y_median']\nprint(f'stand_x_upper-{stand_x_upper} stand_x_lower-{stand_x_lower} stand_y_upper-{stand_y_upper} stand_y_lower-{stand_y_lower}')\n\nDisp_dict_LIBRE = cal_mask(coor_data_LIBRE_1, stand_x_upper, stand_x_lower, \n                           stand_y_upper, stand_y_lower)\nplot_x_y_coord(Disp_dict_LIBRE, metadata, duration=None, start_sample=0, seq_name='LIBRE')\nfiltered_coor_data_LIBRE, Preserve_mask, Discard_mask = filter_XY_coord(coor_data_LIBRE_1, Disp_dict_LIBRE, None)\n</code></pre> <pre><code>{'x_mean': 397.06954140148815, 'x_median': 395.5, 'x_std': 13.955737401446955, 'x_min': 353.3999938964844, 'x_max': 609.7999877929688, 'x_q25': 388.2999877929688, 'x_q75': 405.2999877929688, 'y_mean': 305.2710657401416, 'y_median': 305.29998779296875, 'y_std': 9.855362153300605, 'y_min': 225.8000030517578, 'y_max': 460.7000122070313, 'y_q25': 301.0, 'y_q75': 309.0}\nstand_x_upper-9.799987792968807 stand_x_lower--7.200012207031193 stand_y_upper-3.70001220703125 stand_y_lower--4.29998779296875\nX_coord [        nan         nan         nan ... 398.8999939 398.5\n 398.6000061]\nY_coord [         nan          nan          nan ... 302.1000061  301.79998779\n 300.1000061 ]\nAfter cleaning nan, eminating data affected by blinking,            and preserving the fixation \nThe length of X coordinate data: 374558\nThe length of Y coordinate data: 374558\n</code></pre> <p></p> <p></p> <p></p> <pre><code>coor_data_LIBRE_1 = copy.deepcopy(coor_data_LIBRE)\nstats_dict = cal_stats(coor_data_LIBRE_1)\nprint(stats_dict)\n\n# You can play with the upper and lower boundary to filter the data as you need\nstand_x_upper=40\nstand_x_lower=-40\nstand_y_upper=40\nstand_y_lower=-40\nprint(f'stand_x_upper-{stand_x_upper} stand_x_lower-{stand_x_lower} stand_y_upper-{stand_y_upper} stand_y_lower-{stand_y_lower}')\n\nDisp_dict_LIBRE = cal_mask(coor_data_LIBRE_1, stand_x_upper, stand_x_lower, \n                           stand_y_upper, stand_y_lower)\nplot_x_y_coord(Disp_dict_LIBRE, metadata, duration=None, start_sample=0, seq_name='LIBRE')\nfiltered_coor_data_LIBRE, Preserve_mask, Discard_mask = filter_XY_coord(coor_data_LIBRE_1, Disp_dict_LIBRE, None)\n</code></pre> <pre><code>{'x_mean': 397.06954140148815, 'x_median': 395.5, 'x_std': 13.955737401446955, 'x_min': 353.3999938964844, 'x_max': 609.7999877929688, 'x_q25': 388.2999877929688, 'x_q75': 405.2999877929688, 'y_mean': 305.2710657401416, 'y_median': 305.29998779296875, 'y_std': 9.855362153300605, 'y_min': 225.8000030517578, 'y_max': 460.7000122070313, 'y_q25': 301.0, 'y_q75': 309.0}\nstand_x_upper-40 stand_x_lower--40 stand_y_upper-40 stand_y_lower--40\nX_coord [        nan         nan         nan ... 398.8999939 398.5\n 398.6000061]\nY_coord [         nan          nan          nan ... 302.1000061  301.79998779\n 300.1000061 ]\nAfter cleaning nan, eminating data affected by blinking,            and preserving the fixation \nThe length of X coordinate data: 374558\nThe length of Y coordinate data: 374558\n</code></pre> <p></p> <p></p> <p></p> <pre><code># You can play with the upper and lower boundary to filter the data as you need\nstand_x_upper=40\nstand_x_lower=-40\nstand_y_upper=40\nstand_y_lower=-40\nprint(f'stand_x_upper-{stand_x_upper} stand_x_lower-{stand_x_lower} stand_y_upper-{stand_y_upper} stand_y_lower-{stand_y_lower}')\n\nDisp_dict_LIBRE = cal_mask(coor_data_LIBRE_1, stand_x_upper, stand_x_lower, \n                           stand_y_upper, stand_y_lower)\nplot_x_y_coord(Disp_dict_LIBRE, metadata, duration=None, start_sample=0, seq_name='LIBRE')\nfiltered_coor_data_LIBRE, Preserve_mask, Discard_mask = filter_XY_coord(coor_data_LIBRE_1, Disp_dict_LIBRE, None)\n</code></pre> <pre><code>stand_x_upper-40 stand_x_lower--40 stand_y_upper-40 stand_y_lower--40\nX_coord [        nan         nan         nan ... 398.8999939 398.5\n 398.6000061]\nY_coord [         nan          nan          nan ... 302.1000061  301.79998779\n 300.1000061 ]\nAfter cleaning nan, eminating data affected by blinking,            and preserving the fixation \nThe length of X coordinate data: 374558\nThe length of Y coordinate data: 374558\n</code></pre> <p></p> <p></p> <p></p> <pre><code>def visualization_func(fig_title, filtered_coor_data_LIBRE):\n    from matplotlib.font_manager import FontProperties\n    title_font = FontProperties(family='Times New Roman', size=20, weight='bold')\n    axis_font = FontProperties(family='Times New Roman', size=20)\n\n    fig, ax= plt.subplots(figsize=(8, 6))\n    plt.title(fig_title, fontproperties=title_font)\n#     -----------------------------------------------------------------------\n#     # Plot the data, flipping X coordinates and using dots as markers\n#     X_coord = coor_data_STANDARD['x_coordinate']\n#     Y_coord = coor_data_STANDARD['y_coordinate']\n#     plt.plot(X_coord, Y_coord, '.', color='#ADD8E6', markersize=15, label='Standard')\n#     -----------------------------------------------------------------------\n    coor_data_LIBRE_vis = copy.deepcopy(coor_data_LIBRE)\n    X_coord_1 = coor_data_LIBRE_raw['x_coordinate']\n    Y_coord_1 = coor_data_LIBRE_raw['y_coordinate']\n\n    # plt.plot(X_coord_1, Y_coord_1, '.', color='#728FCE', markersize=15, label='LIBRE w.o. binning')\n    plt.scatter(X_coord_1, Y_coord_1, s=50, c='#728FCE', alpha=0.1, edgecolors='#728FCE', linewidth=0.5)\n\n#     -----------------------------------------------------------------------\n    X_coord = filtered_coor_data_LIBRE['x_coordinate']\n    Y_coord = filtered_coor_data_LIBRE['y_coordinate']\n    print(F\"if same {X_coord_1 == X_coord}\")\n    # plt.plot(X_coord, Y_coord, '.', color='#f4d03f', markersize=15, label='LIBRE binning')\n    plt.scatter(X_coord, Y_coord, s=50, c='#f4d03f', alpha=0.1, edgecolors='#f4d03f', linewidth=0.5)  # Larger points\n#     ----------------------------------------------------------------------------------------------\n    # plt.legend(prop={'family': 'Times New Roman', 'size': 10})\n\n    plt.tick_params(axis='x', labelsize=14)\n    plt.tick_params(axis='y', labelsize=14)\n    plt.xlim((0, 800))\n    plt.ylim((0, 600))\n    # Set plot title\n#     plt.title(fig_title, fontproperties=title_font)\n    for label in plt.gca().get_xticklabels():\n        label.set_fontproperties(axis_font)\n\n    for label in plt.gca().get_yticklabels():\n        label.set_fontproperties(axis_font)\n    # Reverse the direction of the Y-axis\n    plt.gca().invert_yaxis()\n    plt.gca().invert_xaxis()\n\n\n# Visualization of filtered coor data LIBRE\nvisualization_func(fig_title='Before vs. After Filtering', \n                   filtered_coor_data_LIBRE=filtered_coor_data_LIBRE)\n</code></pre> <pre><code>if same 0         False\n1         False\n2         False\n3         False\n4         False\n          ...  \n374553     True\n374554     True\n374555     True\n374556     True\n374557     True\nName: x_coordinate, Length: 374558, dtype: bool\n</code></pre> <p></p>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#step-6-investigate-the-statistics-of-eye-movement-events-across-different-subjects","title":"Step 6: Investigate the statistics of eye movement events across different subjects","text":"<p>Eye movement events such as blinks, saccades, and fixations provide valuable information about gaze behavior and data quality. By analyzing these events across different subjects, we can assess the consistency and reliability of the recorded data.</p>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#61-statistics-calculation","title":"6.1 Statistics calculation","text":"<pre><code># Specify the subject and modality you want to inspect\n\nT_idx = 1\nrecording_list = []\nfor subject_idx in range(1,5):\n    print(f'Load recording for subject {subject_idx}...')\n    if T_idx == 1:\n        mode = 'T1'\n    else:\n        mode = 'T2'\n\n\n    BIDS_PATH = Path(\"./data/\")  # file within a subdirectory\n    FILE_NAME = f\"sub00{subject_idx}_T{T_idx}\"    \n\n\n    tsv_name = f\"{FILE_NAME}.tsv.gz\"\n\n    recording_file = BIDS_PATH / tsv_name\n    print(f'recording_file: {recording_file}')\n\n    recording = pd.read_csv(\n        recording_file,\n    #     sep=r\"\\s+\",\n        sep=\"\\t\",\n        na_values=\"n/a\",\n    )\n\n    # Duration setting according to the MRI sequence info\n    if  subject_idx == 1:\n        T1_LIBRE = 374.558\n        T2_LIBRE = 650.185\n    elif subject_idx == 2:\n        T1_LIBRE = 374.380\n        T2_LIBRE = 650.185\n    elif subject_idx == 3:\n        T1_LIBRE = 334.237\n        T2_LIBRE = 650.1925 \n    else:\n        T1_LIBRE = 374.565\n        T2_LIBRE = 650.1875\n\n    if T_idx == 1:\n        print(f\"The length of T1_LIBRE Subject{subject_idx} should be: {T1_LIBRE}\")\n    else:\n        print(f\"The length of T2_LIBRE Subject{subject_idx} should be: {T2_LIBRE}\")\n\n    coor_recording = recording\n    start_margin = int(0.0)\n    if mode == 'T1':\n        print(f'mode: {mode}')\n        T1_LIBRE_sample = T1_LIBRE*metadata['SamplingFrequency']\n\n    else:\n        print(f'mode: {mode}')\n        T2_LIBRE_sample = T2_LIBRE*metadata['SamplingFrequency']\n\n\n    if mode == 'T1':\n        coor_recording_LIBRE = coor_recording[start_margin:int(T1_LIBRE_sample)]   \n    else:\n        coor_recording_LIBRE = coor_recording[start_margin:int(T2_LIBRE_sample)] \n    coor_recording_LIBRE = coor_recording_LIBRE.rename(columns={'eye1_saccade': 'saccade'})\n    coor_recording_LIBRE = coor_recording_LIBRE.rename(columns={'eye1_fixation': 'fixation'})\n    coor_recording_LIBRE = coor_recording_LIBRE.rename(columns={'eye1_blink': 'blink'})\n    recording_list.append(coor_recording_LIBRE)\n</code></pre> <pre><code>Load recording for subject 1...\nrecording_file: data/sub001_T1.tsv.gz\nThe length of T1_LIBRE Subject1 should be: 374.558\nmode: T1\nLoad recording for subject 2...\nrecording_file: data/sub002_T1.tsv.gz\nThe length of T1_LIBRE Subject2 should be: 374.38\nmode: T1\nLoad recording for subject 3...\nrecording_file: data/sub003_T1.tsv.gz\nThe length of T1_LIBRE Subject3 should be: 334.237\nmode: T1\nLoad recording for subject 4...\nrecording_file: data/sub004_T1.tsv.gz\nThe length of T1_LIBRE Subject4 should be: 374.565\nmode: T1\n</code></pre> <pre><code>len(recording_list)\n</code></pre> <pre><code>4\n</code></pre> <pre><code># Identify start and end indices of each event\ndef identify_start_end_event(df, events):\n    event_dict = {}\n    for event in events:\n        event_change = event+'_change'\n\n        event_starts = df.index[df[event_change] == 1].tolist()\n        event_ends = df.index[df[event_change] == -1].tolist()\n\n        # If the last row is part of a event, add it as an end\n        if df[event].iloc[-1] == 1:\n            event_ends.append(df.index[-1])\n\n        # Calculate number of events\n        num_events = len(event_starts)\n\n        # Calculate duration for each event\n        event_durations = []\n        for start, end in zip(event_starts, event_ends):\n            # Duration in milliseconds or based on the timestamp difference\n            duration = end - start + 1  # If in milliseconds\n            # If you have a timestamp column (e.g., in ms), you could calculate it as:\n            # duration = df['timestamp'].iloc[end] - df['timestamp'].iloc[start]\n            event_durations.append(duration)\n\n        # Display results\n        num_event_str = 'num_'+event\n        event_dict[num_event_str] = num_events\n        event_duration_str = event+'_durations'\n        event_dict[event_duration_str] = event_durations\n\n    return event_dict\n</code></pre> <pre><code>events = ['saccade', 'fixation', 'blink']\nsubject_event_dict = []\nfor idx in range(4):\n    df = copy.deepcopy(recording_list[idx])\n    # Detect the start and end of each saccade\n    df['saccade_shift'] = df['saccade'].shift(1, fill_value=0)\n    df['saccade_change'] = df['saccade'] - df['saccade_shift']\n\n    # Detect the start and end of each fixation\n    df['fixation_shift'] = df['fixation'].shift(1, fill_value=0)\n    df['fixation_change'] = df['fixation'] - df['fixation_shift']\n\n    # Detect the start and end of each blink\n    df['blink_shift'] = df['blink'].shift(1, fill_value=0)\n    df['blink_change'] = df['blink'] - df['blink_shift']\n\n    event_dict = identify_start_end_event(df, events)\n    subject_event_dict.append(event_dict)\n</code></pre> <pre><code>def cal_event_stat(event_duration):\n\n    event_stat_dict={}\n    event_stat_dict['mean_duration'] = np.mean(event_duration)\n    event_stat_dict['median_duration'] = np.median(event_duration)\n    event_stat_dict['std_duration'] = np.std(event_duration)\n    event_stat_dict['min_duration'] = np.min(event_duration)\n    event_stat_dict['max_duration'] = np.max(event_duration)\n    event_stat_dict['total_duration'] = sum(event_duration)\n    event_stat_dict['times'] = len(event_duration)\n\n    return event_stat_dict\n\ndef voilinplot(subject_event_dict, event,color='skyblue'):\n    sub_num = len(subject_event_dict)\n    event_duration_list=[]\n    for idx in range(sub_num):\n        event_durations_str = f'{event}_durations'\n        event_duration_list.append(subject_event_dict[idx][event_durations_str])\n\n    df = pd.DataFrame({\n        'Duration': [duration for sublist in event_duration_list for duration in sublist],\n        'Subject': [f'Subject {i+1}' for i, sublist in enumerate(event_duration_list) for _ in sublist]\n    })\n\n    plt.figure(figsize=(6, 5))\n    sns.violinplot(data=df, x='Subject', y='Duration', color=color)\n    plt.title(f\"Violin Plot of {event} Durations Across Subjects\")\n    plt.ylabel(\"Duration (ms)\")\n    plt.xlabel(\"Subject\")\n    plt.show()\n</code></pre> <pre><code>subject_blink_stat = []\nsubject_saccade_stat = []\nsubject_fixation_stat = []\nfor idx in range(4):\n    event_dict = subject_event_dict[idx]\n\n    blink_stat = cal_event_stat(event_dict['blink_durations'])\n    subject_blink_stat.append(blink_stat)\n\n    saccade_stat = cal_event_stat(event_dict['saccade_durations'])\n    subject_saccade_stat.append(saccade_stat)\n\n    fixation_stat = cal_event_stat(event_dict['fixation_durations'])\n    subject_fixation_stat.append(fixation_stat)\n</code></pre> <pre><code>subject_fixation_stat\n</code></pre> <pre><code>[{'mean_duration': 485.3278463648834,\n  'median_duration': 247.0,\n  'std_duration': 623.9966482850247,\n  'min_duration': 22,\n  'max_duration': 4157,\n  'total_duration': 353804,\n  'times': 729},\n {'mean_duration': 672.0169811320754,\n  'median_duration': 491.5,\n  'std_duration': 594.362456671281,\n  'min_duration': 17,\n  'max_duration': 5003,\n  'total_duration': 356169,\n  'times': 530},\n {'mean_duration': 417.9804195804196,\n  'median_duration': 246.0,\n  'std_duration': 466.94418247439074,\n  'min_duration': 24,\n  'max_duration': 3348,\n  'total_duration': 298856,\n  'times': 715},\n {'mean_duration': 128.8170276325616,\n  'median_duration': 92.0,\n  'std_duration': 117.27843003164334,\n  'min_duration': 9,\n  'max_duration': 1012,\n  'total_duration': 172486,\n  'times': 1339}]\n</code></pre> <p>A Violin Plot is a Useful Tool for Visualizing Event Statistics.</p> <p>A violin plot combines the features of a box plot and a density plot, making it particularly useful for visualizing the distribution of data. It provides a comprehensive view of both the summary statistics and the underlying data distribution, which is highly beneficial when analyzing eye movement event statistics across different subjects.</p> <pre><code>voilinplot(subject_event_dict, event='blink', color='skyblue')\nvoilinplot(subject_event_dict, event='saccade',color='coral')\nvoilinplot(subject_event_dict, event='fixation',color='teal')\n</code></pre> <p></p> <p></p> <p></p> <pre><code>\n</code></pre>"},{"location":"post-processing/1_1_et_QAQC_of_mreye_data/#62-statistics-test","title":"6.2 Statistics Test","text":"<p>Statistical tests, such as the Mann-Whitney U Test, t-Test, and Kolmogorov-Smirnov Test are used to compare statistics to determine whether differences between groups or conditions are significant or due to random variation. Without statistical tests, comparisons might rely on subjective observations or visual inspection, which are prone to bias. Statistical tests provide quantifiable and reproducible evidence to support conclusions.</p> <ul> <li> <p>The Mann-Whitney U test is a non-parametric statistical test used to determine whether there is a significant difference between the distributions of two independent samples.  It is particularly useful when comparing groups where the data may not meet the assumptions required by parametric tests like the t-test.</p> </li> <li> <p>T-test requires normally distributed data and equal variances, which may not hold true for eye movement statistics. Violating these assumptions could lead to misleading results.</p> </li> <li> <p>Kolmogorov-Smirnov Test compares entire distributions but lacks the sensitivity of the Mann-Whitney U test for detecting location shifts (differences in medians).</p> </li> </ul> <p>Eye-tracking event statistics (e.g., blink counts, fixation durations) may not follow normal distributions due to inter-subject variability or artifacts. The Mann-Whitney U test allows robust comparison of these statistics between groups or conditions without requiring strict assumptions about the data.</p> <pre><code>def mann_whitney_u_stats(subject_event_dict, event):\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import pandas as pd\n    from scipy.stats import mannwhitneyu\n    sub_num = len(subject_event_dict)\n    event_duration_list=[]\n    for idx in range(sub_num):\n        event_durations_str = f'{event}_durations'\n        event_duration_list.append(subject_event_dict[idx][event_durations_str])\n\n    # Prepare data for Seaborn\n    df = pd.DataFrame({\n        'Duration': [duration for sublist in event_duration_list for duration in sublist],\n        'Subject': [f'subjects {i+1}' for i, sublist in enumerate(event_duration_list) for _ in sublist]\n    })\n\n    print(f\"Pairwise Mann-Whitney U test results for {event} durations:\")\n    for i in range(sub_num):\n        for j in range(i + 1, sub_num):\n            stat, p_value = mannwhitneyu(event_duration_list[i], event_duration_list[j], alternative='two-sided')\n            print(f\"Comparison between {f'subjects {i+1}'} and {f'subjects {j+1}'}: U = {stat}, p = {p_value:.4f}\")\n</code></pre> <pre><code>mann_whitney_u_stats(subject_event_dict,'saccade')\n</code></pre> <pre><code>Pairwise Mann-Whitney U test results for saccade durations:\nComparison between subjects 1 and subjects 2: U = 126157.0, p = 0.0000\nComparison between subjects 1 and subjects 3: U = 162432.0, p = 0.0000\nComparison between subjects 1 and subjects 4: U = 154110.5, p = 0.0000\nComparison between subjects 2 and subjects 3: U = 165390.0, p = 0.0002\nComparison between subjects 2 and subjects 4: U = 158977.5, p = 0.0000\nComparison between subjects 3 and subjects 4: U = 267041.5, p = 0.0000\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/","title":"MREye_Track","text":"<p>Derived from: https://www.axonlab.org/hcph-sops/data-management/eyetrack-qc/</p> <p>Author: Yiwei Jia</p> <p>In this notebook, we delve into eye-tracking data analysis, focusing specifically on the MREye_track project. The project involves visual stimuli appearing in different directions: up, down, left, right and center. Within this notebook, we inspect the eye-tracking signals, determine regions of interest (ROIs) corresponding to each stimulus, and generate eye-tracking (ET) masks. Additionally, we calculate the distance distribution between two adjacent gaze points and also analyze that of specific time windows of eye-tracking data.</p> <pre><code># Derived from:\n#     https://www.axonlab.org/hcph-sops/data-management/eyetrack-qc\n# Load the autoreload extension\n%load_ext autoreload\n# Set autoreload to update the modules every time before executing a new line of code\n%autoreload 2\n\n%matplotlib inline\nfrom pathlib import Path\nimport json\nimport ppjson\nfrom importlib import reload  # For debugging purposes\n\nimport numpy as np\nimport pandas as pd\n\nimport eyetrackingrun as et\n\nfrom IPython.display import HTML\nfrom matplotlib import animation\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport copy\nfrom write_bids_yiwei import EyeTrackingRun, write_bids, write_bids_from_df\n</code></pre> <pre><code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code></pre> <pre><code>subject_idx = 1\nT_idx = 1\n\nif T_idx == 1:\n    mode = 'T1'\nelse:\n    mode = 'T2'\n\nBIDS_PATH = Path(\"/Users/cag/Documents/Dataset/MREyeTrack/EDF\")\n\nif subject_idx == 1:    \n    FILE_NAME = f\"000001_fixed_dot-16_grid_T1w_2024-10-14_17h24\"    \nelif subject_idx == 2:\n    FILE_NAME = f\"000002_fixed_dot-16_grid_T1w_2024-10-14_16h54\" \nelif subject_idx == 3: \n    FILE_NAME = f\"000003_fixed_dot-16_grid_T1w_2024-10-14_16h35\"\n    # \"OT4.EDF\"\nelse:\n    FILE_NAME = f\"\"\n\n# D:\\Eye_Dataset\\Sub001\\230928_anatomical_MREYE_study\\ET_EDF\nsession = \"001\" #can be a string to be defined when required\ntsv_name = f\"{FILE_NAME}.tsv.gz\"\ngif_name = f'{FILE_NAME}.gif'\n</code></pre> <pre><code># Read in tsv.gz file\n# Naming convention can be modified later\n# recording_file = BIDS_PATH / \"sub-001\" / f\"ses-{session}\" / \"dwi\" / f\"sub-001_ses-{session}_acq-highres_dir-RL_recording-eyetrack_physio.tsv.gz\"\nrecording_file = BIDS_PATH / tsv_name\nprint(f'recording_file: {recording_file}')\nrecording = pd.read_csv(\n    recording_file,\n#     sep=r\"\\s+\",\n    sep=\"\\t\",\n    na_values=\"n/a\",\n)\nrecording.head()\nprint(f'recording len: {len(recording)}')\nmetadata = json.loads((\n    recording_file.parent\n    / recording_file.name.replace(\".tsv.gz\", \".json\")\n).read_text())\n# print(f'meta_data: {metadata}')\n\nprint('The duration of mri acquisition sub001 sub002 and sub003: 655240 ms')\n</code></pre> <pre><code>recording_file: /Users/cag/Documents/Dataset/MREyeTrack/EDF/000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.tsv.gz\nrecording len: 657900\nThe duration of mri acquisition sub001 sub002 and sub003: 655240 ms\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#1-metadata-and-calibration","title":"1 Metadata and calibration","text":"<p>We always run a calibration before it is started. As a result, we will typically see the metadata corresponding to calibration on this particular run.</p> <p>Now, let's print out the contents of the BIDS' sidecar JSON corresponding to the DWI in this session.</p> <pre><code>target_message = \"ET: Start routine 'centered_dot'\"\ntarget_timestamp = 0\nfor index, element in enumerate(metadata['LoggedMessages']):\n    if element[1] == target_message:\n        print(f\"First occurrence is at index {index}: {element}\")\n        target_timestamp = element[0]\n        print(f\"at timestamp {target_timestamp}\")\n        break\ngap_start_dot = target_timestamp - metadata['StartTime']\n\n# Here I don't consider the lag between start and the first stimuli anymore\n# Start time = first trigger\n\n# gap_start_dot = 0\nprint(f\"Timestamp to start recording: {metadata['StartTime']}\")\nprint(f'From the start of recording to the first dot: {gap_start_dot}')\n</code></pre> <pre><code>First occurrence is at index 27: [15155843, \"ET: Start routine 'centered_dot'\"]\nat timestamp 15155843\nTimestamp to start recording: 15155345\nFrom the start of recording to the first dot: 498\n</code></pre> <pre><code>if subject_idx == 1:\n    print(recording[655335:655338])\n    # 655336 timestamp 15810681\n    # From there it became NaN\n</code></pre> <pre><code>        Unnamed: 0  eye1_x_coordinate  eye1_y_coordinate  eye1_pupil_size  \\\n655335      655335         443.899994         471.299988           1594.0   \n655336      655336         443.899994         471.299988           1594.0   \n655337      655337                NaN                NaN              NaN\n\n        eye1_pupil_x_coordinate  eye1_pupil_y_coordinate  \\\n655335                  -5161.0                  -2741.0   \n655336                  -5161.0                  -2741.0   \n655337                 -32768.0                 -32768.0\n\n        eye1_href_x_coordinate  eye1_href_y_coordinate  fast_href_x_velocity  \\\n655335                   434.0                  1691.0         -1.946136e+14   \n655336                   434.0                  1691.0         -1.946136e+14   \n655337                 -7936.0                 -7936.0         -1.946136e+14\n\n        fast_raw_x_velocity  screen_ppdeg_x_coordinate  \\\n655335         2.161728e+20                  26.700001   \n655336         2.161728e+20                  26.700001   \n655337         2.161728e+20                  26.700001\n\n        screen_ppdeg_y_coordinate  timestamp  eye1_fixation  eye1_saccade  \\\n655335                  26.799999   15810680              0             1   \n655336                  26.799999   15810681              0             1   \n655337                  26.799999   15810682              0             1\n\n        eye1_blink  \n655335           1  \n655336           1  \n655337           1\n</code></pre> <pre><code>t_axis = (\n    recording.timestamp.values - recording.timestamp[0]\n) / metadata[\"SamplingFrequency\"]*1000\nprint(f\"The end of the timestamp: {t_axis[-1]}\")\n</code></pre> <pre><code>The end of the timestamp: 657899.0\n</code></pre> <pre><code>print(metadata[\"Columns\"])\nrecording = recording.rename(\n        columns={\n            f\"eye1_pupil_size\": f\"pupil_size\",\n            f\"eye1_fixation\": f\"fixation\",\n            f\"eye1_saccade\": f\"saccade\",\n            f\"eye1_blink\": f\"blink\",\n            f\"eye1_x_coordinate\": f\"x_coordinate\", \n            f\"eye1_y_coordinate\": f\"y_coordinate\"         \n        }\n    )\n\nmetadata[\"Columns\"] = recording.columns.tolist()\nprint(metadata[\"Columns\"])\n</code></pre> <pre><code>['eye1_x_coordinate', 'eye1_y_coordinate', 'eye1_pupil_size', 'eye1_pupil_x_coordinate', 'eye1_pupil_y_coordinate', 'eye1_href_x_coordinate', 'eye1_href_y_coordinate', 'fast_href_x_velocity', 'fast_raw_x_velocity', 'screen_ppdeg_x_coordinate', 'screen_ppdeg_y_coordinate', 'timestamp', 'eye1_fixation', 'eye1_saccade', 'eye1_blink']\n['Unnamed: 0', 'x_coordinate', 'y_coordinate', 'pupil_size', 'eye1_pupil_x_coordinate', 'eye1_pupil_y_coordinate', 'eye1_href_x_coordinate', 'eye1_href_y_coordinate', 'fast_href_x_velocity', 'fast_raw_x_velocity', 'screen_ppdeg_x_coordinate', 'screen_ppdeg_y_coordinate', 'timestamp', 'fixation', 'saccade', 'blink']\n</code></pre> <pre><code>recording.columns\n</code></pre> <pre><code>Index(['Unnamed: 0', 'x_coordinate', 'y_coordinate', 'pupil_size',\n       'eye1_pupil_x_coordinate', 'eye1_pupil_y_coordinate',\n       'eye1_href_x_coordinate', 'eye1_href_y_coordinate',\n       'fast_href_x_velocity', 'fast_raw_x_velocity',\n       'screen_ppdeg_x_coordinate', 'screen_ppdeg_y_coordinate', 'timestamp',\n       'fixation', 'saccade', 'blink'],\n      dtype='object')\n</code></pre> <pre><code># See cleaned cell 4\n</code></pre> <pre><code>plot_pupil_saccade = False\n\nif plot_pupil_saccade:\n    fig = plt.figure(figsize=(16, 2))\n\n    plt.plot(\n        t_axis,\n        recording[\"pupil_size\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"saccade\"].values * 5000,\n    )\n\n    plt.xlabel(\"time [s]\")\n    plt.ylabel(\"pupil area [a.u.]\")\n    plt.xlim((200, 220))\n    plt.title('Pupil area and saccading')\n    print(f'At this time, since blinking is detected, we cannot get clear pupil area ?')\n</code></pre> <pre><code>plot_x_saccade = False\nif plot_x_saccade:\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"x_coordinate\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"saccade\"].values * 1000,\n    )\n    plt.xlim((200, 220))\n</code></pre> <p>Check fixation</p> <pre><code>plot_fixation = False\nif plot_fixation:\n    # Fixation\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"x_coordinate\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"fixation\"].values * 500,\n    )\n    plt.xlim((340, 350))\n    plt.title('X coordinate of gazing with fixation')\n\n    fig = plt.figure(figsize=(16, 2))\n    plt.plot(\n        t_axis,\n        recording[\"y_coordinate\"].values,\n    )\n\n    plt.plot(\n        t_axis,\n        recording[\"fixation\"].values * 500,\n    )\n    plt.xlim((340, 350))\n    plt.title('Y coordinate of gazing with fixation')\n</code></pre> <p>The analysis did not reveal a strong correlation between pupil size and saccade or fixation events.</p> <p>The coordinate sequence is stable during fixation, we should extract the data in the event.</p>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#2-data-cleaning","title":"2 Data Cleaning","text":"<ul> <li>Separate the two phases of sequences</li> <li>Extract the X Y coordinates</li> <li>Eliminate the blinking and non-fixation area of LIBRE data, and keep the raw data of STANDARD.Because there is no mechanism for the second STANDARD one to correct the eye fixation.</li> <li>Design some criteria for cleaning the noise data<ul> <li>Visual angle</li> <li>Heat maps</li> <li>Fixation algorithm (literature review)</li> </ul> </li> </ul>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#21-separate-the-two-phases-of-sequences","title":"2.1 Separate the two phases of sequences","text":"<pre><code># -----------------------------\nreset_index_bool = False\neliminate_all_nan = False\neliminate_first_nan = False\n# -----------------------------\n\nsize = (\n    metadata[\"ScreenAOIDefinition\"][1][1],\n    metadata[\"ScreenAOIDefinition\"][1][3],\n)\n\ncoor_data = recording[[\"x_coordinate\", \"y_coordinate\"]]\nprint(f\"Extract X Y coordinates from recording: {len(coor_data)}\")\n\nif eliminate_all_nan:\n    coor_data = coor_data[coor_data.x_coordinate.notna() &amp; coor_data.y_coordinate.notna()]\n    coor_recording = recording[recording.x_coordinate.notna() &amp; recording.y_coordinate.notna()]\nelif eliminate_first_nan:\n    # Find the first non-NaN row\n    first_valid_index = coor_data[['x_coordinate', 'y_coordinate']].dropna(how='all').index[0]\n\n    # Slice the DataFrame from the first non-NaN row onward\n    coor_data = coor_data.iloc[first_valid_index:]\n\n    coor_recording = recording.iloc[first_valid_index:]\n\nelse:\n    coor_recording = recording\n\nif reset_index_bool:\n    coor_data.reset_index(drop=True, inplace=True)\n    coor_recording.reset_index(drop=True, inplace=True)\n\nprint(coor_data)\nprint(f\"Eliminating nan values? {eliminate_first_nan}: {len(coor_data)}\")\nprint(f'len of coor_data {len(coor_data)} should be equal to len of coor_recording {len(coor_recording)}')\n</code></pre> <pre><code>Extract X Y coordinates from recording: 657900\n        x_coordinate  y_coordinate\n0         466.899994    279.200012\n1         466.899994    279.899994\n2         466.899994    279.799988\n3         465.500000    280.399994\n4         464.100006    281.000000\n...              ...           ...\n657895           NaN           NaN\n657896           NaN           NaN\n657897           NaN           NaN\n657898           NaN           NaN\n657899           NaN           NaN\n\n[657900 rows x 2 columns]\nEliminating nan values? False: 657900\nlen of coor_data 657900 should be equal to len of coor_recording 657900\n</code></pre> <pre><code># Duration setting according to the protocol (in sec) not any more\n# Duration setting according to the raw data info\n\n\nT1_LIBRE = (len(coor_data))/metadata['SamplingFrequency']\n\nT1_GAP = 0\nT1_VIBE = 0\n\n\nprint(f\"The length of T1_LIBRE Subject{subject_idx} should be: {T1_LIBRE} sec (len(coor_data)-gap_start_dot)\")\n</code></pre> <pre><code>The length of T1_LIBRE Subject1 should be: 657.9 sec (len(coor_data)-gap_start_dot)\n</code></pre> <pre><code>start_margin = 0\nif mode == 'T1':\n    print(f'mode: {mode}')\n    print(f\"set start margin as {start_margin}\")\n    T1_LIBRE_sample = T1_LIBRE*metadata['SamplingFrequency']\n    T1_GAP_sample = T1_GAP*metadata['SamplingFrequency']\n    T1_VIBE = len(coor_data) - T1_LIBRE_sample - T1_GAP_sample\nelse:\n    print(f'mode: {mode}')\n    print(f\"set start margin as {start_margin}\")\n    T2_LIBRE_sample = T2_LIBRE*metadata['SamplingFrequency']\n    T2_GAP_sample = T2_GAP*metadata['SamplingFrequency']\n    T2_TSE = len(coor_data) - T2_LIBRE_sample - T2_GAP_sample\n\n\n\nt_axis_xy = (\n    coor_data.index \n) / metadata[\"SamplingFrequency\"]\n\n# x coordinate\nfig = plt.figure(figsize=(16, 2))\n\nplt.plot(\n    t_axis_xy,\n    coor_data[\"x_coordinate\"].values,\n)\n\nif mode == 'T1':\n    plt.axvline(x=start_margin/metadata[\"SamplingFrequency\"], color='r', linestyle='--', label='LIBRE starts')\n    plt.axvline(x=(T1_LIBRE_sample+start_margin)/metadata[\"SamplingFrequency\"], color='b', linestyle='--', label='LIBRE ends')\n    plt.axvline(x=(T1_LIBRE_sample+start_margin+T1_GAP_sample)/metadata[\"SamplingFrequency\"], color='g', \n                linestyle='--', label='Standard starts')\nelse:\n    plt.axvline(x=start_margin/metadata[\"SamplingFrequency\"], color='r', linestyle='--', label='LIBRE starts')\n    plt.axvline(x=(T2_LIBRE_sample+start_margin)/metadata[\"SamplingFrequency\"], color='b', linestyle='--', label='LIBRE ends')\n    plt.axvline(x=(T2_LIBRE_sample+start_margin+T2_GAP_sample)/metadata[\"SamplingFrequency\"], color='g', \n                linestyle='--', label='Standard starts')\n\nplt.title(\"x_coordinate along time\")\nplt.xlabel(\"time [s]\")\nplt.ylabel(\"x coordinate [px]\")\nplt.legend()\n\n# y coordinate\nfig = plt.figure(figsize=(16, 2))\n\nplt.plot(\n    t_axis_xy,\n    coor_data[\"y_coordinate\"].values,\n)\n\nplt.axvline(x=start_margin/metadata[\"SamplingFrequency\"], color='r', linestyle='--', label='LIBRE starts')\nplt.axvline(x=(T1_LIBRE_sample+start_margin)/ metadata[\"SamplingFrequency\"], color='b', linestyle='--', label='LIBRE ends')\nplt.axvline(x=(T1_LIBRE_sample+start_margin+T1_GAP_sample)/metadata[\"SamplingFrequency\"], color='g', \n            linestyle='--', label='Standard starts')\n\n\nplt.title(\"y_coordinate along time\")\nplt.xlabel(\"time [s]\")\nplt.ylabel(\"y coordinate [px]\")\nplt.legend()\n\n\ncoor_data_LIBRE = coor_data[start_margin:int(T1_LIBRE_sample)+start_margin] \ncoor_recording_LIBRE = coor_recording[start_margin:int(T1_LIBRE_sample)+start_margin] \ncoor_data_LIBRE_raw = copy.deepcopy(coor_data_LIBRE)\nprint(len(coor_data_LIBRE))\n</code></pre> <pre><code>mode: T1\nset start margin as 0\n657900\n</code></pre> <pre><code># import scipy.io\n# coor_array = coor_data_LIBRE[['x_coordinate', 'y_coordinate']].to_numpy()\n# scipy.io.savemat(f'coor_data_raw_t1w_libre_sub00{subject_idx}.mat', {'coordinates': coor_array})\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#22-eliminating-blinking","title":"2.2 Eliminating blinking","text":"<pre><code>use_eliminate_blink = True\n\ndef eliminate_blink(coor_data, coor_recording, seq_name):\n#     coor_data = coor_data[coor_recording.blink &lt; 1]\n#     coor_recording = coor_recording[coor_recording.blink &lt; 1]\n# Note: instead of filtering them out, I assign nan to the invalid elements\n    print(f'The data will be eliminated: {len(coor_data[coor_recording.blink &gt; 0])}')\n    coor_data.loc[coor_recording.blink &gt; 0,  ['x_coordinate', 'y_coordinate']] = np.nan\n    coor_recording.loc[coor_recording.blink &gt; 0,  ['x_coordinate', 'y_coordinate']] = np.nan\n    print(f'After eliminating blinking: \\nlen of coor_data_{seq_name} {len(coor_data)} \\\n      should be equal to len of coor_recording {len(coor_recording)}')\n    return coor_data, coor_recording\n\n\nif use_eliminate_blink:\n    coor_data_LIBRE,  coor_recording_LIBRE = eliminate_blink(coor_data_LIBRE, coor_recording_LIBRE, 'LIBRE')\n# coor_data_STANDARD,  coor_recording_STANDARD = eliminate_blink(coor_data_STANDARD, \n#                                                                coor_recording_STANDARD, 'STANDARD')\nplot_heatmap = False\nif plot_heatmap:\n    import plot\n    plot.plot_heatmap_coordinate(coor_data_LIBRE, density=False, screen_size=size, \n                                 title='LIBRE: The gaze from the beginning to the end')\n    plot.plot_heatmap_coordinate(coor_data_STANDARD, density=False, screen_size=size, \n                                 title='STANDARD: The gaze from the beginning to the end')\n</code></pre> <pre><code>The data will be eliminated: 11775\nAfter eliminating blinking: \nlen of coor_data_LIBRE 657900       should be equal to len of coor_recording 657900\n</code></pre> <pre><code># coor_data_LIBRE[coor_recording_LIBRE.blink &gt; 0]\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#23-preserving-fixation","title":"2.3 Preserving fixation","text":"<pre><code>use_preserve_fixation = True\n\ndef preserve_fixation(coor_data, coor_recording, seq_name):\n#     coor_data = coor_data[coor_recording.fixation &gt; 0]\n#     coor_recording = coor_recording[coor_recording.fixation &gt; 0]\n    print(f'The data will be eliminated: {len(coor_data[coor_recording.fixation &lt; 1])}')\n    coor_data.loc[coor_recording.fixation &lt; 1, ['x_coordinate', 'y_coordinate']] = np.nan\n    coor_recording.loc[coor_recording.fixation &lt; 1, ['x_coordinate', 'y_coordinate']] = np.nan\n\n    print(f'After preserving fixation: \\nlen of coor_data_{seq_name} {len(coor_data)} \\\n      should be equal to len of coor_recording {len(coor_recording)}')\n    return coor_data, coor_recording\n\n\nif use_preserve_fixation:\n    coor_data_LIBRE,  coor_recording_LIBRE = preserve_fixation(coor_data_LIBRE, coor_recording_LIBRE, 'LIBRE')\n    # coor_data_STANDARD,  coor_recording_STANDARD = preserve_fixation(coor_data_STANDARD, \n    #                                                                coor_recording_STANDARD, 'STANDARD')\nplot_heatmap = False\nif plot_heatmap:\n    import plot\n    plot.plot_heatmap_coordinate(coor_data_LIBRE, density=False, screen_size=size, \n                                 title='LIBRE: The gaze from the beginning to the end')\n    plot.plot_heatmap_coordinate(coor_data_STANDARD, density=False, screen_size=size, \n                                 title='STANDARD: The gaze from the beginning to the end')\n</code></pre> <pre><code>The data will be eliminated: 30428\nAfter preserving fixation: \nlen of coor_data_LIBRE 657900       should be equal to len of coor_recording 657900\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#24-plot-the-gazing-dots","title":"2.4 Plot the gazing dots","text":"<pre><code># see cleaned_cell [1]\n</code></pre> <pre><code># Example data (replace with your actual data)\nX_coord = coor_data_LIBRE_raw['x_coordinate']\nY_coord = coor_data_LIBRE_raw['y_coordinate']\nfig, ax= plt.subplots(figsize=(8, 6))\n# Plot the data, flipping X coordinates and using dots as markers\nplt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\nplt.xlim((0, 800))\nplt.ylim((0, 600))\n# Set plot title\nplt.title('LIBRE: Before filtering')\nplt.xlabel('x coordinate [pixels]')\nplt.ylabel('y coordinate [pixels]')\n# Reverse the direction of the Y-axis\nplt.gca().invert_yaxis()\nplt.gca().invert_xaxis()\n\n# ============================================================\n# Example data (replace with your actual data)\nX_coord = coor_data_LIBRE['x_coordinate']\nY_coord = coor_data_LIBRE['y_coordinate']\nfig, ax= plt.subplots(figsize=(8, 6))\n# Plot the data, flipping X coordinates and using dots as markers\nplt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\nplt.xlim((0, 800))\nplt.ylim((0, 600))\n# Set plot title\nplt.title('LIBRE: After Filtering')\nplt.xlabel('x coordinate [pixels]')\nplt.ylabel('y coordinate [pixels]')\n# Reverse the direction of the Y-axis\nplt.gca().invert_yaxis()\nplt.gca().invert_xaxis()\n# ============================================================\n</code></pre> <p>By far, we initially cleaned data based on the inherent properties in the metadata from EyeLink. And we plotted the fixation evaluation. </p> <pre><code># see cleaned_cell [2]\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#25-visualize-gaze-groups","title":"2.5 Visualize gaze groups","text":"<pre><code>def within_2d_range(x_vals, y_vals, x_range, y_range):\n    return np.sum((x_vals &gt;= x_range[0]) &amp; (x_vals &lt;= x_range[1]) &amp;\n                      (y_vals &gt;= y_range[0]) &amp; (y_vals &lt;= y_range[1]))\n\n\n\ndef label_correction(labels, counts, intervals):\n\n    interval_len = len(labels)\n    if intervals == 5000:\n        intervals = [intervals]*interval_len\n        print(intervals)\n    print(f'len of intervals {interval_len} in label_correction')\n    corrected_labels = []\n    for i, label in enumerate(labels):\n        if (label=='center') &amp; (i&gt;5) &amp; (i&lt;interval_len-1-5):\n            label='invalid'\n            print(f'index-{i}: label corrected, at this time there is no center stimulus ')\n        if counts[i]&lt;intervals[i]/2:\n            label='invalid'\n            print(f'index-{i}: label corrected, count of valid points is less than half of interval')\n        corrected_labels.append(label)\n\n    return corrected_labels\n</code></pre> <pre><code># see cleaned_cell [3]\n</code></pre> <p>Subjects saccaded from one position to another need a short delay, so the actual gaze points are not necessarily aligned with the stimuli occurring. We need to adjust a bit or tolerate such delay.</p> <p>Classification</p> <pre><code># More detailed classification\n# play with the timestamp and the first fixation point -&gt; finer subject delay.\nmetadata['LoggedMessages']\n\ndot_message_1 = \"ET: Start routine 'centered_dot'\"\ndot_message_2 = \"ET: Start routine 'dots'\"\ndot_message_3 = 'ET: dot moved!'\n\n\ndot_message = []\nfor index, element in enumerate(metadata['LoggedMessages']):\n    if (element[1] == dot_message_1) or (element[1] == dot_message_2) or (element[1] == dot_message_3):\n        dot_message.append(element)\n\nprint(f'total length: {len(dot_message)}')\n\nstart_timestamp = metadata['StartTime']\nfirst_dot_timestamp = dot_message[0][0]\ndot_message = [[timestamp - start_timestamp, msg] for timestamp, msg in dot_message]\n# dot_message = [[timestamp, msg] for timestamp, msg in dot_message]\nprint('Shifted')\nprint(dot_message[-1])\n\nprint(dot_message)\n</code></pre> <pre><code>total length: 131\nShifted\n[651901, \"ET: Start routine 'centered_dot'\"]\n[[498, \"ET: Start routine 'centered_dot'\"], [5489, \"ET: Start routine 'centered_dot'\"], [10489, \"ET: Start routine 'centered_dot'\"], [15490, \"ET: Start routine 'centered_dot'\"], [20506, \"ET: Start routine 'centered_dot'\"], [25506, \"ET: Start routine 'centered_dot'\"], [30506, \"ET: Start routine 'dots'\"], [35506, 'ET: dot moved!'], [40507, 'ET: dot moved!'], [45524, 'ET: dot moved!'], [50540, \"ET: Start routine 'dots'\"], [55557, 'ET: dot moved!'], [60573, 'ET: dot moved!'], [65574, 'ET: dot moved!'], [70590, \"ET: Start routine 'dots'\"], [75606, 'ET: dot moved!'], [80607, 'ET: dot moved!'], [85623, 'ET: dot moved!'], [90624, \"ET: Start routine 'dots'\"], [95640, 'ET: dot moved!'], [100657, 'ET: dot moved!'], [105658, 'ET: dot moved!'], [110677, \"ET: Start routine 'dots'\"], [115691, 'ET: dot moved!'], [120707, 'ET: dot moved!'], [125708, 'ET: dot moved!'], [130709, \"ET: Start routine 'dots'\"], [135725, 'ET: dot moved!'], [140743, 'ET: dot moved!'], [145758, 'ET: dot moved!'], [150776, \"ET: Start routine 'dots'\"], [155791, 'ET: dot moved!'], [160808, 'ET: dot moved!'], [165809, 'ET: dot moved!'], [170825, \"ET: Start routine 'dots'\"], [175841, 'ET: dot moved!'], [180842, 'ET: dot moved!'], [185859, 'ET: dot moved!'], [190876, \"ET: Start routine 'dots'\"], [195893, 'ET: dot moved!'], [200908, 'ET: dot moved!'], [205909, 'ET: dot moved!'], [210926, \"ET: Start routine 'dots'\"], [215927, 'ET: dot moved!'], [220942, 'ET: dot moved!'], [225944, 'ET: dot moved!'], [230960, \"ET: Start routine 'dots'\"], [235976, 'ET: dot moved!'], [240977, 'ET: dot moved!'], [245992, 'ET: dot moved!'], [250994, \"ET: Start routine 'dots'\"], [256010, 'ET: dot moved!'], [261028, 'ET: dot moved!'], [266043, 'ET: dot moved!'], [271061, \"ET: Start routine 'dots'\"], [276077, 'ET: dot moved!'], [281077, 'ET: dot moved!'], [286094, 'ET: dot moved!'], [291111, \"ET: Start routine 'dots'\"], [296127, 'ET: dot moved!'], [301127, 'ET: dot moved!'], [306144, 'ET: dot moved!'], [311145, \"ET: Start routine 'dots'\"], [316161, 'ET: dot moved!'], [321161, 'ET: dot moved!'], [326178, 'ET: dot moved!'], [331179, \"ET: Start routine 'dots'\"], [336195, 'ET: dot moved!'], [341211, 'ET: dot moved!'], [346228, 'ET: dot moved!'], [351229, \"ET: Start routine 'dots'\"], [356245, 'ET: dot moved!'], [361245, 'ET: dot moved!'], [366245, 'ET: dot moved!'], [371264, \"ET: Start routine 'dots'\"], [376278, 'ET: dot moved!'], [381295, 'ET: dot moved!'], [386296, 'ET: dot moved!'], [391313, \"ET: Start routine 'dots'\"], [396330, 'ET: dot moved!'], [401346, 'ET: dot moved!'], [406363, 'ET: dot moved!'], [411380, \"ET: Start routine 'dots'\"], [416396, 'ET: dot moved!'], [421413, 'ET: dot moved!'], [426415, 'ET: dot moved!'], [431431, \"ET: Start routine 'dots'\"], [436446, 'ET: dot moved!'], [441447, 'ET: dot moved!'], [446447, 'ET: dot moved!'], [451464, \"ET: Start routine 'dots'\"], [456480, 'ET: dot moved!'], [461497, 'ET: dot moved!'], [466514, 'ET: dot moved!'], [471531, \"ET: Start routine 'dots'\"], [476547, 'ET: dot moved!'], [481564, 'ET: dot moved!'], [486564, 'ET: dot moved!'], [491581, \"ET: Start routine 'dots'\"], [496597, 'ET: dot moved!'], [501598, 'ET: dot moved!'], [506614, 'ET: dot moved!'], [511632, \"ET: Start routine 'dots'\"], [516647, 'ET: dot moved!'], [521648, 'ET: dot moved!'], [526665, 'ET: dot moved!'], [531681, \"ET: Start routine 'dots'\"], [536698, 'ET: dot moved!'], [541715, 'ET: dot moved!'], [546715, 'ET: dot moved!'], [551716, \"ET: Start routine 'dots'\"], [556732, 'ET: dot moved!'], [561749, 'ET: dot moved!'], [566765, 'ET: dot moved!'], [571766, \"ET: Start routine 'dots'\"], [576782, 'ET: dot moved!'], [581782, 'ET: dot moved!'], [586798, 'ET: dot moved!'], [591800, \"ET: Start routine 'dots'\"], [596816, 'ET: dot moved!'], [601832, 'ET: dot moved!'], [606834, 'ET: dot moved!'], [611837, \"ET: Start routine 'dots'\"], [616850, 'ET: dot moved!'], [621866, 'ET: dot moved!'], [626883, 'ET: dot moved!'], [631902, \"ET: Start routine 'centered_dot'\"], [636901, \"ET: Start routine 'centered_dot'\"], [641902, \"ET: Start routine 'centered_dot'\"], [646901, \"ET: Start routine 'centered_dot'\"], [651901, \"ET: Start routine 'centered_dot'\"]]\n</code></pre> <pre><code>def classify_gaze_region_v2(X_coord, Y_coord, dot_message, sb_delay=0, display=False):\n    from collections import Counter\n\n    time_len = len(X_coord)\n    print(f'time length: {len(X_coord)}')\n    # label the dot position\n    x_left_range = [600,800]\n    x_right_range = [0, 200]\n    x_center_range = [300, 500]\n\n    y_up_range = [0, 200]\n    y_down_range = [400, 600]\n    y_center_range = [200, 400]\n    labels = []\n    counts = []\n\n\n    if dot_message == []:\n        interval = 5000\n        default_range = range(sb_delay, int(time_len), interval)\n        time_range = default_range\n        print('default time range')\n        intervals = [interval] * len(default_range)\n\n    else:\n        print(f'len of dot messages {len(dot_message)}')\n        dot_time_range = [item[0]+sb_delay for item in dot_message]\n        time_range = dot_time_range\n        time_range.append(dot_time_range[-1])\n        print('time range with ET message')\n\n        # Extract the time values (first elements of each pair)\n        time_values = [item[0] for item in dot_message]\n\n        # Compute the time intervals\n        intervals = [time_values[i] - time_values[i - 1] for i in range(1, len(time_values))]\n        intervals.append(5000)\n        print(f'Before appending: time duration of intervals: {sum(intervals)}')\n        for i in range(10):\n            if len(intervals) != len(time_range):\n                intervals.append(5000)\n                print('append +1 interval')\n            else:\n                break\n        print(\"Time Intervals:\", intervals)\n        print(f'len of intervals: {len(intervals)}')\n\n    print(f'length of time_range {len(time_range)}')\n\n    for start_time, interval in zip(time_range, intervals):\n        end_time = start_time + interval\n        x_interval = X_coord[start_time:end_time]\n        y_interval = Y_coord[start_time:end_time]\n\n        # Calculate counts for each region\n        left_count = within_2d_range(x_interval, y_interval, x_left_range, y_center_range)\n        right_count = within_2d_range(x_interval, y_interval, x_right_range, y_center_range)\n        center_count = within_2d_range(x_interval, y_interval, x_center_range, y_center_range)\n        up_count = within_2d_range(x_interval, y_interval, x_center_range, y_up_range)\n        down_count = within_2d_range(x_interval, y_interval, x_center_range, y_down_range)\n\n        region_counts = {\n            'left': left_count,\n            'right': right_count,\n            'center': center_count,\n            'up': up_count,\n            'down': down_count,\n        }\n\n        max_label, max_count = max(region_counts.items(), key=lambda item: item[1])\n        # Append the result\n        if max_count &gt; 0:\n            labels.append(max_label)\n            counts.append(max_count)\n        else:\n            labels.append('unknown')\n            counts.append(0)\n\n    if display:\n        for i, label in enumerate(labels):\n            print(f'Interval {i}-{(i+1)}: {label} -&gt; {counts[i]}')\n\n    return labels, counts, intervals\n</code></pre> <pre><code>import json\nX_coord = coor_data_LIBRE['x_coordinate']\nY_coord = coor_data_LIBRE['y_coordinate']\nlabels,counts, intervals = classify_gaze_region_v2(X_coord, Y_coord, dot_message, sb_delay=0, display=False)\n\nwith open(f'./labels/labels_subj_00{subject_idx}.json', 'w') as file:\n    json.dump(labels, file)\n\ncorr_labels = label_correction(labels, counts, intervals)\nwith open(f'./labels/corrected_labels_subj_00{subject_idx}.json', 'w') as file:\n    json.dump(corr_labels, file)\n\nsb_delay = 0\nnum_rows=33\nnum_cols=4\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(4*(num_cols+1), 3*(num_rows+1)))\naxes = axes.flatten()\ninvalid_indices = [i for i, element in enumerate(corr_labels) if element == \"invalid\"]\nprint(invalid_indices)\n\nfor i, ax in enumerate(axes):\n    if i&lt;len(dot_message):\n        x = X_coord[dot_message[i][0]+sb_delay:dot_message[i][0]+sb_delay+intervals[i]]\n        y = Y_coord[dot_message[i][0]+sb_delay:dot_message[i][0]+sb_delay+intervals[i]]\n    else:\n        x = X_coord[dot_message[-1][0]+sb_delay+intervals[-1]:]\n        y = Y_coord[dot_message[-1][0]+sb_delay+intervals[-1]:]\n\n    if i in invalid_indices:\n        ax.scatter(x, y, c='coral', alpha=0.1, edgecolors='coral')\n    else:\n        ax.scatter(x, y, c='#00468b', alpha=0.1, edgecolors='#00468b')\n    ax.set_title(f'{i}-{i+1}: {corr_labels[i]} -&gt; {counts[i]}')\n    ax.set_xlim([0, 800])\n    ax.set_ylim([0, 600])\n    ax.invert_xaxis()\n    ax.invert_yaxis()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>time length: 657900\nlen of dot messages 131\ntime range with ET message\nBefore appending: time duration of intervals: 656403\nappend +1 interval\nTime Intervals: [4991, 5000, 5001, 5016, 5000, 5000, 5000, 5001, 5017, 5016, 5017, 5016, 5001, 5016, 5016, 5001, 5016, 5001, 5016, 5017, 5001, 5019, 5014, 5016, 5001, 5001, 5016, 5018, 5015, 5018, 5015, 5017, 5001, 5016, 5016, 5001, 5017, 5017, 5017, 5015, 5001, 5017, 5001, 5015, 5002, 5016, 5016, 5001, 5015, 5002, 5016, 5018, 5015, 5018, 5016, 5000, 5017, 5017, 5016, 5000, 5017, 5001, 5016, 5000, 5017, 5001, 5016, 5016, 5017, 5001, 5016, 5000, 5000, 5019, 5014, 5017, 5001, 5017, 5017, 5016, 5017, 5017, 5016, 5017, 5002, 5016, 5015, 5001, 5000, 5017, 5016, 5017, 5017, 5017, 5016, 5017, 5000, 5017, 5016, 5001, 5016, 5018, 5015, 5001, 5017, 5016, 5017, 5017, 5000, 5001, 5016, 5017, 5016, 5001, 5016, 5000, 5016, 5002, 5016, 5016, 5002, 5003, 5013, 5016, 5017, 5019, 4999, 5001, 4999, 5000, 5000, 5000]\nlen of intervals: 132\nlength of time_range 132\nlen of intervals 132 in label_correction\nindex-74: label corrected, count of valid points is less than half of interval\nindex-91: label corrected, at this time there is no center stimulus \nindex-109: label corrected, count of valid points is less than half of interval\nindex-115: label corrected, count of valid points is less than half of interval\n[74, 91, 109, 115]\n</code></pre> <p></p> <pre><code>def cal_diff_distance(x_coord, y_coord):\n\n    # Calculate differences\n    # * 81.3 / 176\n    # * 62 / 137\n    dx = np.diff(x_coord)  # Difference in x\n    dy = np.diff(y_coord)   # Difference in y\n\n    # Check for NaN values, ravel() is the must\n\n    nan_mask = (\n    np.isnan(x_coord[:-1]).ravel() |\n    np.isnan(x_coord[1:]).ravel()  |\n    np.isnan(y_coord[:-1]).ravel() |\n    np.isnan(y_coord[1:]).ravel()\n    )\n\n    # Calculate Euclidean distances\n    distances = np.sqrt(dx**2 + dy**2)\n    # Apply NaN mask\n\n    distances[nan_mask] = np.nan\n\n\n    return distances\n\ndef cal_diff_sum_window(x_coord, y_coord, window_size=7):\n    # Calculate differences\n    # * 81.3 / 176\n    # * 62 / 137\n    distances = cal_diff_distance(x_coord, y_coord)\n    half_window = window_size // 2\n\n    # Create an empty array to store the sum of each window\n    window_sums = []\n    for i in range(len(x_coord)):\n        # Get the start and end indices of the window\n        start_idx = max(0, i - half_window)\n        end_idx = min(len(distances), i + half_window + 1)\n\n        # Slice the array and calculate the sum of the window\n        window_sum = np.nansum(distances[start_idx:end_idx])\n\n        # Append the window sum to the result list\n        window_sums.append(window_sum)\n\n    # Convert the list of sums to a numpy array for easier manipulation\n    window_sums = np.array(window_sums)\n\n    return window_sums\n\n\ndef calculate_max_pairwise_distance(x_coord, y_coord):\n    \"\"\"\n    Calculate the maximum Euclidean distance between each pair of coordinates.\n\n    Args:\n    coords (numpy.ndarray): An array of shape (n, 2), where n is the number of coordinates, \n                            and each row contains (x, y) coordinates.\n\n    Returns:\n    float: The maximum distance between any pair of coordinates.\n    \"\"\"\n\n    coords = np.column_stack((x_coord, y_coord))\n    diffs = coords[:, np.newaxis, :] - coords[np.newaxis, :, :]  # Shape (n, n, 2)\n\n    # Compute Euclidean distances for all pairs\n    distances = np.sqrt(np.nansum(diffs**2, axis=-1))  # Shape (n, n)\n\n    # Find the maximum distance\n    max_distance = np.nanmax(distances)\n\n    return max_distance\n\ndef cal_max_distance_window(x_coord, y_coord, window_size=7):\n    from tqdm import tqdm\n    half_window = window_size // 2\n\n    # Create an empty array to store the sum of each window\n    window_max_array = []\n    for i in tqdm(range(len(x_coord)), desc=\"Processing\"):\n\n        # Get the start and end indices of the window\n        start_idx = max(0, i - half_window)\n        end_idx = min(len(x_coord), i + half_window + 1)\n\n        # Slice the array and calculate the sum of the window\n        window_max = calculate_max_pairwise_distance(x_coord[start_idx:end_idx], y_coord[start_idx:end_idx])\n\n        # Append the window sum to the result list\n        window_max_array.append(window_max)\n\n    # Convert the list of sums to a numpy array for easier manipulation\n    window_max_array = np.array(window_max_array)\n\n    return window_max_array\n</code></pre> <pre><code>X_coord = coor_data_LIBRE['x_coordinate']\nY_coord = coor_data_LIBRE['y_coordinate']\n\nt_axis_xy = (\n    coor_data.index \n) / metadata[\"SamplingFrequency\"]\ndistances = cal_diff_distance(X_coord, Y_coord)\nprint(distances.shape)\n\nfig, ax= plt.subplots(figsize=(8, 4))\nplt.plot(t_axis_xy[1:],\n         distances)\n\n\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.xlabel('time')\nplt.ylabel('distance (pixel)')\n</code></pre> <pre><code>(657899,)\n\n\n\n\n\nText(0, 0.5, 'distance (pixel)')\n</code></pre> <p></p> <pre><code>fig, ax= plt.subplots(figsize=(10, 4))\ndistances_win = cal_diff_sum_window(X_coord, Y_coord, window_size=7*2)\nprint(distances_win.shape)\nplt.plot(t_axis_xy,\n         distances_win)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.xlabel('time')\nplt.ylabel('distance (pixel)')\n</code></pre> <pre><code>(657900,)\n\n\n\n\n\nText(0, 0.5, 'distance (pixel)')\n</code></pre> <p></p> <pre><code>fig, ax= plt.subplots(figsize=(10, 4))\nwindow_size = 80\ndistances_max_win = cal_max_distance_window(X_coord, Y_coord, window_size=window_size)\nprint(distances_win.shape)\nplt.plot(t_axis_xy,\n         distances_max_win)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.xlabel('time')\nplt.ylabel('distance (pixel)')\n</code></pre> <pre><code>Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 657900/657900 [01:24&lt;00:00, 7772.62it/s]\n\n\n(657900,)\n\n\n\n\n\nText(0, 0.5, 'distance (pixel)')\n</code></pre> <p></p> <pre><code>save_mask_path = f'./masks_1206/subject_{subject_idx}_max_dist_win_{window_size}.mat'\nsio.savemat(save_mask_path, {'distances_max_win': distances_max_win})\nprint(f'The mask file has been saved here: {save_mask_path}')\nprint(len(distances_max_win[distances_max_win&gt;60]))\nprint(len(distances_max_win))\n</code></pre> <pre><code>The mask file has been saved here: ./masks_1206/subject_1_max_dist_win_80.mat\n2946\n657900\n</code></pre> <pre><code>def plt_distribution(distances, threshold=0.8):\n    import seaborn as sns\n\n    # Filter out NaN values from the distances\n    valid_distances = distances[~np.isnan(distances)]\n\n    # Plot the distribution: KDE\n    fig, ax= plt.subplots(figsize=(8, 4))\n\n    kde_plot = sns.kdeplot(valid_distances, color='coral', linewidth=2, label='KDE Plot')\n    # Get the x and y values from the plot (not the plot object itself)\n    x_values = kde_plot.get_lines()[0].get_xdata()\n    kde_values = kde_plot.get_lines()[0].get_ydata()\n    # Add labels and legend\n    ax.set_xlabel('Distance', fontsize=14)\n    ax.set_ylabel('Density', fontsize=14)\n    ax.set_title('Distribution of Distances', fontsize=16)\n\n\n    # Plot the cdf values\n\n    cdf_values = np.cumsum(kde_values)  # Cumulative sum\n    # Normalize the CDF to range from 0 to 1\n    cdf_values /= cdf_values[-1]\n    x_values = np.linspace(min(valid_distances), max(valid_distances), len(cdf_values))  # Define x range\n    ax.plot(x_values, cdf_values, label='CDF', color='blue')\n    # Normalize the CDF to range from 0 to 1\n    cdf_values /= cdf_values[-1]\n    # Find the threshold value for 80% accumulation\n    threshold_index = np.argmax(cdf_values &gt;= threshold)\n    threshold_value = x_values[threshold_index]\n\n    ax.axvline(threshold_value, color='green', linestyle='--', label=f'80% Threshold: {threshold_value:.2f}')\n\n\n    # Show plot\n    plt.show()\n    return threshold_value\n\nthreshold_value = plt_distribution(distances, threshold=0.8)\nprint(threshold_value)\n\nthreshold_value = plt_distribution(distances_win, threshold=0.8)\nprint(threshold_value)\n\nthreshold_value = plt_distribution(distances_max_win, threshold=0.9)\nprint(threshold_value)\n</code></pre> <p></p> <pre><code>1.748739720799676\n</code></pre> <p></p> <pre><code>21.51341017672875\n</code></pre> <p></p> <pre><code>51.6691085454658\n</code></pre> <pre><code>def cal_statistics(distances):\n    mean_distance = np.nanmean(distances)\n    std_distance = np.nanstd(distances)\n    median_distance = np.nanmedian(distances)\n    min_distance = np.nanmin(distances)\n    max_distance = np.nanmax(distances)\n    nan_count = np.sum(np.isnan(distances))  # Count NaN values\n\n    # Display the results\n    print(f\"Mean distance: {mean_distance:.2f}\")\n    print(f\"Standard deviation: {std_distance:.2f}\")\n    print(f\"Median distance: {median_distance:.2f}\")\n    print(f\"Minimum distance: {min_distance:.2f}\")\n    print(f\"Maximum distance: {max_distance:.2f}\")\n    print(f\"Number of NaN values: {nan_count}\")\n\ncal_statistics(distances)\ncal_statistics(distances_win)\ncal_statistics(distances_max_win)\n</code></pre> <pre><code>Mean distance: 1.12\nStandard deviation: 0.71\nMedian distance: 1.00\nMinimum distance: 0.00\nMaximum distance: 5.80\nNumber of NaN values: 30960\nMean distance: 16.07\nStandard deviation: 7.00\nMedian distance: 15.38\nMinimum distance: 0.00\nMaximum distance: 51.58\nNumber of NaN values: 0\nMean distance: 10.15\nStandard deviation: 14.86\nMedian distance: 8.77\nMinimum distance: 0.00\nMaximum distance: 360.06\nNumber of NaN values: 0\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#generate-movement-distance-mask","title":"Generate movement distance mask","text":"<pre><code>def generate_movement_mask(distances_max_win, offset, move_criteria):\n    # Generate the mask\n    # the threshold should be within voxel size, or some relationship with voxel size.\n    distances_max_win = distances_max_win[offset:]\n\n    move_discard_mask = (\n        (distances_max_win &gt; move_criteria) | np.isnan(distances_max_win)\n        )\n    move_preserve_mask = ~(move_discard_mask)\n\n\n    # Check the result\n    print(\"Mask shape:\", move_preserve_mask.shape)\n\n    return move_preserve_mask\n\n\n# offset_first_trigger_mriStart = first trigger interval (according to log.txt) - (first mri meas - first marker)\nif subject_idx == 1:\n    offset_first_trigger_mriStart = round(1989.4 - 247.5)\nelif subject_idx == 2:\n    offset_first_trigger_mriStart = round(2006.6 - 1985)\nelse:\n    offset_first_trigger_mriStart = round(1994 - 47.5)\n\nmove_criteria = 60\noffset = offset_first_trigger_mriStart\nmove_preserve_mask = generate_movement_mask(distances_max_win, offset, move_criteria)\n\nsave_move_preserve_mask = True\nif save_move_preserve_mask:\n    save_mask_path = f'./masks_1206/subject_{subject_idx}_move_win{window_size}_crit{move_criteria}_mask.mat'\n    sio.savemat(save_mask_path, {'move_preserve_mask': move_preserve_mask})\n    print(f'The mask file has been saved here: {save_mask_path}')\n</code></pre> <pre><code>Mask shape: (656158,)\nThe mask file has been saved here: ./masks_1206/subject_1_move_win80_crit60_mask.mat\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#generate-the-raw-masks-5-masks","title":"Generate the raw masks: 5 masks","text":"<p>According to the stimuli intervals</p> <pre><code>see_raw=False\n\nif see_raw:\n    coor_data_dot_raw = copy.deepcopy(coor_data_LIBRE_raw)\n\n    print(len(coor_data_dot_raw))\n\n    coor_data_dot_raw = coor_data_dot_raw.reset_index(drop=True)\n\n\n    if subject_idx == 1:\n        offset_first_dot_mriStart = 1244 #1243.9 -&gt; 1244\n    elif subject_idx == 2:\n        offset_first_dot_mriStart = -475\n    else:\n        offset_first_dot_mriStart= 1451 #1450.5 -&gt; 1451\n\n    # I want to generate a table, concatenate the labels along with the recordings.\n    # for example, I have 657402 coordinates pandas dataframe, and I have subsequent 131 intervals and 131 labels,\n    # I want to add the labels to the dataframe as a new column according to the intervals.\n\n\n\n    # Initialize a new column for labels\n    coor_data_dot_raw['label'] = None\n\n    # Assign labels based on intervals\n    for i, dot_m in enumerate(dot_message):\n        start = dot_m[0]\n        end = start+intervals[i]\n        coor_data_dot_raw.loc[start:end, 'label'] = labels[i]\n\n    # Check result\n    # print(coor_data_dot_raw[dot_message[19][0]:dot_message[19][0]+intervals[19]])\n</code></pre> <pre><code>if see_raw:\n    coor_data_dot_offset = coor_data_dot_raw[offset_first_dot_mriStart:].reset_index(drop=True)\n    mri_duration = 655240\n    coor_data_dot_offset = coor_data_dot_offset[:mri_duration]\n    print(len(coor_data_dot_offset))\n    print(coor_data_dot_offset)\n    unique_labels = coor_data_dot_offset[\"label\"].unique()\n    print(\"Unique labels:\", unique_labels)\n    coor_data_dot_offset[coor_data_dot_offset[\"label\"] == 'center'][-2000:-1990]\n</code></pre> <pre><code>def generate_raw_mask(coor_data, label_type='label', label='up'):\n    # Generate the mask\n    raw_mask = np.where(coor_data[label_type] == label, 1, 0)\n\n    # Check the result\n    print(\"Mask:\", raw_mask)\n    print(\"Mask shape:\", raw_mask.shape)\n\n\n    return raw_mask\n\nif see_raw:\n    # Generate 5 raw mask\n    import os\n    import scipy.io as sio\n\n\n\n    label_list = ['up', 'down', 'left', 'right', 'center', 'invalid']\n    raw_mask_5p = []\n    raw_mask_up = generate_raw_mask(coor_data_dot_offset, label_type='label', label=label_list[0])\n    raw_mask_5p.append(raw_mask_up)\n\n    raw_mask_down = generate_raw_mask(coor_data_dot_offset, label_type='label', label=label_list[1])\n    raw_mask_5p.append(raw_mask_down)\n\n    raw_mask_left = generate_raw_mask(coor_data_dot_offset, label_type='label', label=label_list[2])\n    raw_mask_5p.append(raw_mask_left)\n\n    raw_mask_right = generate_raw_mask(coor_data_dot_offset,label_type='label', label=label_list[3])\n    raw_mask_5p.append(raw_mask_right)\n\n    raw_mask_center = generate_raw_mask(coor_data_dot_offset,label_type='label', label=label_list[4])\n    raw_mask_5p.append(raw_mask_center)\n\n    coor_data_dot_offset[raw_mask_center.astype(bool)]\n\n    save_mask = True\n    if save_mask:\n        for m_idx in range(5):\n            save_mask_path = f'./masks/subject_{subject_idx}_raw_mask_{label_list[m_idx]}.mat'\n            sio.savemat(save_mask_path, {'array': raw_mask_5p[m_idx]})\n            print(f'The mask file has been saved here: {save_mask_path}')\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#generate-the-location-masks-5-masks","title":"Generate the location masks: 5 masks","text":"<p>According to the absolute locations</p> <pre><code>coor_data_dot_raw = copy.deepcopy(coor_data_LIBRE_raw)\nprint(len(coor_data_dot_raw))\ncoor_data_dot_raw = coor_data_dot_raw.reset_index(drop=True)\n\n# offset_first_trigger_mriStart = first trigger interval (according to log.txt) - (first mri meas - first marker)\nif subject_idx == 1:\n    offset_first_trigger_mriStart = round(1989.4 - 247.5)\nelif subject_idx == 2:\n    offset_first_trigger_mriStart = round(2006.6 - 1985)\nelse:\n    offset_first_trigger_mriStart = round(1994 - 47.5)\n\n# Initialize a new column for labels\ncoor_data_dot_raw['lc_label'] = None\n\n\ncoor_data_dot_offset = coor_data_dot_raw[offset_first_trigger_mriStart:].reset_index(drop=True)\nmri_duration = 655240\n\ncoor_data_dot_offset = coor_data_dot_offset[:]\nprint(len(coor_data_dot_offset))\nprint(coor_data_dot_offset)\n</code></pre> <pre><code>657900\n656158\n        x_coordinate  y_coordinate lc_label\n0         402.500000    300.700012     None\n1         402.899994    299.799988     None\n2         403.000000    300.000000     None\n3         402.399994    302.200012     None\n4         401.700012    304.299988     None\n...              ...           ...      ...\n656153           NaN           NaN     None\n656154           NaN           NaN     None\n656155           NaN           NaN     None\n656156           NaN           NaN     None\n656157           NaN           NaN     None\n\n[656158 rows x 3 columns]\n</code></pre> <pre><code># Define the ranges\n# x_left_range = [500, 800]\n# x_right_range = [0, 300]\n# x_center_range = [300, 500]\n\n# y_up_range = [0, 200]\n# y_down_range = [400, 600]\n# y_center_range = [200, 400]\n\nleft_range_x = [450, 800]\nleft_range_y = [150, 450]\nright_range_x = [0, 350]\nright_range_y = [150, 450]\n\nup_range_x = [225, 575]\nup_range_y = [0, 300]\ndown_range_x = [225, 575]\ndown_range_y = [300, 600]\n\ncenter_range_x = [300, 500]\ncenter_range_y = [200, 400]\n\n\n# Function to classify coordinates\ndef classify_region(x, y):\n    if (up_range_x[0] &lt;= x &lt;= up_range_x[1])&amp;(up_range_y[0] &lt;= y &lt;= up_range_y[1]):\n        return \"up\"\n    elif (down_range_x[0] &lt;= x &lt;= down_range_x[1])&amp;(down_range_y[0] &lt;= y &lt;= down_range_y[1]):\n        return \"down\"\n    elif (center_range_x[0] &lt;= x &lt;= center_range_x[1])&amp;(center_range_y[0] &lt;= y &lt;= center_range_y[1]):\n        return \"center\"\n    elif (left_range_x[0] &lt;= x &lt;= left_range_x[1])&amp;(left_range_y[0] &lt;= y &lt;= left_range_y[1]):\n        return \"left\"\n    elif (right_range_x[0] &lt;= x &lt;= right_range_x[1])&amp;(right_range_y[0] &lt;= y &lt;= right_range_y[1]):\n        return \"right\"\n    else:        \n        return \"invalid\"\n\n# Apply the function to the DataFrame\ncoor_data_dot_offset['lc_label'] = coor_data_dot_offset.apply(lambda row: classify_region(row['x_coordinate'], row['y_coordinate']), axis=1)\n\n# Display the resulting DataFrame\nprint(coor_data_dot_offset)\nnan_counts_per_column = coor_data_dot_offset.isna().sum()\nprint(nan_counts_per_column)\n</code></pre> <pre><code>        x_coordinate  y_coordinate lc_label\n0         402.500000    300.700012     down\n1         402.899994    299.799988       up\n2         403.000000    300.000000       up\n3         402.399994    302.200012     down\n4         401.700012    304.299988     down\n...              ...           ...      ...\n656153           NaN           NaN  invalid\n656154           NaN           NaN  invalid\n656155           NaN           NaN  invalid\n656156           NaN           NaN  invalid\n656157           NaN           NaN  invalid\n\n[656158 rows x 3 columns]\nx_coordinate    8449\ny_coordinate    8802\nlc_label           0\ndtype: int64\n</code></pre> <pre><code># Generate 5 raw mask\nimport os\nimport scipy.io as sio\n\n\nlabel_list = ['up', 'down', 'left', 'right', 'center', 'invalid']\nraw_mask_lc = []\nraw_mask_up = generate_raw_mask(coor_data_dot_offset, label_type='lc_label', label=label_list[0])\nprint(np.sum(raw_mask_up))\nraw_mask_lc.append(raw_mask_up)\n\nraw_mask_down = generate_raw_mask(coor_data_dot_offset, label_type='lc_label', label=label_list[1])\nprint(np.sum(raw_mask_down))\nraw_mask_lc.append(raw_mask_down)\n\nraw_mask_left = generate_raw_mask(coor_data_dot_offset, label_type='lc_label', label=label_list[2])\nprint(np.sum(raw_mask_left))\nraw_mask_lc.append(raw_mask_left)\n\nraw_mask_right = generate_raw_mask(coor_data_dot_offset, label_type='lc_label', label=label_list[3])\nprint(np.sum(raw_mask_right))\nraw_mask_lc.append(raw_mask_right)\n\nraw_mask_center = generate_raw_mask(coor_data_dot_offset, label_type='lc_label', label=label_list[4])\nprint(np.sum(raw_mask_center))\nraw_mask_lc.append(raw_mask_center)\n\ncoor_data_dot_offset[raw_mask_center.astype(bool)]\n\nsave_mask = True\nif save_mask:\n    for m_idx in range(5):\n        save_mask_path = f'./masks/subject_{subject_idx}_lc_mask_{label_list[m_idx]}.mat'\n        sio.savemat(save_mask_path, {'array': raw_mask_lc[m_idx]})\n        print(f'The mask file has been saved here: {save_mask_path}')\n</code></pre> <pre><code>Mask: [0 1 1 ... 0 0 0]\nMask shape: (656158,)\n186531\nMask: [1 0 0 ... 0 0 0]\nMask shape: (656158,)\n173746\nMask: [0 0 0 ... 0 0 0]\nMask shape: (656158,)\n146410\nMask: [0 0 0 ... 0 0 0]\nMask shape: (656158,)\n140273\nMask: [0 0 0 ... 0 0 0]\nMask shape: (656158,)\n0\nThe mask file has been saved here: ./masks/subject_1_lc_mask_up.mat\nThe mask file has been saved here: ./masks/subject_1_lc_mask_down.mat\nThe mask file has been saved here: ./masks/subject_1_lc_mask_left.mat\nThe mask file has been saved here: ./masks/subject_1_lc_mask_right.mat\nThe mask file has been saved here: ./masks/subject_1_lc_mask_center.mat\n</code></pre> <pre><code>for idx in range(5):\n    # Example data (replace with your actual data)\n    if see_raw:\n        X_coord = coor_data_dot_offset['x_coordinate']*raw_mask_5p[idx]\n        Y_coord = coor_data_dot_offset['y_coordinate']*raw_mask_5p[idx]\n        fig, ax= plt.subplots(figsize=(4, 3))\n        # Plot the data, flipping X coordinates and using dots as markers\n        plt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\n        plt.xlim((0, 800))\n        plt.ylim((0, 600))\n        # Set plot title\n        plt.title('Interval classification')\n        plt.xlabel('x coordinate [pixels]')\n        plt.ylabel('y coordinate [pixels]')\n        # Reverse the direction of the Y-axis\n        plt.gca().invert_yaxis()\n        plt.gca().invert_xaxis()\n\n    # ============================================================\n    # Example data (replace with your actual data)\n    X_coord = coor_data_dot_offset['x_coordinate']*raw_mask_lc[idx]\n    Y_coord = coor_data_dot_offset['y_coordinate']*raw_mask_lc[idx]\n\n    fig, ax= plt.subplots(figsize=(4, 3))\n    # Plot the data, flipping X coordinates and using dots as markers\n    plt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\n    plt.xlim((0, 800))\n    plt.ylim((0, 600))\n    # Set plot title\n    plt.title('Location classification')\n    plt.xlabel('x coordinate [pixels]')\n    plt.ylabel('y coordinate [pixels]')\n    # Reverse the direction of the Y-axis\n    plt.gca().invert_yaxis()\n    plt.gca().invert_xaxis()\n    # ============================================================\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#generate-the-filtered-5-mask","title":"Generate the filtered 5 mask","text":"<p>based on the filtered coordinate</p> <pre><code># def find_mean(x)\n\ncoor_data_dot_filtered = copy.deepcopy(coor_data_LIBRE)\nprint(len(coor_data_dot_raw))\ncoor_data_dot_filtered = coor_data_dot_filtered.reset_index(drop=True)\n\nif subject_idx == 1:\n    offset_first_trigger_mriStart = round(1989.4 - 247.5)\nelif subject_idx == 2:\n    offset_first_trigger_mriStart = round(2006.6 - 1985)\nelse:\n    offset_first_trigger_mriStart = round(1994 - 47.5)\n\ncoor_data_dot_filtered_offset = coor_data_dot_filtered[offset_first_trigger_mriStart:].reset_index(drop=True)\nmri_duration = 655240\ncoor_data_dot_filtered_offset = coor_data_dot_filtered_offset[:]\nprint(len(coor_data_dot_filtered_offset))\n\n\nnan_counts_per_column = coor_data_dot_filtered_offset.isna().sum()\nprint(f\"\\nnum of nan:\\n{nan_counts_per_column} \\n\")\n\n# Ensuring both coordinates are NaN if either is NaN\ncoor_data_dot_filtered_offset[['x_coordinate', 'y_coordinate']] = coor_data_dot_filtered_offset[['x_coordinate', 'y_coordinate']].apply(\n    lambda row: row if not row.isna().any() else [np.nan, np.nan], axis=1\n)\nnan_counts_per_column = coor_data_dot_filtered_offset.isna().sum()\nprint(f\"\\nnum of nan:\\n{nan_counts_per_column} \\n\")\n\n\ncoor_data_dot_filtered_offset['lc_label'] = coor_data_dot_offset['lc_label']\nprint(f'{len(coor_data_dot_filtered_offset)} -- {len(coor_data_dot_offset)}')\n# print(coor_data_dot_filtered_offset)\n\n# Here I want to clean all the NaN elements with label \"invalid\"\nprint(f\"before processing: {len(coor_data_dot_filtered_offset[coor_data_dot_filtered_offset['lc_label'] == 'invalid'])}\")\ncoor_data_dot_filtered_offset.loc[coor_data_dot_filtered_offset.isna().any(axis=1), 'lc_label'] = \"invalid\"\nprint(f\"after processing: {len(coor_data_dot_filtered_offset[coor_data_dot_filtered_offset['lc_label'] == 'invalid'])}\")\n</code></pre> <pre><code>657900\n656158\n\nnum of nan:\nx_coordinate    30344\ny_coordinate    30344\ndtype: int64\n\n\nnum of nan:\nx_coordinate    30344\ny_coordinate    30344\ndtype: int64\n\n656158 -- 656158\nbefore processing: 9198\nafter processing: 30344\n</code></pre> <pre><code>label_list = ['up', 'down', 'left', 'right', 'center', 'invalid']\nft_mask_list = []\nft_mask_up = generate_raw_mask(coor_data_dot_filtered_offset, label_type='lc_label', label=label_list[0])\nprint(np.sum(ft_mask_up))\nft_mask_list.append(ft_mask_up)\n\nft_mask_down = generate_raw_mask(coor_data_dot_filtered_offset, label_type='lc_label', label=label_list[1])\nprint(np.sum(ft_mask_down))\nft_mask_list.append(ft_mask_down)\n\nft_mask_left = generate_raw_mask(coor_data_dot_filtered_offset, label_type='lc_label', label=label_list[2])\nprint(np.sum(ft_mask_left))\nft_mask_list.append(ft_mask_left)\n\nft_mask_right = generate_raw_mask(coor_data_dot_filtered_offset, label_type='lc_label', label=label_list[3])\nprint(np.sum(ft_mask_right))\nft_mask_list.append(ft_mask_right)\n\nft_mask_center = generate_raw_mask(coor_data_dot_filtered_offset, label_type='lc_label', label=label_list[4])\nprint(np.sum(ft_mask_center))\nft_mask_list.append(ft_mask_center)\n\ncoor_data_dot_filtered_offset[ft_mask_center.astype(bool)]\n\nsave_mask = True\nif save_mask:\n    for m_idx in range(5):\n        save_mask_path = f'./masks/subject_{subject_idx}_ft_mask_{label_list[m_idx]}.mat'\n        sio.savemat(save_mask_path, {'array': ft_mask_list[m_idx]})\n        print(f'The mask file has been saved here: {save_mask_path}')\n</code></pre> <pre><code>Mask: [0 1 1 ... 0 0 0]\nMask shape: (656158,)\n179497\nMask: [1 0 0 ... 0 0 0]\nMask shape: (656158,)\n168256\nMask: [0 0 0 ... 0 0 0]\nMask shape: (656158,)\n141776\nMask: [0 0 0 ... 0 0 0]\nMask shape: (656158,)\n136285\nMask: [0 0 0 ... 0 0 0]\nMask shape: (656158,)\n0\nThe mask file has been saved here: ./masks/subject_1_ft_mask_up.mat\nThe mask file has been saved here: ./masks/subject_1_ft_mask_down.mat\nThe mask file has been saved here: ./masks/subject_1_ft_mask_left.mat\nThe mask file has been saved here: ./masks/subject_1_ft_mask_right.mat\nThe mask file has been saved here: ./masks/subject_1_ft_mask_center.mat\n</code></pre> <pre><code>for idx in range(5):\n    # ============================================================\n    # Example data (replace with your actual data)\n    X_coord = coor_data_dot_offset['x_coordinate']*raw_mask_lc[idx]\n    Y_coord = coor_data_dot_offset['y_coordinate']*raw_mask_lc[idx]\n\n    fig, ax= plt.subplots(figsize=(4, 3))\n    # Plot the data, flipping X coordinates and using dots as markers\n    plt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\n    plt.xlim((0, 800))\n    plt.ylim((0, 600))\n    # Set plot title\n    plt.title('Location classification')\n    plt.xlabel('x coordinate [pixels]')\n    plt.ylabel('y coordinate [pixels]')\n    # Reverse the direction of the Y-axis\n    plt.gca().invert_yaxis()\n    plt.gca().invert_xaxis()\n\n    # ============================================================\n    # Example data (replace with your actual data)\n    X_coord = coor_data_dot_filtered_offset['x_coordinate']*ft_mask_list[idx]\n    Y_coord = coor_data_dot_filtered_offset['y_coordinate']*ft_mask_list[idx]\n    fig, ax= plt.subplots(figsize=(4, 3))\n    # Plot the data, flipping X coordinates and using dots as markers\n    plt.scatter(X_coord, Y_coord, s=50, c='#00468b', alpha=0.1, edgecolors='#00468b', linewidth=0.5)\n    plt.xlim((0, 800))\n    plt.ylim((0, 600))\n    # Set plot title\n    plt.title('Filtered classification')\n    plt.xlabel('x coordinate [pixels]')\n    plt.ylabel('y coordinate [pixels]')\n    # Reverse the direction of the Y-axis\n    plt.gca().invert_yaxis()\n    plt.gca().invert_xaxis()\n\n    np.sum(raw_mask_lc[0] != ft_mask_list[0] )\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#filtered-the-coor_data_dot_filtered_offset","title":"Filtered the coor_data_dot_filtered_offset","text":"<p>Region by the gaze points</p> <pre><code>from mask_clean import find_mean_position, cal_angles, cal_disp, filter_criteria\nfrom mask_clean import filter_XY_with_mask, visualization_func, plot_h_v_disp\n\n# left mask\ncoor_data_left = copy.deepcopy(coor_data_dot_filtered_offset)\n\ncoor_data_left['x_coordinate'] = coor_data_left['x_coordinate']*ft_mask_list[2]\ncoor_data_left['y_coordinate'] = coor_data_left['y_coordinate']*ft_mask_list[2]\nmask_zero = (coor_data_left['x_coordinate'] == 0) &amp; (coor_data_left['y_coordinate'] == 0)\ncoor_data_left.loc[mask_zero, ['x_coordinate'] ] = np.nan\ncoor_data_left.loc[mask_zero, ['y_coordinate'] ] = np.nan\n\nX_coord_left = coor_data_left['x_coordinate']\nY_coord_left = coor_data_left['y_coordinate']\n# print(X_coord_left)\n# print(Y_coord_left)\nplt.scatter(X_coord_left, Y_coord_left)\n\nmed_coor_left = find_mean_position(X_coord_left, Y_coord_left)\ntheta_h_, theta_h_m, rho_v_, rho_v_m = cal_angles(X_coord_left, Y_coord_left, med_coor_left)\nprint(len(theta_h_))\nh_dis_left, v_dis_left = cal_disp(theta_h_, theta_h_m, rho_v_, rho_v_m)\nprint(len(v_dis_left))\n\n# print(v_dis_left)\ncriteria_ratio=0.3\ndiscarded_x_mask, discarded_y_mask = filter_criteria(h_dis_left, v_dis_left, criteria_ratio=criteria_ratio)\n# preserve_mask = ~(discarded_x_mask | discarded_y_mask)\n</code></pre> <pre><code>To find the median coor of the point clouds\nThe mean position of the gaze region: [683.4000244140625, 283.70001220703125] (px)\nTo calculate visual angles w.r.t. points\n656158\nTo calculate the displacement in both directions (mm)\n656158\nTo generate discard masks for x and y direction...\n</code></pre> <p></p> <pre><code>plot_h_v_disp(h_dis_left, v_dis_left, discarded_x_mask, \n              discarded_y_mask, criteria_ratio=criteria_ratio)\n</code></pre> <p></p> <p></p> <pre><code>coor_data_left_clean, Preserve_mask, Discard_mask = filter_XY_with_mask(coor_data_left, discarded_x_mask, \n                                                                        discarded_y_mask, seq_name=None)\n</code></pre> <p></p> <pre><code>print('\\n====\\nx_coordinate --- statistics')\ncal_statistics(coor_data_left_clean['x_coordinate'])\nprint('\\n====\\ny_coordinate --- statistics')\ncal_statistics(coor_data_left_clean['y_coordinate'])\nthreshold_value_x = plt_distribution(coor_data_left_clean['x_coordinate'], threshold=0.8)\nthreshold_value_y = plt_distribution(coor_data_left_clean['y_coordinate'], threshold=0.8)\n</code></pre> <pre><code>====\nx_coordinate --- statistics\nMean distance: 683.69\nStandard deviation: 13.11\nMedian distance: 683.10\nMinimum distance: 655.10\nMaximum distance: 711.80\nNumber of NaN values: 527290\n\n====\ny_coordinate --- statistics\nMean distance: 284.47\nStandard deviation: 11.11\nMedian distance: 283.50\nMinimum distance: 255.20\nMaximum distance: 312.20\nNumber of NaN values: 527290\n</code></pre> <p></p> <p></p> <pre><code># Visualization of filtered coor data LIBRE\ncoor_data_raw_left = copy.deepcopy(coor_data_dot_offset)\n\ncoor_data_raw_left['x_coordinate'] = coor_data_raw_left['x_coordinate']*raw_mask_lc[2]\ncoor_data_raw_left['y_coordinate'] = coor_data_raw_left['y_coordinate']*raw_mask_lc[2]\nprint(f'sum of raw mask: {sum(raw_mask_lc[2])}')\nprint(f'sum of filtered mask: {sum(ft_mask_list[2])}')\nprint(f'sum of mask clean: {sum(Preserve_mask)}')\nvisualization_func(fig_title='Before vs After (filtering)', \n                   coor_data_raw = coor_data_raw_left, coor_data=coor_data_left, coor_data_clean=coor_data_left_clean)\n</code></pre> <pre><code>sum of raw mask: 146410\nsum of filtered mask: 141776\nsum of mask clean: 128868\n</code></pre> <p></p> <pre><code>\n</code></pre> <pre><code>count_true = np.sum(Preserve_mask)\nprint(count_true)\n# Save the Preserve_mask\nimport os\nimport scipy.io as sio\nsave_mask = True\n\nif save_mask:\n    mask_name = f'subject_{subject_idx}_mask_clean_{criteria_ratio}_2.mat'\n    if subject_idx == 1:\n        SAVE_PATH = './masks/'\n    elif subject_idx == 2:\n        SAVE_PATH = './masks/'\n    elif subject_idx == 3:\n        SAVE_PATH = './masks/'\n\n\n    # Create the folder if it doesn't exist\n    os.makedirs(SAVE_PATH, exist_ok=True)\n    mask_file = SAVE_PATH + mask_name\n\n    # Save array to a .mat file\n    sio.savemat(mask_file, {'array': Preserve_mask})\n    print(f'The mask file has been saved here: {mask_file}')\n</code></pre> <pre><code>128868\nThe mask file has been saved here: ./masks/subject_1_mask_clean_0.3_2.mat\n</code></pre> <pre><code>\n</code></pre> <pre><code>save_Discard_mask = False\nprint(len(Preserve_mask))\nif save_Discard_mask:\n    Discard_mask = ~Preserve_mask\n    count_true = np.sum(Discard_mask)\n    print(count_true)\n    # Save the Preserve_mask\n    import os\n    import scipy.io as sio\n\n    mask_name = f'{FILE_NAME}_discard_mask_meth2_0_33.mat'\n    if subject_idx == 1:\n        SAVE_PATH = Path(\"/Users/cag/Documents/Dataset/1_Pilot_MREye_Data/Sub001/230928_anatomical_MREYE_study/ET_EDF/meth2_0_33/\")\n    elif subject_idx == 2:\n        SAVE_PATH = Path(\"/Users/cag/Documents/Dataset/1_Pilot_MREye_Data/Sub002/230926_anatomical_MREYE_study/ET_EDF/meth2_0_33/\")\n    elif subject_idx == 3:\n        SAVE_PATH = Path(\"/Users/cag/Documents/Dataset/1_Pilot_MREye_Data/Sub003/230928_anatomical_MREYE_study/ET_EDF/meth2_0_33/\")\n    else:\n         SAVE_PATH = Path(\"/Users/cag/Documents/Dataset/1_Pilot_MREye_Data/Sub004/230923_anatomical_MREYE_study/ET_EDF/meth2_0_33/\")\n\n    # Create the folder if it doesn't exist\n    os.makedirs(SAVE_PATH, exist_ok=True)\n    mask_file = SAVE_PATH / mask_name\n\n    # Save array to a .mat file\n    sio.savemat(mask_file, {'array': Discard_mask})\n    print(f'The mask file has been saved here: {mask_file}')\n</code></pre> <pre><code>656158\n</code></pre>"},{"location":"post-processing/1_2_et_QAQC_of_mreye_tracking/#multiple-masks","title":"Multiple Masks","text":"<p>if num=4, divide the valid range into 4 parts</p> <pre><code>from mask_clean import find_mean_position, cal_angles, cal_disp, filter_criteria, filter_XY_with_mask, visualization_func\nPreserve_mask_list = []\nfor region_idx in range(4):\n    # 0:up 1:down 2:left 3:right 4:center mask\n    print(f'\\n=======\\nProcessing region {label_list[region_idx]}')\n    coor_data_region = copy.deepcopy(coor_data_dot_filtered_offset)\n\n    coor_data_region['x_coordinate'] = coor_data_region['x_coordinate']*ft_mask_list[region_idx]\n    coor_data_region['y_coordinate'] = coor_data_region['y_coordinate']*ft_mask_list[region_idx]\n    mask_zero = (coor_data_region['x_coordinate'] == 0) &amp; (coor_data_region['y_coordinate'] == 0)\n    coor_data_region.loc[mask_zero, ['x_coordinate'] ] = np.nan\n    coor_data_region.loc[mask_zero, ['y_coordinate'] ] = np.nan\n\n    X_coord_region = coor_data_region['x_coordinate']\n    Y_coord_region = coor_data_region['y_coordinate']\n\n\n    med_coor_region = find_mean_position(X_coord_region, Y_coord_region)\n    theta_h_, theta_h_m, rho_v_, rho_v_m = cal_angles(X_coord_region, Y_coord_region, med_coor_region)\n    print(len(theta_h_))\n    h_dis_region, v_dis_region = cal_disp(theta_h_, theta_h_m, rho_v_, rho_v_m)\n    print(len(v_dis_region))\n\n    # print(v_dis_region)\n    criteria_ratio=0.3\n    discarded_x_mask, discarded_y_mask = filter_criteria(h_dis_region, v_dis_region, criteria_ratio=criteria_ratio)\n    # preserve_mask = ~(discarded_x_mask | discarded_y_mask)\n    # =================================================================================\n    plot_h_v_disp(h_dis_region, v_dis_region, discarded_x_mask, discarded_y_mask, criteria_ratio=criteria_ratio)\n\n    coor_data_region_clean, Preserve_mask, Discard_mask = filter_XY_with_mask(coor_data_region, discarded_x_mask, \n                                                                            discarded_y_mask, seq_name=None)\n    Preserve_mask_list.append(Preserve_mask)\n    print('=====\\nx coordinate statistics:')\n    cal_statistics(coor_data_region_clean['x_coordinate'])\n    print('=====\\ny coordinate statistics:')\n    cal_statistics(coor_data_region_clean['y_coordinate'])\n    # ================================================================================= \n    # Visualization of filtered coor data LIBRE\n    coor_data_raw_region = copy.deepcopy(coor_data_dot_offset)\n    coor_data_raw_region['x_coordinate'] = coor_data_raw_region['x_coordinate']*raw_mask_lc[region_idx]\n    coor_data_raw_region['y_coordinate'] = coor_data_raw_region['y_coordinate']*raw_mask_lc[region_idx]\n    print(f'sum of raw mask: {sum(raw_mask_lc[region_idx])}')\n    print(f'sum of filtered mask: {sum(ft_mask_list[region_idx])}')\n    print(f'sum of mask clean: {sum(Preserve_mask)}')\n    visualization_func(fig_title='Before vs After (filtering)', \n                       coor_data_raw = coor_data_raw_region, coor_data=coor_data_region, coor_data_clean=coor_data_region_clean)\n\n    # ================================================================================= \n\n    # Save the Preserve_mask\n    import os\n    import scipy.io as sio\n    save_mask = True\n\n    if save_mask:\n        mask_name = f'subject_{subject_idx}_mask_clean_{criteria_ratio}_{region_idx}.mat'\n        SAVE_PATH = './masks/'\n\n        # Create the folder if it doesn't exist\n        os.makedirs(SAVE_PATH, exist_ok=True)\n        mask_file = SAVE_PATH + mask_name\n\n        # Save array to a .mat file\n        sio.savemat(mask_file, {'array': Preserve_mask})\n        print(f'The mask file has been saved here: {mask_file}')\n</code></pre> <pre><code>=======\nProcessing region up\nTo find the median coor of the point clouds\nThe mean position of the gaze region: [422.1000061035156, 119.5999984741211] (px)\nTo calculate visual angles w.r.t. points\n656158\nTo calculate the displacement in both directions (mm)\n656158\nTo generate discard masks for x and y direction...\n=====\nx coordinate statistics:\nMean distance: 421.74\nStandard deviation: 13.23\nMedian distance: 421.90\nMinimum distance: 394.20\nMaximum distance: 450.00\nNumber of NaN values: 542504\n=====\ny coordinate statistics:\nMean distance: 116.00\nStandard deviation: 10.17\nMedian distance: 114.30\nMinimum distance: 90.90\nMaximum distance: 148.20\nNumber of NaN values: 542504\nsum of raw mask: 186531\nsum of filtered mask: 179497\nsum of mask clean: 113654\nThe mask file has been saved here: ./masks/subject_1_mask_clean_0.3_0.mat\n\n=======\nProcessing region down\nTo find the median coor of the point clouds\nThe mean position of the gaze region: [410.2000122070313, 475.7999877929688] (px)\nTo calculate visual angles w.r.t. points\n656158\nTo calculate the displacement in both directions (mm)\n656158\nTo generate discard masks for x and y direction...\n=====\nx coordinate statistics:\nMean distance: 409.31\nStandard deviation: 14.15\nMedian distance: 408.70\nMinimum distance: 382.30\nMaximum distance: 438.10\nNumber of NaN values: 534316\n=====\ny coordinate statistics:\nMean distance: 478.16\nStandard deviation: 11.65\nMedian distance: 479.00\nMinimum distance: 447.20\nMaximum distance: 504.50\nNumber of NaN values: 534316\nsum of raw mask: 173746\nsum of filtered mask: 168256\nsum of mask clean: 121842\nThe mask file has been saved here: ./masks/subject_1_mask_clean_0.3_1.mat\n\n=======\nProcessing region left\nTo find the median coor of the point clouds\nThe mean position of the gaze region: [683.4000244140625, 283.70001220703125] (px)\nTo calculate visual angles w.r.t. points\n656158\nTo calculate the displacement in both directions (mm)\n656158\nTo generate discard masks for x and y direction...\n=====\nx coordinate statistics:\nMean distance: 683.69\nStandard deviation: 13.11\nMedian distance: 683.10\nMinimum distance: 655.10\nMaximum distance: 711.80\nNumber of NaN values: 527290\n=====\ny coordinate statistics:\nMean distance: 284.47\nStandard deviation: 11.11\nMedian distance: 283.50\nMinimum distance: 255.20\nMaximum distance: 312.20\nNumber of NaN values: 527290\nsum of raw mask: 146410\nsum of filtered mask: 141776\nsum of mask clean: 128868\nThe mask file has been saved here: ./masks/subject_1_mask_clean_0.3_2.mat\n\n=======\nProcessing region right\nTo find the median coor of the point clouds\nThe mean position of the gaze region: [167.1999969482422, 284.79998779296875] (px)\nTo calculate visual angles w.r.t. points\n656158\nTo calculate the displacement in both directions (mm)\n656158\nTo generate discard masks for x and y direction...\n=====\nx coordinate statistics:\nMean distance: 166.63\nStandard deviation: 14.50\nMedian distance: 166.20\nMinimum distance: 139.00\nMaximum distance: 195.30\nNumber of NaN values: 556130\n=====\ny coordinate statistics:\nMean distance: 284.44\nStandard deviation: 10.58\nMedian distance: 284.10\nMinimum distance: 256.30\nMaximum distance: 313.30\nNumber of NaN values: 556130\nsum of raw mask: 140273\nsum of filtered mask: 136285\nsum of mask clean: 100028\nThe mask file has been saved here: ./masks/subject_1_mask_clean_0.3_3.mat\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <pre><code>print(len(Preserve_mask_list))\nprint(len(Preserve_mask_list[0]))\n</code></pre> <pre><code>4\n656158\n</code></pre> <pre><code>plt.scatter(coor_data_region_clean['x_coordinate'], coor_data_region_clean['y_coordinate'])\n</code></pre> <pre><code>&lt;matplotlib.collections.PathCollection at 0x31d9317f0&gt;\n</code></pre> <p></p> <pre><code>coor_data_region_clean[coor_data_region_clean['x_coordinate']==0]\n</code></pre>"},{"location":"post-processing/1_et2_QAQC_of_eye-tracking_data/","title":"1 et2 QAQC of eye tracking data","text":"<p>derived from: https://www.axonlab.org/hcph-sops/data-management/eyetrack-qc/</p> <p>Author: Yiwei Jia</p> <p>This notebook ROI determination, ET mask generating</p>"},{"location":"post-processing/2_MRI_post_processing/","title":"MRI data","text":"<ul> <li>DICOM from standard clinical protocols: spm12 -&gt;reslicing and registration revise the codes.</li> <li>raw data from MR-Eye or MR_Track protocols: binning with ET masks -&gt; Compressed sensing recon</li> </ul>"},{"location":"post-processing/2_MRI_post_processing/#1-reconstruction-from-raw-data","title":"1 Reconstruction from raw data","text":"<p>For the motion-resolved task, a binning mask must be manually generated, and image reconstruction is performed using the Monalisa toolkit, which supports non-Cartesian trajectories of the T1w/T2w-LIBRE protocol. The reconstruction results are typically saved in .mat format.</p> <p>Given that the matrix size of T1w/T2w-LIBRE is 480, the reconstruction process requires computational resources and should be executed on Debi or the HES-SO server.</p> <p>The practical steps for reconstructing the data are as follows:</p>"},{"location":"post-processing/2_MRI_post_processing/#prepare-raw-data-path-on-debi","title":"Prepare raw data path on Debi","text":"<p>To accelerate data processing, we can first prepare a list of all raw data paths required for the upcoming reconstruction. <pre><code>reconDir = '/path/to/recon/folder';\n\n% Subject 001\n/path/to/raw/data/Subject001/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n\n% Subject 002\n/path/to/raw/data/Subject002/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n\n% Subject 003\n/path/to/raw/data/Subject003/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n\n% Subject 004\n/path/to/raw/data/Subject004/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n</code></pre></p>"},{"location":"post-processing/2_MRI_post_processing/#load-all-the-raw-data-files","title":"Load all the raw data files","text":"<p>It is important to consider the number of channels (nCh), as it may vary across different subjects and acquisitions.  Additionally, make sure to record the duration of the raw data by examining the PMU, as this information will be crucial for the subsequent binning process. <pre><code>%Sub001\nN     = 480 \nnSeg  = 22 \nnShot = 1000 \nnLine = 22000 \nnPar  = 1 \nnCh   = 42 \nnEcho = 1 \nThe duration of the rawdata is: 650185 ms with data points:22000\n\n%Sub002\nN     = 480 \nnSeg  = 22 \nnShot = 1000 \nnLine = 22000 \nnPar  = 1 \nnCh   = 44 \nnEcho = 1 \nThe duration of the rawdata is: 650185 ms with data points:22000\n</code></pre></p>"},{"location":"post-processing/2_MRI_post_processing/#generate-et-masks","title":"Generate ET masks","text":"<ul> <li>Synchronize the eye-tracking (ET) data with the MRI readouts using the trigger information. </li> <li> <p>Once the criteria for selecting the ET data have been established, we will generate an ET mask that matches the duration of the MRI readout.   For instance, if the raw data duration is 650,185 ms, as determined in a previous step, and the ET sampling rate is 1 kHz, an ET mask with a length of 650,185 samples can be generated after identifying the start timestamp of the ET data.</p> </li> <li> <p>Save the generated ET mask in .mat format.</p> </li> </ul> <p>Further details on generating the ET mask and saving it will be provided in Section ref()."},{"location":"post-processing/2_MRI_post_processing/#binning-the-raw-data-according-to-et-masks","title":"Binning the raw data according to ET masks.","text":"<ul> <li>For the raw data, we first eliminate the SI projection of raw data by setting the corresponding mask elements to a value of 0. </li> <li>Then we will bin the data according to the ET mask. <ul> <li>Given that the repetition time (TR) is 8.01 ms for T1w-LIBRE and 29.14 ms for T2w-LIBRE, each MRI readout corresponds to approximately 8 ET points for T1w and 29\u201330 ET points for T2w. </li> <li>Based on this, we apply a sliding window with a length of roughly three times the TR: 30 points for T1w and 90 points for T2w.</li> <li>Within each window, we calculate the number of valid ET mask points (where the value = 1, indicating that the gaze point falls within the selected central range of the screen). If the number of valid points exceeds the user-defined threshold\u2014set as 3/4 of the window length in our case\u2014the corresponding MRI readout is preserved; otherwise, it is excluded.</li> </ul> </li> <li>Finally, we generate a binned mask that selects only the readouts unaffected by significant eye movements.</li> <li>Record the number of readouts remaining after binning for each subject and each derivative method. For example:</li> </ul> <p><pre><code>Subject 001 with Binning, preserved #line: 17314 out of 22000\n\nSubject 002 with Binning, preserved #line: 9573 out of 22000\n\nSubject 003 with Binning, preserved #line: 15322 out of 22000\n\nSubject 004 with Binning, preserved #line: 8835 out of 22000\n</code></pre> If the preserved number of readouts are less than 10k, we will expect a low quality of reconstructed images.</p>"},{"location":"post-processing/2_MRI_post_processing/#create-mitosius","title":"Create Mitosius","text":"<p>If you are working on the HES-SO server, the dataset can be directly mounted from the data storage server, making it most efficient to run Mitosius directly on the server.</p> <p>If you are using an HPC and need to transfer data, it is recommended to create the Mitosius folder locally first and then upload the folder to the HPC instead of transferring the raw data.</p> <p>The scripts about how to use mitosius please refer to the Monalisa document.</p>"},{"location":"post-processing/2_MRI_post_processing/#recon-function","title":"Recon function","text":"<p>In our reconstruction, we utilize <code>Steva</code> function for Single-frame Least-square Regularized Reconstruction, where reularizaiton is the l1-norm of spatial gradient of the image. The reconstructed results are in the format of <code>**.mat</code>. You can find more details in the Monalisa document.</p>"},{"location":"post-processing/2_MRI_post_processing/#2-pipeline-for-processing-dicom","title":"2 Pipeline for processing Dicom","text":"<p>Whereas, the recon results from standard clinical protocols are in DICOM format, directly exported from scanner PC.</p> <ul> <li>Convert Dicom to .nii files: download the MRIcroGL software.<ul> <li>Download and install the software.</li> <li>Prepare the nifti folder under the Dicom folder.</li> <li><code>MRICron.exe &gt;&gt; import &gt;&gt; Dcm2nii &gt;&gt; uncompressed NifTi (.nii)</code></li> </ul> </li> <li>Inspect the .nii files: <ul> <li>Prepare spm software on the laptop.</li> <li>Open spm -&gt; display buttons.</li> </ul> </li> <li>Coregister: Estimate &amp; Reslice.   Coregister the DICOM results from the standard clinical protocol with those from LIBRE. Although the DICOM result from LIBRE will not be used for further analysis, it serves as a reference image, allowing us to transform the clinical protocol image to the same spatial domain as LIBRE for subsequent comparison.</li> </ul> <pre><code>addpath('/Users/cag/Documents/forclone/spm')\n%%\n\n% Define image paths\n\nnii_folder = '/path/to/NIFTI_NII_origin/';\n% Libre image\na_image_name = 'DICOM_BEAT_LIBREon_eye.nii';\n% Vibe image\nb_image_name = 'DICOM_t1_vibe.nii'; \n\na_image = fullfile(nii_folder, a_image_name);\nb_image = fullfile(nii_folder, b_image_name);\n\n%% Inspect the header before co-registration\nV_a_1 = spm_vol(a_image);\ndisp('V_a_1'); disp(V_a_1);\nV_b_1 = spm_vol(b_image);\ndisp('V_b_1'); disp(V_b_1);\n\n% Co_register\nco_register(b_image, a_image)\n\n% Inspect the header after co-registration\nV_a_2 = spm_vol(a_image);\ndisp('V_a_2'); disp(V_a_2);\nV_b_2 = spm_vol(b_image);\ndisp('V_b_2'); disp(V_b_2)\n\ndisp(['V_b remains the same? ', num2str(isequal(V_b_1.mat, V_b_2.mat))])\n%%\n%---------------------------------------------\n% Reslicing part\n%---------------------------------------------\nreslicing(b_image, a_image)\n</code></pre>"},{"location":"recruitment-scheduling-screening/participants-recruitment/","title":"Recruitment and screening","text":""},{"location":"recruitment-scheduling-screening/participants-recruitment/#recruitment-shortlist","title":"Recruitment shortlist","text":"<p>The recruitment was done speaking with people within the CHUV UNIL and using a recruitment announcement on EPFL site (https://myjob.epfl.ch).</p>"},{"location":"recruitment-scheduling-screening/participants-recruitment/#first-contact","title":"First contact","text":"<ul> <li>Write an email to them within the next 24 hours attaching the MRI Safety and Screening Questionnaire and the Informed Consent Form.</li> <li>Confirm the receipt of the email AND the documents.</li> <li>If the inclusion criteria for performing an MRI are met, a day is scheduled for scanning. An in-person meeting 20 minutes before scanning is required to discuss the documentation presented.</li> </ul>"},{"location":"recruitment-scheduling-screening/participants-recruitment/#in-person-meeting","title":"In-person meeting","text":"<ul> <li>Confirm whether the potential participant understood the MRI Safety &amp; Screening Questionnaire and discuss with them any questions or potential reasons that may disqualify them to participate.</li> <li>Female participants will be informed and must acknowledge that they must take a pregnancy test before the first scanning session.</li> <li>Make sure that the participant's questions about the study are all addressed and answered.</li> <li>If the candidate participant does not meet our inclusion criteria or is no longer available to participate in the experiment after having thoroughly discussed the experiment, the participation will be withdrawn.</li> <li>If participants are eligible and confirm they are willing to continue, they are asked to sign the informed consent.</li> </ul>"},{"location":"recruitment-scheduling-screening/scanner-scheduling/","title":"Scheduling","text":"<p>The acquisition day was defined based on the availability of the participant and the availability of the PrismaFit system. The PrismaFit system is then booked following these steps:</p> <ol> <li>Open the scheduling system on a browser.</li> <li>Click on the preferred slot.</li> <li>Select the adequate length for the session.</li> <li>Select Research on healthy subjects in the Type of Scan box.</li> <li>Select true in Technician Required if you are not a certified operator of the system.</li> </ol>"}]}