{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mat-TechLab's Standard Operating Procedures","text":"<p>Welcome to MattchLab's SOPs website. These SOPs refers to different protocols which data are collected simultaneously and that show very similar or overlapping procedures. These refers to several different protocols: </p> <ul> <li>visual checkerboard fMRI protocol</li> <li>audio fMRI protocol </li> <li>resting state audio-visual fMRI protocol. </li> </ul> <p>In the Data Collection section, the different protocols are extensively descripted. </p>"},{"location":"#summary","title":"Summary","text":"<p>Understanding how the brain's structure influences its distributed functions and processing dynamics holds great potential for revolutionizing neuroscience and clinical applications. Functional magnetic resonance imaging (fMRI) has proven to be a valuable, non-invasive tool for examining both the architecture and activity of the brain in vivo. Despite significant mathematical and computational advances in reconstructing resonance signals, most studies rely on the blood oxygen level-dependent (BOLD) signal using echo-planar imaging (EPI). This method employs Cartesian sampling and image reconstruction with a direct one-to-one correspondence between the number of acquired volumes and reconstructed images. However, EPI techniques face trade-offs between spatial and temporal resolutions. In this project, we aim to overcome these limitations by measuring the BOLD signal using a gradient recalled echo (GRE) with a 3D radial-spiral phyllotaxis trajectory at a high sampling rate. This approach allows the reconstruction of 3D signal time courses with whole-brain coverage, achieving simultaneously higher spatial (1 mm\u00b3) and temporal (up to 250 ms) resolutions compared to optimized EPI schemes. Additionally, artifacts are corrected before image reconstruction, and the desired temporal resolution can be selected post-scan without assumptions about the hemodynamic response shape. In total, each protocol involves MRI/fMRI acquisition from 20 healthy adult human subjects using three different 3.0 Tesla (T) MRI scanners available at CHUV.</p>"},{"location":"#impact","title":"Impact","text":"<p>Overall, this project will introduce a new framework for acquiring and analyzing fMRI data, addressing many of the current limitations. This method represents a significant advancement in functional BOLD imaging and has the potential to fundamentally redefine the research questions fMRI can address regarding cognitive processes. The project will publicly release valuable datasets that are essential for enhancing the acquisition and analysis of functional resonance data. In conclusion, this study is poised to be a turning point for MRI research in both neuroscientific and clinical applications.</p>"},{"location":"data-collection/","title":"Data collection","text":""},{"location":"data-collection/#example-1","title":"Example 1","text":"<p>The following is an example checklist for setting up a participant in the MRI to avoid motion-related or foreign object-related artifacts:</p> <p>BEFORE SCAN DATE</p> <ul> <li> Confirm that participant does not have any MRI contraindications</li> <li> Remind participant that any jewelry should be removed prior to the scan </li> <li> If allowed to wear street clothes, remind participant to avoid clothing with metal or that would uncomfortable to lie in for the duration of the scan</li> <li> If participant has indicated nervousness or history of claustrophobia, utilize mock scanner </li> </ul> <p>DAY OF SCAN, PRIOR TO PARTICIPANT ARRIVAL</p> <ul> <li> Prepare consent documents and MRI safety screener </li> <li> Prepare scrubs and MR-compatible glasses if applicable</li> <li> Setup scanner bed with proper headcoil, under-knee padding, neck padding, and any other padding necessary for safety and comfort </li> <li> Check stimulus display and response device </li> </ul> <p>SCAN TIME</p> <ul> <li> Have participant fill out consent documents and MRI safety screener, and verbally confirm responses, paying attention to frequently forgotten devices and implants, like orthodontia</li> <li> Have participant empty their pockets or change into scrubs, and remove all jewelry/hair accessories and check for any missed metallic objects with the scan center\u2019s preferred method</li> <li> Instruct participant on staying still and encourage them to request breaks if necessary </li> <li> Solicit feedback on participant\u2019s comfort while positioning them on the scanner bed and suggest ergonomic positioning of arms to avoid discomfort</li> <li> Follow the Scan console checklist.</li> </ul> <p>DURING SCAN</p> <ul> <li> Check in with participant frequently</li> <li> Watch for motion if you can see the participant, or use motion monitoring equipment</li> </ul> <p>AFTER SCAN</p> <ul> <li> Solicit more feedback on participant\u2019s comfort for future sessions</li> <li> Run MRIQC to evaluate data</li> </ul>"},{"location":"data-collection/#example-2","title":"Example 2","text":"<p>BEFORE THE SCAN DATE (1-2 days)</p> <ul> <li> Call or email the participant before the appointment to remind them of the details and times, and confirm that they can still make it</li> <li> Register their confirmation in STRATA or reschedule the appointment (see scheduling SOPs).</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/","title":"DEBI protocol","text":"Note for writers <ul> <li>Precisely point out what is connected to what by which kind of cables.</li> <li>Need double check with JB</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#overall-experimental-setting","title":"Overall experimental setting","text":"<p>The experimental setting is to obtain anatomical MRI scan with synchronized eye tracking recording (the right eye gaze trajectories and eye movement events including blinking, saccades and fixation).</p> <p>Here is the illustration of the overall experiment:</p> <p></p> <p>The graph above can be divided into the following components: - Syncbox: A NordicLabs Syncbox sends TTL (transistor-transistor logic) triggers to the scanner and forward the signal converted to the keyboard signal \"s\" to the PsychoPy laptop. </p> <p></p> <p>Trigger to PsychoPy PC</p> <p></p> <p>Trigger to Scanner</p> <ul> <li>Scanner: 3T clinical scanner (MAGNETOM PrismaFit, Siemens Healthineers) with a 64-channel head-neck coil with an attached mirror.</li> <li>Eye tracker: We use the EyeLink 1000 Plus (SR Research Ltd.) for eye tracking.   (i) The eye tracker consists of an infrared lens and camera sensor on one side, along with an infrared lamp to illuminate the subject's right eye. It is positioned inside the scanner bore.   (ii) An infrared mirror is mounted on the head coil.</li> <li>The Eye Tracker PC is bi-directionally connected to both the eye tracker and the PsychoPy laptop. It receives eye tracking data from the tracker, processes the eye movement trajectories and images, calculates pupil sizes by segmenting the pupil area, and classifies eye movement events based on predefined thresholds. The ET PC also receives trigger and task event messages from the PsychoPy laptop, logs data, and sends both the eye tracking data and logs back to the PsychoPy laptop.</li> <li>Stimuli laptop (PsychoPy Laptop): The PsychoPy laptop runs the PsychoPy software, which is used to execute the task programs. These programs coordinate various hardware components, including the ET PC, eye tracker, the screen, and syncbox. In our DEBI experiment, the 'MR-Eye' protocol displays a central dot that changes color at a frequency of 10Hz. In the 'MREye-Track' protocol, a gray dot sequentially appears at the top, bottom, left, and right sides of the screen. The laptop also stores the data and logs generated by the Eye Tracker PC after the experiment.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#session-preparation","title":"Session Preparation","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#configure-the-ip-address","title":"Configure the IP address","text":"<p>If you are connecting your PC to the eye tracker (ET) for the first time, you need to reconfigure the IP address. On windows: - Go to <code>Control Panel</code> -&gt; <code>Network and Internet</code> -&gt; <code>Network Connections</code> - Double-click on the <code>Ethernet</code> connection. - Select <code>Internet Protocol Version 4 (TCP/IPv4)</code> and click <code>Properties</code>. - Update the IP address to <code>100.1.1.2</code> and the subnet mask <code>255.255.255.0</code>. (Photos from Helene's sop)</p> <p> </p>"},{"location":"data-collection/data-collection_DEBI_protocol/#prepare-et-set-up-outside-the-scanner-room","title":"Prepare ET set up outside the scanner room","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#setting-up-the-projector","title":"Setting up the projector","text":"<p>Turn on the Sony projector located in the back room of the scanner room. </p> <p>Ensure the projector beam is directed into the scanner room.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#set-up-the-psychopy-laptop-on-the-table","title":"Set up the Psychopy laptop on the table","text":"<ul> <li> <p>Insert the hdmi into the Psychopy laptop to monitor the visual stimuli on the screen. The hdmi should be from the 3-cable bundle (which connects PC, projector and the Psychopy laptop). </p> </li> <li> <p>If the monitor does not automatically switch the screen source, use the button below to manually change it.  </p> </li> <li> <p>Ensure that the PC beneath the monitor remains turned on. </p> </li> <li> <p>Connect the USB cable from the Syncbox to the PsychoPy laptop.  </p> </li> <li> <p>Plug the Ethernet from the ET computer to the psychopy laptop. </p> </li> </ul> <p>Make sure the IP address is reset if the laptop is connected to the ET PC for the first time!</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#set-up-the-syncbox","title":"Set up the syncbox","text":"<ul> <li>Ensure the SyncBox cable (RJ45) already plugged onto the interface to the scanner room. </li> <li>Plug the other end of the cable into the SyncBox. </li> <li>Turn on the syncbox</li> <li>Go to the <code>Simulation</code>, and we can see the <code>Start Session</code> on the page.  </li> <li>Configure the TR time to 2500 ms according to our sequence. The TR determines the interval between two triggers. </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#set-up-the-et-workstation","title":"Set up the ET workstation","text":"<ul> <li>Make sure the ET PC is charged </li> <li>Before turning on the ET computer, ensure it is connected to the PsychoPy laptop (this should have been done in the previous step).</li> <li>Turn on the ET PC.</li> <li>Initialize the ET software from the ET work station here by typing \"elcl.exe\" in the terminal </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#prepare-the-et","title":"Prepare the ET","text":"<ul> <li>All the lenses, mirrors, and other equipment are in the box in JB's office. </li> <li>Install the 50mm lens onto the eye tracker (the compatible lens has a silver screw on it) (photos from Oscar's SOP)</li> </ul> <p> what value is the default position of the screw for convenient focus ?</p> <p>The default position of the screw for convenient focus is typically set to <code>1</code>. However, the exact value can vary depending on the specific participant and setup for each time. It's recommended to start with the screw at value <code>1</code> and adjust from there for optimal focus during setup. If you're following a specific SOP, it might provide additional details for your equipment.</p> <p></p>"},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-infrared-mirror-onto-the-head-coil","title":"Place the infrared-mirror onto the head coil","text":"<ul> <li>Detach the standard mirror's frame from the head coil, if it is placed there. </li> <li>Take the infrared mirror out of the \u00abfMRI\u00a0usage\u00bb box. It should be always protected by a mask unless in use.</li> </ul> <p>This infrared mirror is the most delicate part, because the morror cannot be replaced nor cleaned. This mirror is\u00a0EXTREMELY EXPENSIVE.</p> <p></p> <ul> <li>Get two gloves (e.g., from the box hanging at the entrance of the scanner room)</li> <li>Put the gloves on, and\u00a0DON'T TOUCH ANYTHING. You must have the standard mirror dismounted and ahead of this step.\u00a0</li> <li>WITH THE GLOVES\u00a0proceed to extract the infra-red mirror from its box, being extremely careful.\u00a0YOU CAN ONLY TOUCH THE MIRROR WITH GLOVES, because it cannot be cleaned up. Watch out for FINGERPRINTS\u00a0and once taken out of the protection mask,\u00a0IMMEDIATELY AND CAREFULLY ATTACH IT to the\u00a0head coil. [A photo of the head coil with the mirror correctly installed is missing]</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#preparation-in-the-scanner-room","title":"Preparation in the scanner room","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#connect-three-external-cables-to-et-and-scanner","title":"Connect three external cables to ET and scanner","text":"<p> - Two plugs for the black and one plug for the orange to ET   - Connect the external cable from the syncbox to the scanner. No photo here due to magnetic field</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-eye-tracker-and-screen","title":"Place the eye tracker and screen","text":"<ul> <li>Place the glass plate (stored in JB's office) on the scanner</li> <li>Position the ET on the glass plate according to the stickers on the plate.</li> <li>Place the half-circle one-direction screen, which is on the table behind the scanner, onto the glass plate. Position it between the projector and the eye tracker to reflect the projector's image. </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-participant","title":"Place the participant","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#place-the-subject-on-the-bed","title":"Place the subject on the bed","text":"<ul> <li>Apply a blanket, ear plugs and sand bags to the participant.</li> <li>Adjust the head coil and the mirror. If necessary, apply some pads to adjust the participant's head position to ensure the forehead tightly positioned against the head coil.</li> <li>Ensure the participant does not cross their legs.</li> <li>Provide the participant with the emergency button and explain that they can press it in case of an emergency.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#adjust-the-scanner-before-send-the-subject-inside","title":"Adjust the scanner before send the subject inside","text":"<ul> <li>Twist the knob to adjust the height of the bed and\u00a0 wait for it to stop.</li> <li>Gently move the participant with the manual controls. Stop when the head is under the head-localizer. Instruct the participant to close his eyes.</li> <li>Turn on the red light to localize the head. Put one hand on the head coil, then turn the knob left or right to align the red light with the mark on the head coil. Once aligned, turn off the red light and instruct the participant to open their eyes.</li> <li>Turn off the ventilation and set the scanner light to the minimum level.</li> <li>SSend the participant into the scanner, then proceed to the console at the back of scanner.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#adjust-the-lens-of-eye-tracker","title":"Adjust the lens of eye tracker","text":"<ul> <li>Point the lens of eye tracker towards the participant's right eye.</li> <li>[Add the default position of the lens for convenient adjustment]</li> <li>Rotate the lens until the pupil is in focus on the screen during camera mode. Adjust until the image is sharp, with both the pupil and eyelashes well defined.  ask for Mauro's consent</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#et-calibration","title":"ET Calibration","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#inform-the-participant","title":"Inform the participant","text":"<p>Inform the participant that you are leaving the room and will shortly come back for a final preparation.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#open-psychopy-in-the-psychopy-laptop","title":"Open psychopy in the psychopy laptop.","text":"<p>If you are connecting the eye tracker to the experimental laptop for the first time, you will need to configure it (see Section 0).</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#run-the-experiment-on-psychopy","title":"Run the experiment on psychopy","text":"<p>Click the <code>Run Experiment</code> button, or run the experiment from the terminal by typing: <code>python experiment.py</code></p> <p>Make sure that once the experiment start after the calibration, the data are being stored to the xx.EDF file. There should be a message about that displayed at the ET\u2019s PC screen. (According to Helene's, but we did not notice there was such message popping out before)</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#run-the-calibration","title":"Run the calibration","text":"<p>Once the stimulation begins, follow the messages on the screen to run the calibration, make sure the following options are selected correctly</p> <ul> <li>Calibration Type: <code>5 points calibration</code></li> <li>Sampling rate: <code>1k</code></li> <li>Tracking mode: <code>Pupil-CR</code></li> <li>Pupil Tracking: <code>Ellipse</code></li> <li>Camera Position: <code>Right</code>\"</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#apply-threshold-to-find-the-pupil","title":"Apply threshold to find the pupil","text":"<ul> <li>On the ET PC, click <code>Apply Threshold</code> (top left corner, as shown in the figure below). Ensure that the pupil is detected and that you see the blue cross on the eye. If you encounter issues, check the lighting inside the scanner (ensure it's not too bright or too dim) and verify the participant's position inside the coil. Once the calibration starts, accept the calibration points when they turn green by clicking <code>Accept Fixation</code>.</li> <li>If the calibration was successful, you will see the sentence <code>calibration successful</code> at the bottom in green. Check the stability of the accepted points and overall score of the calibration. </li> </ul> <p>If the calibration points form a cross, it is the perfect calibration.</p> <p></p>"},{"location":"data-collection/data-collection_DEBI_protocol/#follow-up-with-the-validation","title":"Follow up with the validation.","text":"<p>What you should see in an ideal situation is: the reference dot on the center of the screen and another dot that corresponds to the pupil calibration. The calibration dot is more or less stable moving a little around the reference dot.</p> <p>If the calibration dot is unstable and is moving around far from the reference dot, the experimenter should go back clicking the restart button, adjust the contrast and redo the calibration. Once the calibration dot is quite stable, proceed with the validation clicking the accept fixation.</p> <p></p>"},{"location":"data-collection/data-collection_DEBI_protocol/#go-into-the-scanner-room-and-inform-the-participant","title":"Go into the scanner room and inform the participant","text":"<p>Inform the participant that you are leaving the room and will now close the door to start. Let them also know that you are going to communicate with them very shortly to check that communications through the speaker are functioning.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#exit-the-scanning-room","title":"Exit the Scanning Room.","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#close-the-scanning-room-door","title":"Close the Scanning Room door.","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#running-the-scanning-session","title":"Running the scanning session","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#run-the-experiment","title":"Run the Experiment","text":"<ul> <li>At the end of the ET calibration we are ready to run the experiment.</li> <li>Wait for the sentence \u201cIn this task you will see a color dot. Please keep your eyes on the fixation point. The program is ready for the scanner trigger. Press s to proceed manually.\u201d</li> </ul> <p>Due to the upgrade of the scanner, it cannot immediately start the acquisition after receives the trigger signal from the syncbox. Thus, we need to extract the temporal information of the scanner and ET respectively. In order to make such post-processing easier, we need to first start the acquisition, and then the eye tracker. Thus, the first trigger recorded in the pmu in the raw data will be exactly the same trigger that starts the eye tracker.</p> <p>The order: start of scanner -&gt; press the button of syncbox is important and cannot be exchanged.</p> <ul> <li>Now two people need to get ready beside the syncbox and the scanner.</li> <li>One person first start the scanner acquisition.</li> <li>Then, another person press  <code>start session</code> on the sync box clicking the round button. </li> <li>The stimulation will start with the ET recording. </li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#acquire-a-high-resolution-anatomical-image","title":"Acquire a high-resolution anatomical image","text":"<p>wip</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#acquire-t1w-libre-image-and-t1w-vibe-image","title":"Acquire T1w-LIBRE image and T1w-VIBE image","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#acquire-t2w-libre-image-and-t2w-tse-image","title":"Acquire T2w-LIBRE image and T2w-TSE image","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#session-completed","title":"Session Completed","text":"<ul> <li>At the end of the stimulation, click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> <li>The exam is over, inform the participant that the session has concluded.</li> <li>You can proceed with the tear-down protocol.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#session-tear-down","title":"Session Tear-Down","text":""},{"location":"data-collection/data-collection_DEBI_protocol/#showing-the-participant-out","title":"Showing the Participant Out","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in a few seconds.</li> <li>Extract the participant by pressing the extraction button and then gently rolling the central knob. Alternatively, you can just press the Home button.  </li> <li>Remove the upper side of the head coil:</li> <li>Unplug the head coil from the bed connector.</li> <li>Lift the lever that releases the upper part of the coil and put it aside (e.g., inside the bore or on a chair next to the scanner).</li> <li>Assist the participant to remove the headphones.</li> <li>Help the participant sit down.</li> <li>Help the participant step down and accompany them out to the control room.</li> <li>Help the participant recover their personal belongings and change clothes if necessary.</li> <li>Give the participant the corresponding compensation for the participation and transportation.</li> <li>Ask the participant to sign the receipt of the amount of the financial compensation.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#et-setting","title":"ET setting","text":"<ul> <li>Place the half-circle screen back to the table behind the scanner.</li> <li>Unplug the two cables (signal and power) connected to the ET arm.</li> <li>Roll the two ET cables and put them in the cupboard inside the Scanning room.</li> <li>Remove the mirror frame from its rails mounted on the head coil and lay it on the bed.</li> <li>Put the gloves on and cover the infrared mirror with a mask for storage.</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#clearing-up-the-scanner","title":"Clearing up the Scanner","text":"<ul> <li>Unplug the cable on the scanner from syncbox and roll it back to the shelf.</li> <li>Remove used blankets and bed-sheets ONE-BY-ONE: extend them to let any forgotten items fall on the floor before you fold it; and dispose of them in the adequate bin (soiled linen bag if they are fabric and trash if they are disposable).</li> <li>Dispose of all single-use sanitary protections (padding covers, earplugs, etc.).</li> <li>Put the pillows back in their designated storage places.</li> <li>Remove the head coil and put it in the scanner's bore.</li> <li>Remove the back padding elements and put them back in their designated storage.</li> <li>Reinstall the spine coil.</li> <li>Wipe the bed and the head coil (bottom and upper parts).</li> <li>Lock the head coil back with its bottom part without plugging the connectors.</li> <li>Put the head coil away with the other head-coils on the shelf next to the scanner.</li> <li>Return the bed to its Home position by pressing the button (more info).</li> <li>Take the ET arm, the infrared mirror and the plexiglass panel outside to the control room and store them in the ET/fMRI box.</li> <li>Exit and close the external door.</li> </ul> <p>Everything that is removed for the experiment needs to be put back in place at the end of the experiment, i.e., position of the bed, coil, emergency button, ears cover.</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#collect-et-data","title":"Collect ET data","text":"<ul> <li>Copy data from the subfolder of PsychoPy program into the hard drive. !!! tip \"The subfolder should include the files for one session with the formats like  <code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.EDF</code> <code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.csv</code> <code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.log</code> where '**.EDF' files are eye tracking records, <code>.csv</code> and <code>.log</code> files logs the task messages and the corresponding timestamps.\"</li> </ul>"},{"location":"data-collection/data-collection_DEBI_protocol/#collect-mri-raw-data","title":"Collect MRI raw data","text":"<p>Export Twix raw data - Twix: Username: medadmin, Password: adm\\(pwd\\)4\\(med\\). - Press <code>Ctrl+Esc</code> to open the IDE terminal. - In the IDE terminal: ideacmdtool -&gt; type <code>4</code> -&gt; type <code>6</code> // - Type <code>Twix</code> and the Twix data browser opens. - Select the data you want to copy, right click on the mouse -&gt; Copy Total RAID file -&gt; select destination (your hard drive). -  The only useful thing is the physio [select the flag on External Signal] need more explanation </p> <p>Export the DICOM data (directly reconstructed images from the scanner) - Login as SuperUser by pressing <code>Tab</code> + <code>Delete</code> + <code>(Bottone a DX - 9)</code> to enter the advance mode. - Export DICOM: select the patient, go to export // File System // Browse -&gt; select <code>HD</code>. - Select the \u201cEnhanced\u201d option (1 DICOM  / volume) instead of Interoperability (1 DICOM / slice).</p> <p>The default option is <code>Interoperability</code>! So we have to change it manually!</p>"},{"location":"data-collection/data-collection_DEBI_protocol/#cleaning-up-the-control-room","title":"Cleaning up the Control Room","text":"<ul> <li>Plug back the SyncBox and the VGA projector where they were. Make sure you leave it connected exactly as you found it.</li> <li>Cover the eye tracker lens with the lid.</li> <li>Unscrew the 50mm lens from the eye tracker and place it back to the bag with the tag <code>50mm Lens for MRI use</code> on it. </li> <li>Make sure the infrared mirror covered with a mask and everything stored safely in the ET/MRI box.</li> <li>Store the ET/MRI box back to the office.</li> <li>Switch off ET PC. </li> <li>Switch off the projector.</li> </ul>"},{"location":"data-collection/data-collection_audio/","title":"Audio fMRI protocol","text":""},{"location":"data-collection/data-collection_audio/#data-collection-audio-fmri-protocol","title":"Data Collection audio fMRI protocol","text":""},{"location":"data-collection/data-collection_audio/#overall-experimental-setting","title":"Overall experimental setting","text":"<p>The experimental setup includes:</p> <ul> <li>Syncbox: A NordicLabs Syncbox receives TT (transistor-transistor logic) triggers from the scanner. This box can forward the triggers converted into other formats and/or manipulate them (e.g., filter, generate, etc.).</li> <li>Experimental laptop: It is the laptop where the Psychopy software is installed and with it the task programs are executed. This laptop also stores the data recorded by the ET at the end of the experiment.</li> </ul> <p>ONE DAY BEFORE SCAN DATE</p> <ul> <li>Verify that the Psychopy experiment runs correctly.</li> <li>Verify that all the tools necessary for the data collection run correctly.</li> <li>Print the informed consent form.</li> <li>Print the MRI safety screener (EN|FR).</li> <li>Print a receipt form for each participant that will get scanned.</li> </ul>"},{"location":"data-collection/data-collection_audio/#session-preparation","title":"Session preparation","text":"<p>The following section describes how to prepare the session on the day of the scan BEFORE the participant arrives. Try to arrive at the Control Room at least 30 min ahead of the session start time.</p>"},{"location":"data-collection/data-collection_audio/#setup-preparation-inside-the-scanner-room","title":"Setup Preparation Inside the Scanner room","text":"<ul> <li>Memorize where the other tools for the recordings are to put those back in place at the end (coil, emergency button, ears cover).</li> </ul>"},{"location":"data-collection/data-collection_audio/#setting-up-the-projector","title":"Setting up the projector","text":"<ul> <li>Before entering the scanner room, go to the room where the projector is installed.</li> <li>Switch the projector ON by hitting the power button located on its right side.</li> <li>Verify the aim of the projector's beam by looking through the tube into the Scanning Room.</li> <li>Verify the projection corresponds to the Psychopy laptop screen.</li> <li>Go to the scanner room and take the half-circle one-direction screen from the table behind the scanner and put it on the back of the scanner.</li> </ul>"},{"location":"data-collection/data-collection_audio/#setting-up-cables-and-headphones","title":"Setting up cables and headphones","text":"<ul> <li>Open the door of the cable section between the control room and the scanner room.</li> <li>Connect the scanner machine to the sharing system.</li> <li>Prepare the headphones to connect to the scanner machine. The type of headphone was choose based on the comfortability for the participant. Here are the two possible models:</li> </ul>"},{"location":"data-collection/data-collection_audio/#setting-up-the-coils","title":"Setting up the coils","text":"<ul> <li> <p>If any head coil from the last exam is still plugged, remove it:  <ul> <li>If it is the 64-channel coil, you can just temporarily move it into the scanner's bore.</li> <li>Otherwise, store it on the shelf where the other coils are and bring the 64-channel one in the proximity of the bed (e.g., inside the scanner's bore). Make sure to remove other coil's fitting elements.</li> </ul></p> </li> <li> <p>Remove the spine coil by lifting the corresponding latch, then sliding it toward the head of the bed, lift it from the bed and place it on the floor ensuring it is not obstructing any passage or unstable.</p> </li> <li>Place the two back padding elements filling the spine coil socket.</li> <li>Place the 64-channel head-and-neck coil into its socket at the head end of the bed.</li> </ul>"},{"location":"data-collection/data-collection_audio/#final-setting-inside-the-scanning-room","title":"Final setting inside the scanning room","text":"<ul> <li>Cover the MRI bed with a clean sheet.</li> <li> <p>Prepare padding: under-knee padding, neck-and-head padding, under-elbows padding, head-sides padding, top-head wedge padding.  <ul> <li>Wrap a sanitary cover around each padding.</li> <li>Place a double neck-and-head padding inside the coil, to ensure the eyes are close to the coil's windows.</li> </ul></p> </li> <li> <p>Prepare a blanket to cover the participant.</p> </li> </ul>"},{"location":"data-collection/data-collection_audio/#setup-preparation-inside-the-control-room","title":"Setup Preparation Inside the Control Room","text":""},{"location":"data-collection/data-collection_audio/#setting-up-experiment-instruments","title":"Setting up experiment instruments","text":"<ul> <li>Arrive to the Control Room at least 30 min ahead the session start time.</li> <li>Place the experimental laptop on the designed desk and connect all the as showed in the following picture (in this case Jack audio is not necessary). Specifically, connect the experimental laptop to:  <ul> <li>Plug the power adaptor to the laptop, and the adaptor to the power outlet on the wall.</li> <li>the screen switch box with the corresponding HDMI cable ( This should project your screen on the screen of CHUV's tower).</li> <li>the audio switch box with the corresponding jack cable ( This should share your audio on the audio of CHUV's tower).</li> <li>Connect the SyncBox to the laptop with the USB cable. It is normally plugged into CHUV's stimuli workstation, it must be re-plugged in there after the session.   </li> </ul> <ul> <li>Switch the laptop on and open the psychopy code.</li> <li>Click the switch button to share your PC.</li> <li>Switch the SyncBox on using the button on the right side.</li> <li>Change the SyncBox correctly to send the triggers (Corresponding to push the key-button \u201cS\u201d from keyboard). Take the SyncBox and go on \u201cSimulation\u201d mode.</li> <li>Then, change the parameters in the main menu modifying the pulse length at 100ms and the TR time at 650ms. </li> <li> <p>Push the enter button  and the syncbox will be now waiting for the scanner's trigger signal to forward it.  </p> </li> <li> <p>Open the door of the cable wardrobe between the recording room and the scanner room, and connect the sync box in the following way:</p> </li> </ul> <p> </p>"},{"location":"data-collection/data-collection_audio/#setup-preparation-at-the-technician-position","title":"Setup Preparation at the technician position","text":"<ul> <li>Changes the parameters for the audio system at the box and at the pc:</li> </ul> <ul> <li>Switch audio to the scanner room clicking \u201cinput\u201d button:</li> </ul>"},{"location":"data-collection/data-collection_audio/#participant-preparation","title":"Participant Preparation","text":""},{"location":"data-collection/data-collection_audio/#participant-reception","title":"Participant reception","text":"<ul> <li>Meet the participant at an easily locatable place (e.g., the reception desk of the Radiology Unit) and show them the way into the control room. Allow sufficient time before the experiment for the preparation.</li> <li>Show the participant the scanning room and explain to them how the device is controlled from outside.</li> <li>Ask to the participant to fill out the consent form and MRI safety screener, and verbally confirm responses and discuss further doubts, paying attention to frequently forgotten devices and implants, like orthodontia.</li> <li>Remind the participant to use the bathroom at this moment if they need.</li> <li>Describe the participant how the session will develop and explain clearly the task. Let them interrupt you to ask for clarifications and answer all the questions that may arise.</li> </ul> Script for the session <p>\u201cWe are going to acquire two types of images. The first type is anatomical imaging that we use to study the morphology of the brain. The second type is a functional MRI, which we use to understand how the brain activates as a response to stimuli we will present to you. During the whole duration of the exam, please do not create closed loops by crossing your legs or holding your hands together. It is possible that your peripheral nerves get stimulated at some points, so you will feel twitching of muscles, for instance, of your pectorals. Do not panic, it is okay, but if it feels too uncomfortable, please squeeze the alarm button.For the anatomical MRI we just ask you to relax and try to stay as still and comfortable as possible. Like a photographic camera, the largest problem making analyses difficult is motion. As opposed to a photo camera, the imaging of the brain happens very slow so there is a lot of opportunity for involuntary movements (e.g., when you blink or you take a deeper breath) or semi-voluntary (e.g., you need to swallow).   During the functional MRI you will hear different sounds in the headphones. The audio stimulation consists of short sounds of objects of everyday life (es for example, the sound of vacuum cleaner, the crying of a child, a guitar song, a bell rings, etc.) or animal noises. The experimental protocol consists of 15s audio stimulation using the above tracks followed by 25s of pauses in which we provided a pink noise. For the entire period of the experiment you should takes the eye open on a fixation point (a red cross) at the center of a gray environment. The experiment has a duration of 40 minutes more and less.</p> <p>*Is everything clear to you? Do you have any questions?\u201d</p> <ul> <li>Offer the participant a box to deposit everything they have in their pockets and all jewelry/hair accessories, and indicate the before to continue, we need to make sure we do not introduce any dangerous object in the magnet room. Therefore, it is important to inform the participant to remove every metallic accessory. </li> <li>Ask to remove shoes at the entrance of the scanning room.</li> </ul>"},{"location":"data-collection/data-collection_audio/#participant-preparation-in-the-scanning-room","title":"Participant preparation in the scanning room","text":"<ul> <li>Instruct the participant to lay on the MRI bed and adjust the participant inside.</li> <li>Put the headphones on the ears.</li> <li>Give to the participant the emergency button. Make the participant try it, so they can see it works. To switch off the alarm, there\u2019s a button on the scanner (circular, both on the left and on the right of the hole) </li> <li>Once the previous part is insured, the participant is ready. If the participant is cold, put a blanket on top of him. </li> <li>Connect the coil's cable to the corresponding socket on the table.</li> <li>Gently move the participant with the manual regulation. Stop when the head is under the head-localizer. Ask the participant to close the eyes, localize the head with the infrareds.</li> <li>Switch off the infrareds, now the participant can open the eyes. You can move the participant (always gently as before) inside the scanner, until the mm counter marks \u201cIsometric\u201d.</li> <li>If everything is ok, you can move forward and record.</li> </ul>"},{"location":"data-collection/data-collection_audio/#participant-preparation-in-the-scanning-room_1","title":"Participant preparation in the scanning room","text":"<ul> <li>Inform the participant that you are leaving the room and will now close the door to start. Let them also know that you are going to communicate with them very shortly to check that communications through the speaker are functioning.</li> <li>Exit the Scanning Room.</li> <li>Close the Scanning Room door.</li> </ul>"},{"location":"data-collection/data-collection_audio/#running-the-scanning-session","title":"Running the scanning session","text":"<p>You MUST know the security procedures in case of problem and keep yourself updated with changes</p>"},{"location":"data-collection/data-collection_audio/#before-initiating-the-session-run-the-experiment","title":"Before initiating the session: Run the experiment","text":"<ul> <li>Double click on the psychopy file of the audio protocol to open it.</li> <li>Run the experiment on psychopy clicking the \u201crun experiment\u201d button selected in the red square.  </li> <li>Wait for the sentence \u201cIn this task you will hear different sounds in the headphones. Please keep your eyes on the fixation point. The program is ready for the scanner trigger. Press s to proceed manually.\u201d</li> <li>Then click \u201cstart session\u201d on the sync box clicking the round button. </li> </ul> <ul> <li>The stimulation will start with the scanning. At the end of the experiment click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> </ul>"},{"location":"data-collection/data-collection_audio/#during-the-session","title":"During the session","text":"<ul> <li>Check in with the participant.</li> <li>Check in that everything is correctly working (sounds are played and heart from the participant, volume is loud enough, triggers correctly works, etc.)</li> </ul>"},{"location":"data-collection/data-collection_audio/#acquire-a-localizer","title":"Acquire a localizer","text":"<ul> <li>Indicate the participant that the scanning will soon start.</li> <li>Wait for the participant confirmation and set the speaker off afterward.</li> <li>Launch the AAhead_scout_64ch-head-coil protocol by pressing Continue.</li> <li>Once the localizer is concluded, click on the image stack icon with left click and drag the image with a 1 onto the image viewer. That will open the interpolated localizer on the viewer. </li> <li>If the quality looks good, check the box stating Localizer looked ok. If not, re-acquire the localizer. </li> </ul>"},{"location":"data-collection/data-collection_audio/#acquire-high-resolution-anatomical-image","title":"Acquire high-resolution anatomical image","text":"<ul> <li>Run the wip19_mprage_1iso_cs4p2 protocol by pressing *Continue.</li> </ul> While you are still running the MPRAGE sequence: <pre><code>    Open the parameters of the sequence and ensure that:\n\n    - under Sequence/Part1, the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.\n    - under Routine, TR and TE should be set at the minimum value. the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.\n    - under Contrast/common, TR and TE should be set at the minimum value. The fat-water should be set as standard and the flip angle should be set to 5 degree.\n    - under Contrast/filter, click on the three dots and tick the \u201cunfiltered images\u201d.\n    - under System/coils, select the coils HC3, HC5, HC4, HC6, HC7.\n    - under System/miscellaneous, Put the coil selection as manual.\n    - under Physio, make sure that RECONSTRUCTION is off.\n\n    At the end, click copy and go button\n</code></pre>"},{"location":"data-collection/data-collection_audio/#acquire-functional-image","title":"Acquire functional image","text":"<ul> <li>Inform the participant that we will start with the fMRI block, therefore the participant will start hearing sounds.</li> <li>Run the BEAT_LIBREoff_BOLD_audio_bis protocol by pressing *Continue.</li> </ul>"},{"location":"data-collection/data-collection_audio/#session-completed","title":"Session completed","text":"<ul> <li>The exam is over, inform the participant that the session has concluded.</li> <li>At the end of the stimulation click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> <li>You can proceed with the tear-down protocol.</li> </ul>"},{"location":"data-collection/data-collection_audio/#session-tear-down","title":"Session tear-down","text":""},{"location":"data-collection/data-collection_audio/#showing-the-participant-out","title":"Showing the participant out","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in few seconds.</li> <li> <p>Extract the participant by pressing the extraction button and then gently rolling the central knob. Alternatively, you can just press the Home button.  <li>Remove the upper side of the head coil:     <ul> <li>Unplug the head coil from the bed connector.</li> <li>Lift the lever that releases the upper part of the coil and put it aside (e.g., inside the bore or on a chair next to the scanner).</li> </ul> </li></p> </li> <li> <p>Assist the participant to the headphones.</p> </li> <li>Help the participant sit down.</li> <li>Help the participant step down and accompany them out to the control room.</li> <li>Help the participant recover their personal belongings and change clothes if necessary.</li> <li>Give the participant the corresponding compensation for the participation and transportation.</li> <li>Ask the participant to sign the receipt of the amount of the financial compensation.</li> </ul>"},{"location":"data-collection/data-collection_audio/#cleaning-up-the-scanning-room","title":"Cleaning up the Scanning Room","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in few seconds.</li> <li>Remove used blankets and bed-sheets ONE-BY-ONE: extend them to let any forgotten items fall on the floor before you fold it; and dispose of them in the adequate bin (soiled linen bag if they are fabric and trash if they are disposable).</li> <li>Dispose of all single-use sanitary protections (padding covers, earplugs, etc.).</li> <li>Put the pillows back in their designated storage places.</li> <li>Remove the head coil and put it in the scanner's bore.</li> <li>Remove the back padding elements and put them back in their designated storage.</li> <li>Reinstall the spine coil.</li> <li>Wipe the bed and the head coil (bottom and upper parts).</li> <li>Lock the head coil back with its bottom part without plugging the connectors.</li> <li>Put the head coil away with the other head-coils on the shelf next to the scanner.</li> <li>Return the bed to its Home position by pressing the  button (more info).</li> <li>Exit and close the external door.</li> </ul> <p>Everything that is removed for the experiment should be put back in place and the end of the experiment</p>"},{"location":"data-collection/data-collection_audio/#cleaning-up-the-control-room","title":"Cleaning up the Control room","text":"<ul> <li>Switch off laptop. Plug back the SyncBox and the VGA projector where they were. Make sure you leave it connected exactly as you found it.</li> <li>Switch off the projector.</li> </ul>"},{"location":"data-collection/data-collection_rs/","title":"Resting state fMRI protocol","text":"<p>Recruitment</p> <p>The recruitment section should clarify all aspects regarding how participants will be engaged within the study.</p> <ul> <li> Describe the location (e.g., university campus, hospital, community, city, zipcode area, etc.) of recruitment.</li> <li> Update this document describing the strategy for participant recruitment. Please beware that local ethical review boards may have some guidelines regarding the conditions that apply to recruitment tools and procedures. Some example strategies of recruitment are:<ul> <li>Consecutive ongoing recruitment through principal investigator in daily clinical practice.</li> <li>Participant recruitment through referring physician.</li> <li>Advertisement/flyer (in this case, add the corresponding files under <code>assets/</code> and link them from the body of this document).</li> </ul> </li> <li> Describe whether participants' information will be pulled from some existing database, or collected into a newly created database.   If a database or will be used, describe how to gain access to it and how to operate it.</li> <li> Describe the mechanisms to contact participants:<ul> <li>If contact if over telephone, write down a script for the call</li> <li>If contact is over e-mail or postal mail, write down the communication template</li> </ul> </li> </ul> <p>Screening</p> <p>Every new candidate participant must be screened to assess they meet all the inclusion criteria of your study, as well as do not meet any of the exclusion criteria.</p> <p>Your SOPs should clearly state:</p> <ul> <li> Refer to the inclusion and exclusion criteria of the study protocol, highlighting important aspects of the study (e.g., we are only investigating twins).</li> <li> Describe how the screening must be conducted.</li> <li> Prepare a checklist with all the elements that the candidate participant must be walked through if they accept to participate, for example:</li> <li> Confirm that participant does not have any contraindications<ul> <li> List of obvious implants and medical devices (e.g., peacemaker in an MRI experiment)</li> <li> List of less-obvious circumstances that may disqualify participation, such as non-removable orthodontia, or home-made tattoos.       Please note that minor implants (such as metal braces, retainers and some palatal expanders) that are sufficiently secured to the body may be safe on clinical MR scanners at 1.5T and 3T (tesla). However, these implants may originate artifacts on the generated images if they cause large-enough discontinuity of magnetic properties.</li> </ul> </li> <li> Confirm that participant will accept the participation conditions described in the informed consent form, for example:<ul> <li> If they will be required to abstain from eating, consuming coffee or smoking within a window of time prior to the exam.</li> <li> If their data will be openly shared after proper privacy protection measures.</li> <li> If they want to be informed about relevant incidental findings (and whether they can give up on their right to learn about them).</li> <li> Report life events that are relevant to the experiment and may be cause of stopping the participation for safety and health reasons (e.g., pregnancy) or other reasons (i.e., incompatibility with the study).</li> <li> Remind participant of any incompatibilities with the experiment that may apply (e.g., jewelry, removable orthodontics, some hairstyling products, etc.).</li> <li> If allowed to wear street clothes, remind participant to avoid clothing with metal or that would uncomfortable to lie in for the duration of the scan</li> <li> If participant has indicated nervousness or history of claustrophobia, or they belong to a population at risk (e.g., infants), anticipate the scheduling of a preparation session on a mock scanner prior to the first acquisition.</li> </ul> </li> </ul> <p>Scheduling</p> <p>Participants will likely need to visit your facilities for the development of the study in one or more occasions.</p> <ul> <li> State the number and nature of each of the sessions.</li> <li> Indicate how the scheduling should be conducted with each participant.</li> <li> Establish a system to store and share the information on how the research team will be made aware of the schedule and its changes</li> <li> Clearly explain how to book shared resources utilized by the study (e.g., MRI scanner).</li> </ul>"},{"location":"data-collection/data-collection_visuo/","title":"Visual checkerboard fMRI protocol","text":""},{"location":"data-collection/data-collection_visuo/#data-collection-visual-checkerboard-fmri-protocol","title":"Data Collection Visual checkerboard fMRI protocol","text":""},{"location":"data-collection/data-collection_visuo/#overall-experimental-setting","title":"Overall experimental setting","text":"<p>The experimental setup includes:</p> <ul> <li>Syncbox: A NordicLabs Syncbox receives TTL (transistor-transistor logic) triggers from the scanner. This box can forward the triggers converted into other formats and/or manipulate them (e.g., filter, generate, etc.).</li> <li> <p>Eye tracker (ET): We use the EyeLink 1000 Plus (SR Research Ltd., Ottawa, Canada). Our particular variant \u201cLong Range\u201d is composed of three main elements:   <ul> <li>inside the scanner's bore, we place an arm that holds an infrared lens and camera sensor on one side and an infrared lamp that illuminates the right eye of the subject through</li> <li>a special mirror to reflect the infrared spectrum;</li> <li>a PC tower that receives the camera recordings, post-processes the images, and calculates the final parameters of interest (position of the eye, pupil size, etc.).</li>        The ET is also connected to the experimental laptop.     </ul> </p> </li> <li> <p>Experimental laptop: It is the laptop where the Psychopy software is installed and with it the task programs are executed. This laptop also stores the data recorded by the ET at the end of the experiment.</p> </li> </ul> <p>ONE DAY BEFORE SCAN DATE</p> <ul> <li>Verify that all Psychopy experiments run correctly.</li> <li>Verify that all the tools necessary for the data collection run correctly \u2192 check that the eyelink machine is correctly working.</li> </ul> Configuration <p>If you connect the eye tracker to the experimental laptop for the first time, you need to configure it. - Go to the experimental laptop. - Double click on the ethernet connection inside the control panel.</p> <p></p> <ul> <li>Change the IP address \u2192 double click on the internet protocol version and change IP address with IP 100.1.1.2 and sub with 255.255.255.0.</li> </ul> <ul> <li>Print the informed consent form.</li> <li>Print the MRI safety screener (EN|FR).</li> <li>Print a receipt form for each participant that will get scanned.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#session-preparation","title":"Session preparation","text":"<p>The following section describes how to prepare the session on the day of the scan BEFORE the participant arrives. Try to arrive at the Control Room at least 30 min ahead of the session start time.</p>"},{"location":"data-collection/data-collection_visuo/#setup-preparation-inside-the-scanner-room","title":"Setup Preparation Inside the Scanner room","text":"<ul> <li>Memorize where the other tools for the recordings are to put those back in place at the end (coil, emergency button, ears cover).</li> </ul>"},{"location":"data-collection/data-collection_visuo/#setting-up-the-projector","title":"Setting up the projector","text":"<ul> <li>Before entering the scanner room, go to the room where the projector is installed.</li> <li>Switch the projector ON by hitting the power button located on its right side.</li> <li>Verify the aim of the projector's beam by looking through the tube into the Scanning Room.</li> <li>Verify the projection corresponds to the Psychopy laptop screen.</li> <li>Before exiting the projector room, grab the plexiglass panel where the ET arm will be placed inside the scanner.</li> <li>Take the panel to the Control Room.</li> <li>Go to the scanner room and take the half-circle one-direction screen from the table behind the scanner and put it on the back of the scanner.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#setting-up-the-eye-tracker","title":"Setting up the eye-tracker","text":"<ul> <li>Check out Eye Tracker set-up guidelin (ET_setup_3T_EyeLink_1000Plus).</li> <li>After accurately preparing the ET arm following the ET_setup_3T_EyeLink_1000Plus, bring the plexiglass panel inside the scanning room and place it at the end of the scanner's bore. A sign indicates the top side that MUST face up. The plastic feet must face down to avoid the panel from sliding. To ensure the repeatable positioning of the ET, place the end of the plexiglass such that its edge aligns with the edge of the illuminated MRI rails.</li> <li>Exit the Scanning Room and fetch the ET arm.</li> <li>Enter the Scanning Room and place the ET arm on top of the plexiglass panel with the two posterior feet of the ET arm aligned within the two corner markers made of scotch tape.</li> </ul> <p>Hold the ET arm FIRMLY because the magnetic field imposes some resistance</p> <ul> <li> <p>Unroll and connect the cables (two plugs for the black, one plug for the orange). </p> </li> <li> <p>Take the half-circle one-direction screen from the table behind the scanner and put it on the back of the scanner behind the ET system (don't push the plexiglass yet).</p> </li> </ul> <p></p>"},{"location":"data-collection/data-collection_visuo/#setting-up-the-coils","title":"Setting up the coils","text":"<ul> <li> <p>If any head coil from the last exam is still plugged, remove it:  <ul> <li>If it is the 64-channel coil, you can just temporarily move it into the scanner's bore.</li> <li>Otherwise, store it on the shelf where the other coils are and bring the 64-channel one in the proximity of the bed (e.g., inside the scanner's bore). Make sure to remove other coil's fitting elements.</li> </ul></p> </li> <li> <p>Remove the spine coil by lifting the corresponding latch, then sliding it toward the head of the bed, lift it from the bed and place it on the floor ensuring it is not obstructing any passage or unstable.</p> </li> <li>Place the two back padding elements filling the spine coil socket.</li> <li>Place the 64-channel head-and-neck coil into its socket at the head end of the bed.</li> <li>Attach the dedicated infrared mirror to the coil (see ET_setup_3T_EyeLink_1000Plus):  <ul> <li>Exit the Scanning Room.</li> <li>Fit in a pair of new latex gloves.</li> <li>Extract the dedicated infrared mirror from the ET box CAREFULLY.</li> <li>Remove the mirror protection EXTRA-CAREFULLY.</li> <li>Take the mirror, enter the Scanning Room and lock the mirror onto the frame of the head-coil.</li> </ul> </li> </ul>"},{"location":"data-collection/data-collection_visuo/#final-setting-inside-the-scanning-room","title":"Final setting inside the scanning room","text":"<ul> <li>Cover the MRI bed with a clean sheet.</li> <li> <p>Prepare padding: under-knee padding, neck-and-head padding, under-elbows padding, head-sides padding, top-head wedge padding.  <ul> <li>Wrap a sanitary cover around each padding.</li> <li>Place a double neck-and-head padding inside the coil, to ensure the eyes are close to the coil's windows.</li> </ul></p> </li> <li> <p>Prepare a blanket to cover the participant.</p> </li> <li>Prepare a new pair of earplugs.</li> <li>Completely disable the light inside of the scanner and the ventilation of the scanning room to facilitate the best performance of the ET.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#setup-preparation-inside-the-control-room","title":"Setup Preparation Inside the Control Room","text":""},{"location":"data-collection/data-collection_visuo/#setting-up-experiment-instruments","title":"Setting up experiment instruments","text":"<ul> <li>Arrive to the Control Room at least 30 min ahead the session start time.</li> <li>Place the experimental laptop on the designed desk and connect all the as showed in the following picture (in this case Jack audio is not necessary). Specifically, connect the experimental laptop to:  <ul> <li>Plug the power adaptor to the laptop, and the adaptor to the power outlet on the wall.</li> <li>The screen switch box with the corresponding HDMI cable (This should project your screen on the screen of CHUV's tower).</li> <li>The RJ-45/Ethernet cable from the ET computer into the RJ-45 socket of the experimental laptop.</li> <li>Connect the SyncBox to the laptop with the USB cable. It is normally plugged into CHUV's stimuli workstation, it must be re-plugged in there after the session.</li> </ul> </li> </ul> <ul> <li>Switch the laptop on and open the psychopy code.</li> <li>Click the switch button to share your PC.</li> <li>Switch the SyncBox on using the button on the right side.</li> <li>Change the SyncBox correctly to send the triggers (Corresponding to push the key-button \u201cS\u201d from keyboard). Take the SyncBox and go on \u201cSimulation\u201d mode.</li> <li>Then, change the parameters in the main menu modifying the pulse length at 100ms and the TR time at 650ms. </li> <li> <p>Push the enter button  and the syncbox will be now waiting for the scanner's trigger signal to forward it.  </p> </li> <li> <p>Open the door of the cable wardrobe between the recording room and the scanner room, and connect the sync box in the following way:</p> </li> </ul> <p> </p> <ul> <li>Switch on the ET\u2019s PC using the power-om button at the front</li> <li>Select \"Eyelink\" when given the option of which operating system to launch.  <li>Verify the IP address assigned to the Ethernet interface of the experimental laptop is correct:     <ul> <li>Check the output of the following command and verify that IP/mask is 100.1.1.2/24, and the protocol is IP version 4.</li> <li>Check whether the link is properly established.</li> </ul> </li> </li> </ul>"},{"location":"data-collection/data-collection_visuo/#when-the-participant-arrives","title":"When the Participant Arrives","text":""},{"location":"data-collection/data-collection_visuo/#participant-preparation","title":"Participant Preparation","text":""},{"location":"data-collection/data-collection_visuo/#participant-reception","title":"Participant Reception","text":"<ul> <li>Meet the participant at an easily locatable place (e.g., the reception desk of the Radiology Unit) and show them the way into the control room. Allow sufficient time before the experiment for the preparation.</li> <li>Show the participant the scanning room and explain to them how the device is controlled from outside.</li> <li>Ask the participant to fill out the consent form and MRI safety screener, and verbally confirm responses and discuss further doubts, paying attention to frequently forgotten devices and implants, like orthodontia.</li> <li>Remind the participant to use the bathroom at this moment if they need.</li> <li>Describe to the participant how the session will develop and explain clearly the task. Let them interrupt you to ask for clarifications and answer all the questions that may arise.</li> </ul> Script for the session <p>\u201cWe are going to acquire two types of images. The first type is anatomical imaging that we use to study the morphology of the brain. The second type is a functional MRI, which we use to understand how the brain activates as a response to stimuli we will present to you. During the whole duration of the exam, please do not create closed loops by crossing your legs or holding your hands together. It is possible that your peripheral nerves get stimulated at some points, so you will feel twitching of muscles, for instance, of your pectorals. Do not panic, it is okay, but if it feels too uncomfortable, please squeeze the alarm button.During the functional MRI you will see a point at the center of the screen that will change the colors. For the entire period of the experiment you should takes the eye open on this fixation point at the center of a gray environment. The experiment has a duration of 30 minutes more and less. Before to start with scanning we will need to calibrate the eyetracker, therefore we will ask you to fix different points on the screen.</p> <p>Is everything clear to you? Do you have any questions?\u201d</p> <ul> <li>Offer the participant a box to deposit everything they have in their pockets and all jewelry/hair accessories, and indicate that before continuing, we need to make sure we do not introduce any dangerous objects into the magnet room. Therefore, it is important to inform the participant to remove every metallic accessory.</li> <li>Ask to remove shoes at the entrance of the scanning room.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#participant-preparation-in-the-scanning-room","title":"Participant Preparation in the Scanning Room","text":"<ul> <li>Instruct the participant to lay on the MRI bed and adjust the participant inside. With the paddings, their head position MUST be adjusted and elevated so that the nose and the forehead of the participant are both close to the upper coil. This procedure ensures the ET has the clearest possible view of the eye.</li> <li>Give them the ear-plugs to protect their hearing during acquisition, allow time for them to place them.</li> <li>Give the participant the emergency button. Make the participant try it, so they can see it works. To switch off the alarm, there\u2019s a button on the scanner (circular, both on the left and on the right of the hole).</li> <li>Once the previous part is ensured, the participant is ready. If the participant is cold, put a blanket on top of him.</li> <li>Connect the coil's cable to the corresponding socket on the table.</li> <li>Gently move the participant with the manual regulation. Stop when the head is under the head-localizer. Ask the participant to close the eyes, localize the head with the infrareds.</li> <li>Switch off the infrareds, now the participant can open the eyes. You can move the participant (always gently as before) inside the scanner, until the mm counter marks \u201cIsometric\u201d.</li> <li>If everything is okay, you can move forward and record.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#final-preparations","title":"Final Preparations","text":"<ul> <li>Inform the participant that you are leaving the room and will shortly come back for a final preparation.</li> <li>Proceed with the ET positioning (see ET_setup_3T_EyeLink_1000Plus) and calibration.</li> </ul> ET Calibration <ul> <li>Open psychopy in the experimental laptop.If you connect the eye tracker to the experimental laptop for the first time, you need to configure it.</li> <li>Double click on the psychopy file of the rs protocol to open it.</li> <li>Run the experiment on psychopy clicking the \u201crun experiment\u201d button. IMPORTANT: make sure that once the experiment start after the calibration, the data are being stored to the xx.EDF file. There should be a message about that displayed at the ET\u2019s PC screen.</li> <li>Once the stimulation begins, you follow the messages on the screen to run the calibration (make sure the 5 points calibration has been selected).</li> <li>ET PC \u2013 apply threshold (left upper corner, as on the left figure), make sure that the pupil was found, and you see the blue cross on the eye. In case of troubles \u2013 check if there is enough light inside the scanner and not too much, check the position of the participant inside the coil. Once the calibration starts accept calibration points when green (Accept fixation).</li> </ul> <ul> <li>If the calibration was successful, you will see the sentence \u2018calibration successful\u2019 at the bottom in green. Check the stability of the accepted points and overall score o the calibration.</li> <li>Follow up with the validation. What you should see in an ideal situation is: the reference dot on the center of the screen and another dot, which correspond to the pupil calibration, that it is more and less stable moving a little around the reference dot. If the calibration dot is unstable and is moving around far from the reference dot the experimenter should go back clicking the restart button, adjust the contrast and redo the calibration. Once the calibration dot is quite stable proceed with the validation clicking the accept fixation. </li> </ul> <ul> <li>Inform the participant that you are leaving the room and will now close the door to start. Let them also know that you are going to communicate with them very shortly to check that communications through the speaker are functioning.</li> <li>Exit the Scanning Room.</li> <li>Close the Scanning Room door.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#running-the-scanning-session","title":"Running the Scanning Session","text":"<p>You MUST know the security procedures in case of problem and keep yourself updated with changes</p>"},{"location":"data-collection/data-collection_visuo/#before-initiating-the-session-run-the-experiment","title":"Before Initiating the Session: Run the Experiment","text":"<ul> <li>At the end of the ET calibration we are ready to run the experiment.</li> <li>Wait for the sentence \u201cIn this task you will see a color dot. Please keep your eyes on the fixation point. The program is ready for the scanner trigger. Press s to proceed manually.\u201d</li> <li>Then click \u201cstart session\u201d on the sync box clicking the round button. </li> </ul> <ul> <li>The stimulation will start with the scanning. At the end of the experiment click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#during-the-session","title":"During the Session","text":"<ul> <li>Check in with the participant.</li> <li>Check in that everything is correctly working (e.g., The pupil is correctly detected).</li> <li>Watch for motion and arousal state using the ET's camera. If you detect motion or the participant falls asleep at inadequate points, use the speaker to inform them.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#acquire-a-localizer","title":"Acquire a Localizer","text":"<ul> <li>Indicate the participant that the scanning will soon start.</li> <li>Wait for the participant confirmation and set the speaker off afterward.</li> <li>Launch the AAhead_scout_64ch-head-coil protocol by pressing Continue.</li> <li>Once the localizer is concluded, click on the image stack icon with left click and drag the image with a 1 onto the image viewer. That will open the interpolated localizer on the viewer.</li> <li>If the quality looks good, check the box stating Localizer looked ok. If not, re-acquire the localizer.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#acquire-a-high-resolution-anatomical-image","title":"Acquire a High-Resolution Anatomical Image","text":"<ul> <li>Run the wip19_mprage_1iso_cs4p2 protocol by pressing Continue.</li> </ul> Anatomical image Acquisition <ul> <li>While you are still running the MPRAGE sequence open the parameters of the sequence and ensure that:</li> <li>under Sequence \u2937 Part1, the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.</li> <li>under Routine, TR and TE should be set at the minimum value. the shot per slice is 419. This is crucial so that so that the acquisition time is more and less 1 minute.</li> <li>under Contrast \u2937 common, TR and TE should be set at the minimum value. The fat-water should be set as standard and the flip angle should be set to 5 degree.</li> <li>under Contrast \u2937 filter, click on the three dots and tick the \u201cunfiltered images\u201d.</li> <li>under System \u2937 coils, select the coils HC3, HC5, HC4, HC6, HC7.</li> <li>under System \u2937 miscellaneous, Put the coil selection as manual.</li> <li>under Physio, make sure that RECONSTRUCTION is off. </li> <li>Finally, click copy and go button</li> </ul>"},{"location":"data-collection/data-collection_visuo/#acquire-functional-image","title":"Acquire Functional Image","text":"<ul> <li>Inform the participant that we will start with the fMRI block, therefore the participant will start hearing sounds.</li> <li>Run the BEAT_LIBREoff_BOLD_audio_bis protocol by pressing Continue.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#session-completed","title":"Session Completed","text":"<ul> <li>The exam is over, inform the participant that the session has concluded.</li> <li>At the end of the stimulation click \u201ct\u201d on the experimental laptop and click the round button on the SyncBox to stop the running session.</li> <li>You can proceed with the tear-down protocol.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#session-tear-down","title":"Session Tear-Down","text":""},{"location":"data-collection/data-collection_visuo/#showing-the-participant-out","title":"Showing the Participant Out","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in a few seconds.</li> <li> <p>Extract the participant by pressing the extraction button and then gently rolling the central knob. Alternatively, you can just press the Home button.  <li>Remove the upper side of the head coil:     <ul> <li>Unplug the head coil from the bed connector.</li> <li>Lift the lever that releases the upper part of the coil and put it aside (e.g., inside the bore or on a chair next to the scanner).</li> </ul> </li></p> </li> <li> <p>Assist the participant to the headphones.</p> </li> <li>Help the participant sit down.</li> <li>Help the participant step down and accompany them out to the control room.</li> <li>Help the participant recover their personal belongings and change clothes if necessary.</li> <li>Give the participant the corresponding compensation for the participation and transportation.</li> <li>Ask the participant to sign the receipt of the amount of the financial compensation.</li> </ul>"},{"location":"data-collection/data-collection_visuo/#cleaning-up-the-scanning-room","title":"Cleaning up the Scanning Room","text":"<ul> <li>Enter the scanner room, and announce yourself to the participant saying that you will get out the participant in a few seconds.</li> <li>Remove used blankets and bed-sheets ONE-BY-ONE: extend them to let any forgotten items fall on the floor before you fold it; and dispose of them in the adequate bin (soiled linen bag if they are fabric and trash if they are disposable).</li> <li>Dispose of all single-use sanitary protections (padding covers, earplugs, etc.).</li> <li>Put the pillows back in their designated storage places.</li> <li>Remove the head coil and put it in the scanner's bore.</li> <li>Remove the back padding elements and put them back in their designated storage.</li> <li>Reinstall the spine coil.</li> <li>Wipe the bed and the head coil (bottom and upper parts).</li> <li>Lock the head coil back with its bottom part without plugging the connectors.</li> <li>Put the head coil away with the other head-coils on the shelf next to the scanner.</li> <li>Return the bed to its Home position by pressing the button (more info).</li> <li>Exit and close the external door.</li> </ul> <p>Everything that is removed for the experiment needs to be put back in place at the end of the experiment, i.e., position of the bed, coil, emergency button, ears cover.</p>"},{"location":"data-collection/data-collection_visuo/#cleaning-up-the-control-room","title":"Cleaning up the Control Room","text":"<ul> <li>Switch off the laptop and ET PC Tower. Plug back the SyncBox and the VGA projector where they were. Make sure you leave it connected exactly as you found it.</li> <li>Switch off the projector.</li> </ul>"},{"location":"data-management/0_overall-data-workflow/","title":"Overall data workflow","text":"<p>The data workflow can be divided into two parts: (figure is editing) https://docs.google.com/drawings/d/1J4RUVk647YXuAsq7B-6wf6nKiEy5rKnjl5h4MKooOrw/edit</p> <ul> <li>MRI data. The raw MRI data should be first stored on the HES-SO server. <ul> <li>We recommend to convert the raw MRI data to BIDS. Since from there, the BIDS compliant data can be quality controlled and pre-processed using the corresponding packages (e.g. MRIQC, fMRIPrep, dMRIPrep) to allow computation of analysis-grade derivatives (e.g. functional or structural connectivity). </li> <li> The code snippets for converting MRI raw data into BIDS</li> </ul> </li> <li> <p>Eye tracking data. The eye tracking data is automatically generated and store in the PsychoPy laptop. We should store the edf data in the folder of the MRI dataset.</p> </li> <li> <p>Twix physio data. If we need to extract the physio information from the raw data, we should also store the physio data in the subfolder <code>Twix</code> inside the dataset folder.</p> </li> <li> <p>Example folder structure should be followed as below:</p> </li> </ul> <pre><code>MR-Eye/\n    \u251c\u2500\u2500 code/\n    \u2502   \u251c\u2500\u2500 git_repo_specific_for_this_project/\n    \u2502   \u2502   \u251c\u2500\u2500 README.md  # Add a detailed description of the repository here\n    \u2502   \u2502   \u251c\u2500\u2500 [Your code files for this specific project go here]\n    \u2502   \u2502   \u2514\u2500\u2500 .gitignore  # Include entries to exclude commonly-used packages like Monalisa\n    \u2502\n    \u251c\u2500\u2500 data/\n    \u2502   \u251c\u2500\u2500 update_protocol_description.md\n    \u2502   \u251c\u2500\u2500 subject_table.xlsx\n    \u2502   \u251c\u2500\u2500 final_fixed_protocol.md\n    \u2502   \u251c\u2500\u2500 pilot_/\n    \u2502   \u2502   \u251c\u2500\u2500 year-month-date/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 subject00x/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 [Relevant pilot subject data files]\n    \u2502   \u2502   \u2514\u2500\u2500 [Additional subjects as needed]\n    \u2502   \u251c\u2500\u2500 final_protocol/\n    \u2502   \u2502   \u251c\u2500\u2500 subject001/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 mri/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dicom/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 raw_data/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ISMRMD/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 bids/  # (TBD: Decide on the structure)\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 physio/\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 others/\n    \u2502   \u2502   \u2514\u2500\u2500 [Additional subjects as needed]\n    \u2502\n    \u251c\u2500\u2500 output/\n    \u2502   \u2514\u2500\u2500 [Generated output files go here]\n    \u2502\n    \u251c\u2500\u2500 .gitignore  # Add patterns to exclude unnecessary files globally, like logs or temp files\n    \u2514\u2500\u2500 README.md  # Main project description with an overview, usage, and other details.\n</code></pre>"},{"location":"data-management/0_software_install/","title":"Software installation","text":""},{"location":"data-management/0_software_install/#1-eyelink-installation","title":"1 EyeLink Installation","text":"<ul> <li>The EyeLink MUST be installed before PyEDFRead and Pychopy.</li> <li>Register a new account for downloading the EyeLink Developers Kits. Link for installation.</li> <li>The activation of the new account might take up for 24 hours.</li> <li>Install the EyeLink Developer Kit and EyeLink Data Viewer following the instructions.   </li> </ul>"},{"location":"data-management/0_software_install/#2-additional-steps","title":"2 Additional Steps","text":""},{"location":"data-management/0_software_install/#21-for-windows-platform","title":"2.1 For Windows Platform","text":"<ul> <li>Ensure you have installed the Microsoft C++ Build Tools.</li> <li>You can download them from the Visual Studio website.</li> <li>Make sure to select the \"Desktop development with C++\" workload during the installation.</li> <li>Properly set up the environment variable:<ul> <li>For example on Windows system <pre><code>D:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.40.33807\\bin\\Hostx64\\x64\nD:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.40.33807\\include\n</code></pre></li> </ul> </li> <li>Modify the files in the directory of EyeLink for supporting Windows</li> <li>Edit edftypes.h in the dir: C:\\Program Files (x86)\\SR Research\\EyeLink\\Includes\\eyelink   </li> </ul>"},{"location":"data-management/0_software_install/#22-for-mac-platform","title":"2.2 For Mac Platform","text":"<p>Install the EyeLink Developer Kit and EyeLink Data Viewer following the instructions on the website.</p>"},{"location":"data-management/0_software_install/#23-for-linux-platform","title":"2.3 For Linux Platform","text":"<p>This part is derived from HCPH SOP. Since we did not work on eye tracking data with Linux system, we refer to the HCPH SOP for completeness.</p> <ul> <li>Enable Canonical's universe repository with the following command:   <pre><code>sudo add-apt-repository universe\nsudo apt update\n</code></pre></li> <li>Install and update the ca-certificates package:   <pre><code>sudo apt update\nsudo apt install ca-certificates\n</code></pre></li> <li>Add the SR Research Software Repository signing key:   <pre><code>curl -sS https://apt.sr-research.com/SRResearch_key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/sr-research.gpg\n</code></pre></li> <li>Add the SR Research Software Repository as an Aptitude source:     <pre><code>sudo apt install eyelink-display-software\n</code></pre></li> <li>Install the EyeLink Data Viewer:   <pre><code>sudo apt install eyelink-dataviewer\n</code></pre></li> </ul>"},{"location":"data-management/0_software_install/#3-virtual-environment-preparation","title":"3 Virtual Environment Preparation","text":"<ul> <li> <p>Prepare conda virtual env, with python version above 3.7. Some unexpected issues will occur if the python version=3.6 or below.     <pre><code>$ conda create -n edfenv python=3.8\n$ conda activate edfenv\n$ pip install cython\n$ pip install pandas\n$ pip install h5py\n</code></pre></p> </li> <li> <p>Install pyedfread from the original repo:   <pre><code>$ pip install git+https://github.com/s-ccs/pyedfread\n</code></pre></p> </li> </ul>"},{"location":"data-management/1_data_export/","title":"Data export","text":""},{"location":"data-management/1_data_export/#collect-et-data","title":"Collect ET data","text":"<p>Copy data from the subfolder of PsychoPy program into the hard drive.</p> <p>Tip</p> <p>The subfolder should include the files for one session with the formats like </p> <ul> <li><code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.EDF</code> </li> <li><code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.csv</code> </li> <li><code>000001_fixed_dot-16_grid_T1w_2024-10-14_17h24.37.511.log</code></li> </ul> <p>where '**.EDF' files are eye tracking records, <code>.csv</code> and <code>.log</code> files logs the task messages and the corresponding timestamps.\"</p>"},{"location":"data-management/1_data_export/#collect-mri-raw-data","title":"Collect MRI raw data","text":"<p>Export Twix raw data - Twix: Username: medadmin, Password: adm\\(pwd\\)4\\(med\\). - Press <code>Ctrl+Esc</code> to open the IDE terminal. - In the IDE terminal: ideacmdtool -&gt; type <code>4</code> -&gt; type <code>6</code> // - Type <code>Twix</code> and the Twix data browser opens. - Select the data you want to copy, right click on the mouse -&gt; Copy Total RAID file -&gt; select destination (your hard drive). -  The only useful thing is the physio [select the flag on External Signal] need more explanation </p> <p>Export the DICOM data (directly reconstructed images from the scanner) - Login as SuperUser by pressing <code>Tab</code> + <code>Delete</code> + <code>(Bottone a DX - 9)</code> to enter the advance mode. - Export DICOM: select the patient, go to export // File System // Browse -&gt; select <code>HD</code>. - Select the \u201cEnhanced\u201d option (1 DICOM  / volume) instead of Interoperability (1 DICOM / slice).</p> <p>The default option is <code>Interoperability</code>! So we have to change it manually!</p>"},{"location":"data-management/2_et1_edf_to_bids/","title":"EDF conversion","text":"<p>Derived from edf_to_bids and convert.py Hsop: Converting eye-tracking into BIDS - Standard Operating Procedures of the HCPh project (axonlab.org) TheAxonLab/hcph-sops (github.com)</p> <p>EyeLink eye tracking system produces EDF recording files. In this step we need to first convert the raw edf files to BIDS format for enforcing a standardized structure, naming convention and metadata description. It makes researchers easier to understand and use data from different sources.</p>"},{"location":"data-management/2_et1_edf_to_bids/#0-package-preparation","title":"0 Package preparation","text":"<ul> <li> <p>To use pyEDFRead python package, we must properly install the open-source software EyeLink on our device/laptop in the very first step. You can find the instructions here.</p> </li> <li> <p>Import the environments</p> </li> </ul> <pre><code>from __future__ import annotations \nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom pyedfread import read_edf\nfrom collections import defaultdict\nfrom itertools import product, groupby\nfrom warnings import warn\nimport re\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#1-load-raw-et-data","title":"1 Load raw ET data","text":"<ul> <li>Use <code>pyedfread</code> package to open edf and </li> </ul> <p><pre><code>DATA_PATH = Path(\"/Path/to/EDF folder\")\nedf_name = f\"file1.EDF\"\n\nfile_path = str(DATA_PATH / edf_name)\nprint(file_path)\nori_recording, ori_events, ori_messages = read_edf(file_path)\n</code></pre> The edf file will generate Pandas dataframes:</p> <ul> <li><code>ori_recording</code>: The ET recordings with trajectory information, pupil area and other information.</li> <li><code>ori_events</code>: contains information of task events.</li> <li><code>ori_messages</code>: log messages including ET calibration, validation and user-defined task messages sent from Psychopy program to the device.</li> </ul> <pre><code>ori_messages = ori_messages.rename(\n    columns={\n        # Normalize weird header names generated by pyedfread\n        \"message\": \"trialid\",\n        \"trial\": \"trial\",\n        # Convert some BIDS columns\n        \"time\": \"timestamp\",\n    }\n)\n\nrecording = ori_recording\nmessages = ori_messages\nevents = ori_events\nprint(f'\\nThe entire info of `message`: \\n{messages}')\nrecording.columns\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#2-parsing-the-messages","title":"2 Parsing the messages","text":"<ul> <li>Drop the duplicated codes. <pre><code>messages = messages.rename(\n    columns={c: c.strip() for c in messages.columns.values}\n).drop_duplicates()\n</code></pre></li> <li>Check the information about calibration of ET data.</li> </ul> <pre><code># Extract calibration headers\n_cal_hdr = ori_messages.trialid.str.startswith(\"!CAL\")\ncalibration = ori_messages[_cal_hdr]\n# messages = messages.drop(messages.index[_cal_hdr])\nprint(calibration)\n</code></pre> <ul> <li> <p>Extracting the start time and stop time from metadata. If no information is extracted from metadata, we will keep them as <code>None</code>. <pre><code># Extracting the StartTime and StopTime metadata.\nmessage_first_trigger = 'MODE RECORD'\nmessage_last_trigger = 'end'\nmetadata = {\n    'StopTime': None,\n    'StartTime': None\n}\n\n# Find Start time\nstart_rows = messages.trialid.str.contains(\n    message_first_trigger, case=False, regex=True\n)\nstop_rows = messages.trialid.str.contains(\n    message_last_trigger, case=False, regex=True\n)\n\n\n# Extract calibration headers\n_cal_hdr = messages.trialid.str.startswith(\"!CAL\")\ncalibration = messages[_cal_hdr]\nmessages = messages.drop(messages.index[_cal_hdr])\n\n# Pick the LAST of the start messages\nmetadata[\"StartTime\"] = (\n    int(messages[start_rows].timestamp.values[-1])\n    if start_rows.any()\n    else None\n)\n\n# Pick the FIRST of the stop messages\nmetadata[\"StopTime\"] = (\n    int(messages[stop_rows].timestamp.values[0])\n    if stop_rows.any()\n    else None\n)\n\n# Drop start and stop messages from messages dataframe\nmessages = messages.loc[~start_rows &amp; ~stop_rows, :]\n</code></pre></p> </li> <li> <p>Extracting basic metadata <pre><code># Extracting basic metadata.\n# !MODE RECORD CR 1000 2 0 R\n\nmode_record = messages.trialid.str.startswith(\"!MODE RECORD\")\n\nmeta_record = {\n    \"freq\": DEFAULT_FREQUENCY,\n    \"mode\": DEFAULT_MODE,\n    \"eye\": DEFAULT_EYE,\n}\n\nif mode_record.any():\n    try:\n        meta_record = re.match(\n            r\"\\!MODE RECORD (?P&lt;mode&gt;\\w+) (?P&lt;freq&gt;\\d+) \\d \\d (?P&lt;eye&gt;[RL]+)\",\n            messages[mode_record].trialid.iloc[-1].strip(),\n        ).groupdict()\n\n        meta_record[\"eye\"] = EYE_CODE_MAP[meta_record[\"eye\"]]\n        meta_record[\"mode\"] = (\n            \"P-CR\" if meta_record[\"mode\"] == \"CR\" else meta_record[\"mode\"]\n        )\n    except AttributeError:\n        warn(\n            \"Error extracting !MODE RECORD message, \"\n            \"using default frequency, mode, and eye\"\n        )\n    finally:\n        messages = messages.loc[~mode_record]\n\neye = (\n    (\"right\", \"left\") if meta_record[\"eye\"] == \"both\" else (meta_record[\"eye\"],)\n)\n\nmetadata[\"SamplingFrequency\"] = int(meta_record[\"freq\"])\nmetadata[\"EyeTrackingMethod\"] = meta_record[\"mode\"]\nmetadata[\"RecordedEye\"] = meta_record[\"eye\"]\n</code></pre></p> </li> <li> <p>Extracting screen parameters <pre><code># Extracting screen parameters.\n# GAZE_COORDS 0.00 0.00 800.00 600.00\n\n# Extract GAZE_COORDS message signaling start of recording\ngaze_msg = messages.trialid.str.startswith(\"GAZE_COORDS\")\n\nmetadata[\"ScreenAOIDefinition\"] = [\n    \"square\",\n    DEFAULT_SCREEN,\n]\nif gaze_msg.any():\n    try:\n        gaze_record = re.match(\n            r\"GAZE_COORDS (\\d+\\.\\d+) (\\d+\\.\\d+) (\\d+\\.\\d+) (\\d+\\.\\d+)\",\n            messages[gaze_msg].trialid.iloc[-1].strip(),\n        ).groups()\n        metadata[\"ScreenAOIDefinition\"][1] = [\n            int(round(float(gaze_record[0]))),\n            int(round(float(gaze_record[2]))),\n            int(round(float(gaze_record[1]))),\n            int(round(float(gaze_record[3]))),\n        ]\n    except AttributeError:\n        warn(\"Error extracting GAZE_COORDS\")\n    finally:\n        messages = messages.loc[~gaze_msg]\n\nprint(metadata)\n</code></pre></p> </li> <li> <p>Extracting parameters of the pupil fit model.</p> </li> </ul> <pre><code># Extracting parameters of the pupil fit model.\n# ELCL_PROC ELLIPSE (5)\n# ELCL_EFIT_PARAMS 1.01 4.00  0.15 0.05  0.65 0.65  0.00 0.00 0.30\n# Extract ELCL_PROC AND ELCL_EFIT_PARAMS to extract pupil fit method\npupilfit_msg = messages.trialid.str.startswith(\"ELCL_PROC\")\n\nif pupilfit_msg.any():\n    try:\n        pupilfit_method = [\n            val\n            for val in messages[pupilfit_msg]\n            .trialid.iloc[-1]\n            .strip()\n            .split(\" \")[1:]\n            if val\n        ]\n        metadata[\"PupilFitMethod\"] = pupilfit_method[0].lower()\n        metadata[\"PupilFitMethodNumberOfParameters\"] = int(\n            pupilfit_method[1].strip(\"(\").strip(\")\")\n        )\n    except AttributeError:\n        warn(\"Error extracting ELCL_PROC (pupil fitting method)\")\n    finally:\n        messages = messages.loc[~pupilfit_msg]\n\npupilfit_msg_params = messages.trialid.str.startswith(\"ELCL_EFIT_PARAMS\")\nif pupilfit_msg_params.any():\n    rows = messages[pupilfit_msg_params]\n    row = rows.trialid.values[-1].strip().split(\" \")[1:]\n    try:\n        metadata[\"PupilFitParameters\"] = [\n            tuple(float(val) for val in vals)\n            for k, vals in groupby(row, key=bool)\n            if k\n        ]\n    except AttributeError:\n        warn(\"Error extracting ELCL_EFIT_PARAMS (pupil fitting parameters)\")\n    finally:\n        messages = messages.loc[~pupilfit_msg_params]     \n</code></pre> <ul> <li>Parsing validation messages</li> </ul> <pre><code># Calibration validation.\n# VALIDATE R 4POINT 4 RIGHT at 752,300 OFFSET 0.35 deg. -8.7,-3.8 pix.\n# Extract VALIDATE messages for a calibration validation\nvalidation_msg = messages.trialid.str.startswith(\"VALIDATE\")\n\nif validation_msg.any():\n    metadata[\"ValidationPosition\"] = []\n    metadata[\"ValidationErrors\"] = []\n\nfor i_row, validate_row in enumerate(messages[validation_msg].trialid.values):\n    prefix, suffix = validate_row.split(\"OFFSET\")\n    validation_eye = (\n        f\"eye{eye.index('right') + 1}\"\n        if \"RIGHT\" in prefix\n        else f\"eye{eye.index('left') + 1}\"\n    )\n    validation_coords = [\n        int(val.strip())\n        for val in prefix.rsplit(\"at\", 1)[-1].split(\",\")\n        if val.strip()\n    ]\n    metadata[\"ValidationPosition\"].append(\n        [validation_eye, validation_coords]\n    )\n\n    validate_values = [\n        float(val)\n        for val in re.match(\n            r\"(-?\\d+\\.\\d+) deg\\.\\s+(-?\\d+\\.\\d+),(-?\\d+\\.\\d+) pix\\.\",\n            suffix.strip(),\n        ).groups()\n    ]\n\n    metadata[\"ValidationErrors\"].append(\n        (validation_eye, validate_values[0], tuple(validate_values[1:]))\n    )\nmessages = messages.loc[~validation_msg]\n\nprint(messages)\nprint(metadata)\n</code></pre> <ul> <li>Extract final bits of metadata and THRESHOLDS messages prior to the recording. <pre><code>thresholds_msg = messages.trialid.str.startswith(\"THRESHOLDS\")\nif thresholds_msg.any():\n    metadata[\"PupilThreshold\"] = [None] * len(eye)\n    metadata[\"CornealReflectionThreshold\"] = [None] * len(eye)\n    thresholds_chunks = (\n        messages[thresholds_msg].trialid.iloc[-1].strip().split(\" \")[1:]\n    )\n    eye_index = eye.index(EYE_CODE_MAP[thresholds_chunks[0]])\n    metadata[\"PupilThreshold\"][eye_index] = int(thresholds_chunks[-2])\n    metadata[\"CornealReflectionThreshold\"][eye_index] = int(\n        thresholds_chunks[-1]\n    )\nmessages = messages.loc[~thresholds_msg]\nprint(messages)\nprint(metadata)\n</code></pre></li> <li>Consume the remaining messages <pre><code>if not messages.empty:\n    metadata[\"LoggedMessages\"] = [\n        (int(msg_timestamp), msg.strip())\n        for msg_timestamp, msg in messages[[\"timestamp\", \"trialid\"]].values\n    ]\n\nprint(messages)\nprint(metadata)\n</code></pre></li> </ul>"},{"location":"data-management/2_et1_edf_to_bids/#3-parsing-the-recording-dataframe","title":"3 Parsing the recording dataframe","text":"<pre><code>recording = ori_recording\n</code></pre> <ul> <li>Curation of the input dataframe</li> </ul> <pre><code># Normalize timestamps (should be int and strictly positive)\nrecording = recording.astype({\"time\": int})\nrecording = recording[recording[\"time\"] &gt; 0]\nraw_recording_len = len(recording)\nprint(f'raw_recording length: {raw_recording_len}')\n\nrecording = recording.rename(\n    columns={\n#         # Fix buggy header names generated by pyedfread\n#         \"fhxyvel\": \"fhxvel\",\n#         \"frxyvel\": \"frxvel\",\n        # Normalize weird header names generated by pyedfread\n        \"rx\": \"screen_ppdeg_x_coordinate\",\n        \"ry\": \"screen_ppdeg_y_coordinate\",\n        # Convert some BIDS columns\n        \"time\": \"timestamp\",\n    }\n)\n\n# Split extra columns from the dataframe\nextra = recording[[\"flags\", \"input\", \"htype\"]]\nrecording = recording.drop(columns=[\"flags\", \"input\", \"htype\"])\nprint(len(recording))\n\n# Remove columns that are always very close to zero\nrecording = recording.loc[:, (recording.abs() &gt; 1e-8).any(axis=0)]\n# Remove columns that are always 1e8 or more\nrecording = recording.loc[:, (recording.abs() &lt; 1e8).any(axis=0)]\n# Replace unreasonably high values with NaNs\nrecording = recording.replace({1e8: np.nan})\n\nassert len(recording) == raw_recording_len\n</code></pre> <ul> <li>Clean-up pupil size and gaze position</li> </ul> <pre><code># These are the parameters we most likely we care for, so special curation is applied:\nscreen_resolution = [800, 600]\n\nfor eyenum, eyename in enumerate(eye):\n    # Clean-up implausible values for pupil area (pa)\n    recording.loc[\n        recording[f\"pa_{eyename}\"] &lt; 1, f\"pa_{eyename}\"\n    ] = np.nan\n    recording = recording.rename(\n        columns={f\"pa_{eyename}\": f\"eye{eyenum + 1}_pupil_size\"}\n    )\n    print(f\"pa_{eyename} renamed as: eye{eyenum + 1}_pupil_size\")\n    # Clean-up implausible values for gaze x position\n    recording.loc[\n        (recording[f\"gx_{eyename}\"] &lt; 0)\n        | (recording[f\"gx_{eyename}\"] &gt; screen_resolution[0]),\n        f\"gx_{eyename}\",\n    ] = np.nan\n    # Clean-up implausible values for gaze y position\n    recording.loc[\n        (recording[f\"gy_{eyename}\"] &lt;= 0)\n        | (recording[f\"gy_{eyename}\"] &gt; screen_resolution[1]),\n        f\"gy_{eyename}\",\n    ] = np.nan\n\nprint(recording)\nassert len(recording) == raw_recording_len\n</code></pre> <ul> <li>Shape the columns to comply with BIDS format.</li> </ul> <pre><code># Munging columns to comply with BIDS. \n# At this point, the dataframe is almost ready for writing out as BIDS.\n# Interpolate BIDS column names\ncolumns = list(\n    set(recording.columns)\n    - set(\n        (\n            \"timestamp\",\n            \"screen_ppdeg_x_coordinate\",\n            \"screen_ppdeg_y_coordinate\",\n            \"eye1_pupil_size\",#pa\n            \"eye2_pupil_size\",#pa\n        )\n    )\n)\nbids_columns = []\nfor eyenum, eyename in enumerate(eye):\n    for name in columns:\n        colprefix = f\"eye{eyenum + 1}\" if name.endswith(f\"_{eyename}\") else \"\"\n        _newname = name.split(\"_\")[0]\n        _newname = re.sub(r\"([xy])$\", r\"_\\1_coordinate\", _newname)\n        _newname = re.sub(r\"([xy])vel$\", r\"_\\1_velocity\", _newname)\n        _newname = _newname.split(\"_\", 1)\n        _newname[0] = EDF2BIDS_COLUMNS[_newname[0]]\n        _newname.insert(0, colprefix)\n        bids_columns.append(\"_\".join((_n for _n in _newname if _n)))\n\n# Rename columns to be BIDS-compliant\nrecording = recording.rename(columns=dict(zip(columns, bids_columns)))\n\n# Reorder columns to render nicely (tracking first, pupil size after)\ncolumns = sorted(\n    set(recording.columns.values).intersection(BIDS_COLUMNS_ORDER),\n    key=lambda entry: BIDS_COLUMNS_ORDER.index(entry),\n)\ncolumns += [c for c in recording.columns.values if c not in columns]\nrecording = recording.reindex(columns=columns)\n\nprint(recording)\nassert len(recording) == raw_recording_len\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#4-parsing-the-calibration-messages","title":"4 Parsing the calibration messages","text":"<pre><code># Parse calibration metadata\nmetadata[\"CalibrationCount\"] = 0\nif not calibration.empty:\n    warn(\"Calibration of more than one eye is not implemented\")\n    calibration.trialid = calibration.trialid.str.replace(\"!CAL\", \"\")\n    calibration.trialid = calibration.trialid.str.strip()\n\n    metadata[\"CalibrationLog\"] = list(\n        zip(\n            calibration.timestamp.values.astype(int),\n            calibration.trialid.values,\n        )\n    )\n\n    calibrations_msg = calibration.trialid.str.startswith(\n        \"VALIDATION\"\n    ) &amp; calibration.trialid.str.contains(\"ERROR\")\n    metadata[\"CalibrationCount\"] = calibrations_msg.sum()\n\n    calibration_last = calibration.index[calibrations_msg][-1]\n    try:\n        meta_calib = re.match(\n            r\"VALIDATION (?P&lt;ctype&gt;[\\w\\d]+) (?P&lt;eyeid&gt;[RL]+) (?P&lt;eye&gt;RIGHT|LEFT) \"\n            r\"(?P&lt;result&gt;\\w+) ERROR (?P&lt;avg&gt;-?\\d+\\.\\d+) avg\\. (?P&lt;max&gt;-?\\d+\\.\\d+) max\\s+\"\n            r\"OFFSET (?P&lt;offsetdeg&gt;-?\\d+\\.\\d+) deg\\. \"\n            r\"(?P&lt;offsetxpix&gt;-?\\d+\\.\\d+),(?P&lt;offsetypix&gt;-?\\d+\\.\\d+) pix\\.\",\n            calibration.loc[calibration_last, \"trialid\"].strip(),\n        ).groupdict()\n\n        metadata[\"CalibrationType\"] = meta_calib[\"ctype\"]\n        metadata[\"AverageCalibrationError\"] = [float(meta_calib[\"avg\"])]\n        metadata[\"MaximalCalibrationError\"] = [float(meta_calib[\"max\"])]\n        metadata[\"CalibrationResultQuality\"] = [meta_calib[\"result\"]]\n        metadata[\"CalibrationResultOffset\"] = [\n            float(meta_calib[\"offsetdeg\"]),\n            (float(meta_calib[\"offsetxpix\"]), float(meta_calib[\"offsetypix\"])),\n        ]\n        metadata[\"CalibrationResultOffsetUnits\"] = [\"deg\", \"pixels\"]\n    except AttributeError:\n        warn(\"Calibration data found but unsuccessfully parsed for results\")\n\n\nprint(calibration)\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#5-parsing-the-events-dataframe","title":"5 Parsing the events dataframe","text":"<p>There are three types of eye movements:</p> <ul> <li>fixation </li> <li>saccade</li> <li>blinks</li> </ul> <p>The mask for each event is recorded, with the mask value indicating whether an event occurred at a specific timestamp. A mask value of 1 at a given timestamp signifies that the event was detected at that moment.</p> <pre><code># print(events)\nprint(recording)\n\n# Process events: first generate empty columns\nrecording[\"eye1_fixation\"] = 0\nrecording[\"eye1_saccade\"] = 0\nrecording[\"eye1_blink\"] = 0\n\n# Add fixations\nfor _, fixation_event in events[\n    events[\"type\"] == \"fixation\"\n].iterrows():\n    recording.loc[\n        (recording[\"timestamp\"] &gt;= fixation_event[\"start\"])\n        &amp; (recording[\"timestamp\"] &lt;= fixation_event[\"end\"]),\n        \"eye1_fixation\",\n    ] = 1\n\n# Add saccades, and blinks, which are a sub-event of saccades\nfor _, saccade_event in events[\n    events[\"type\"] == \"saccade\"\n].iterrows():\n    recording.loc[\n        (recording[\"timestamp\"] &gt;= saccade_event[\"start\"])\n        &amp; (recording[\"timestamp\"] &lt;= saccade_event[\"end\"]),\n        \"eye1_saccade\",\n    ] = 1\n\n    if saccade_event[\"contains_blink\"] == 1: #Note here some version is \"blink\", depends on the item name\n        recording.loc[\n            (recording[\"timestamp\"] &gt;= saccade_event[\"start\"])\n            &amp; (recording[\"timestamp\"] &lt;= saccade_event[\"end\"]),\n            \"eye1_blink\",\n        ] = 1\n</code></pre>"},{"location":"data-management/2_et1_edf_to_bids/#6-write-the-data-into-bids-structure","title":"6 Write the data into BIDS structure","text":"<p><pre><code>from copy import deepcopy\n\nmetadata['Columns'] = recording.columns.tolist()\nprint(metadata)\nsave_metadata = deepcopy(metadata)\n# metadata.pop('CalibrationLog', None)\n# print(metadata)\n</code></pre> We need to convert the 'CalibrationCount' into <code>int</code> type before the conversion.</p> <pre><code>def convert_to_int(metadata):\n    if 'CalibrationCount' in metadata:\n        metadata['CalibrationCount'] = int(metadata['CalibrationCount']) if isinstance(metadata['CalibrationCount'], (np.int32, np.int64, int)) else metadata['CalibrationCount']\n    if \"CalibrationLog\" in metadata:\n        metadata[\"CalibrationLog\"] = [(int(x[0]),x[1]) if isinstance(x[0], (np.int32, np.int64, int)) else x for x in metadata['CalibrationLog']]\n    return metadata\n\n\nconvert_metadata = convert_to_int(metadata)\n</code></pre> <p>Write the <code>dataframe</code> into <code>bids</code> <pre><code>out_dir = DATA_PATH\nedf_extension = 'EDF'\nedf_name = edf_name\nfilename = edf_name.split('.')[0]\nprint(f'bid filename: {filename}')\n\ndef write_bids_from_df(\n    recording, metadata,\n    out_dir,\n    filename,\n    # exp_run: str | Path,\n) -&gt; List[str]:\n    \"\"\"\n    Directly save the eye-tracking recording/metadata into a  BIDS structure.\n\n    Parameters\n    ----------\n    recording : dataframe\n        The recording data extracted from the EDF file.\n    metadata : dict\n        The metadata extracted from the EDF file.\n    out_dir : obj:`os.pathlike`\n        The path of EDF file. Refers to the folder (not the EDF file).\n    filename: str\n        The filename of the EDF file. The file name without the suffix, eg: \"Subject001\"\n\n    Returns\n    -------\n    List[str]\n        A list of generated files.\n\n    \"\"\"\n\n    out_json = out_dir / (filename + \".json\")\n    out_json.write_text(\n        json.dumps(metadata, sort_keys=True, indent=2)\n    )\n\n    # Write out data\n    out_tsvgz = out_dir / (filename + \".tsv.gz\")\n\n    recording.to_csv(\n        out_tsvgz,\n        sep=\"\\t\",\n        index=True,\n        header=True,\n        compression=\"gzip\",\n        na_rep=\"n/a\",\n    )\n\n    return str(out_tsvgz), str(out_json)\n\n\nwrite_bids_from_df(\n    recording, convert_metadata,\n    out_dir,\n    filename,\n)\n</code></pre></p> <p>Now the BIDS files are generated: EDF Path</p> <ul> <li>&lt;filename&gt;.json</li> <li>&lt;filename&gt;.tsv.gz</li> </ul>"},{"location":"data-management/3_data_storage/","title":"Data storage","text":"<p>To effectively manage and analyze MRI data, including prescan files, raw data, DICOM files, and associated physiological recordings and codes for analysis, we recommend organizing the collected dataset into the following structure:</p> <pre><code>MR-Eye/\n    \u251c\u2500\u2500 code/\n    \u2502   \u251c\u2500\u2500 git_repo_specific_for_this_project/\n    \u2502   \u2502   \u251c\u2500\u2500 README.md  # Add a detailed description of the repository here\n    \u2502   \u2502   \u251c\u2500\u2500 [Your code files for this specific project go here]\n    \u2502   \u2502   \u2514\u2500\u2500 .gitignore  # Include entries to exclude commonly-used packages like Monalisa\n    \u2502\n    \u251c\u2500\u2500 data/\n    \u2502   \u251c\u2500\u2500 update_protocol_description.md\n    \u2502   \u251c\u2500\u2500 final_fixed_protocol.md\n    \u2502   \u251c\u2500\u2500 subject_table.xlsx\n    \u2502   \u251c\u2500\u2500 pilot_/\n    \u2502   \u2502   \u251c\u2500\u2500 year-month-date/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 subject00x/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 [Relevant pilot subject data files]\n    \u2502   \u2502   \u2514\u2500\u2500 [More dated folders as needed]\n    \u2502   \u251c\u2500\u2500 final_protocol/\n    \u2502   \u2502   \u251c\u2500\u2500 subject001/\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 mri/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 dicom/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 raw_data/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ismrmd/\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 bids/  # (TBD: Decide on the structure)\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 physio/\n    \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 ET/\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **.edf\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 **.tsv.gz\n    \u2502   \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 **.json\n    \u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 twix/**.mat \n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 others/\n    \u2502   \u2502   \u2514\u2500\u2500 [Additional subjects as needed]\n    \u2502\n    \u251c\u2500\u2500 output/\n    \u2502   \u2514\u2500\u2500 [Generated output files go here]\n    \u2502\n    \u2514\u2500\u2500 README.md  # Main project description with an overview, usage, and other details\n</code></pre>"},{"location":"post-processing/1_et2_QAQC_of_eye-tracking_data/","title":"Eye tracking data","text":"<p>derived from: https://www.axonlab.org/hcph-sops/data-management/eyetrack-qc/</p> <p>Author: Yiwei Jia</p> <p>This notebook ROI determination, ET mask generating</p>"},{"location":"post-processing/2_MRI_post_processing/","title":"MRI data","text":"<ul> <li>DICOM from standard clinical protocols: spm12 -&gt;reslicing and registration revise the codes.</li> <li>raw data from MR-Eye or MR_Track protocols: binning with ET masks -&gt; Compressed sensing recon</li> </ul>"},{"location":"post-processing/2_MRI_post_processing/#1-reconstruction-from-raw-data","title":"1 Reconstruction from raw data","text":"<p>For the motion-resolved task, a binning mask must be manually generated, and image reconstruction is performed using the Monalisa toolkit, which supports non-Cartesian trajectories of the T1w/T2w-LIBRE protocol. The reconstruction results are typically saved in .mat format.</p> <p>Given that the matrix size of T1w/T2w-LIBRE is 480, the reconstruction process requires computational resources and should be executed on Debi or the HES-SO server.</p> <p>The practical steps for reconstructing the data are as follows:</p>"},{"location":"post-processing/2_MRI_post_processing/#prepare-raw-data-path-on-debi","title":"Prepare raw data path on Debi","text":"<p>To accelerate data processing, we can first prepare a list of all raw data paths required for the upcoming reconstruction. <pre><code>reconDir = '/path/to/recon/folder';\n\n% Subject 001\n/path/to/raw/data/Subject001/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n\n% Subject 002\n/path/to/raw/data/Subject002/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n\n% Subject 003\n/path/to/raw/data/Subject003/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n\n% Subject 004\n/path/to/raw/data/Subject004/RawData/\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_T2_eye.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_BC_BC.dat\nmeas_MIDxxxx_FIDxxxxx_BEAT_LIBREon_eye_HC_BC.dat\n</code></pre></p>"},{"location":"post-processing/2_MRI_post_processing/#load-all-the-raw-data-files","title":"Load all the raw data files","text":"<p>It is important to consider the number of channels (nCh), as it may vary across different subjects and acquisitions.  Additionally, make sure to record the duration of the raw data by examining the PMU, as this information will be crucial for the subsequent binning process. <pre><code>%Sub001\nN     = 480 \nnSeg  = 22 \nnShot = 1000 \nnLine = 22000 \nnPar  = 1 \nnCh   = 42 \nnEcho = 1 \nThe duration of the rawdata is: 650185 ms with data points:22000\n\n%Sub002\nN     = 480 \nnSeg  = 22 \nnShot = 1000 \nnLine = 22000 \nnPar  = 1 \nnCh   = 44 \nnEcho = 1 \nThe duration of the rawdata is: 650185 ms with data points:22000\n</code></pre></p>"},{"location":"post-processing/2_MRI_post_processing/#generate-et-masks","title":"Generate ET masks","text":"<ul> <li>Synchronize the eye-tracking (ET) data with the MRI readouts using the trigger information. </li> <li> <p>Once the criteria for selecting the ET data have been established, we will generate an ET mask that matches the duration of the MRI readout.   For instance, if the raw data duration is 650,185 ms, as determined in a previous step, and the ET sampling rate is 1 kHz, an ET mask with a length of 650,185 samples can be generated after identifying the start timestamp of the ET data.</p> </li> <li> <p>Save the generated ET mask in .mat format.</p> </li> </ul> <p>Further details on generating the ET mask and saving it will be provided in Section ref()."},{"location":"post-processing/2_MRI_post_processing/#binning-the-raw-data-according-to-et-masks","title":"Binning the raw data according to ET masks.","text":"<ul> <li>For the raw data, we first eliminate the SI projection of raw data by setting the corresponding mask elements to a value of 0. </li> <li>Then we will bin the data according to the ET mask. <ul> <li>Given that the repetition time (TR) is 8.01 ms for T1w-LIBRE and 29.14 ms for T2w-LIBRE, each MRI readout corresponds to approximately 8 ET points for T1w and 29\u201330 ET points for T2w. </li> <li>Based on this, we apply a sliding window with a length of roughly three times the TR: 30 points for T1w and 90 points for T2w.</li> <li>Within each window, we calculate the number of valid ET mask points (where the value = 1, indicating that the gaze point falls within the selected central range of the screen). If the number of valid points exceeds the user-defined threshold\u2014set as 3/4 of the window length in our case\u2014the corresponding MRI readout is preserved; otherwise, it is excluded.</li> </ul> </li> <li>Finally, we generate a binned mask that selects only the readouts unaffected by significant eye movements.</li> <li>Record the number of readouts remaining after binning for each subject and each derivative method. For example:</li> </ul> <p><pre><code>Subject 001 with Binning, preserved #line: 17314 out of 22000\n\nSubject 002 with Binning, preserved #line: 9573 out of 22000\n\nSubject 003 with Binning, preserved #line: 15322 out of 22000\n\nSubject 004 with Binning, preserved #line: 8835 out of 22000\n</code></pre> If the preserved number of readouts are less than 10k, we will expect a low quality of reconstructed images.</p>"},{"location":"post-processing/2_MRI_post_processing/#create-mitosius","title":"Create Mitosius","text":"<p>If you are working on the HES-SO server, the dataset can be directly mounted from the data storage server, making it most efficient to run Mitosius directly on the server.</p> <p>If you are using an HPC and need to transfer data, it is recommended to create the Mitosius folder locally first and then upload the folder to the HPC instead of transferring the raw data.</p> <p>The scripts about how to use mitosius please refer to the Monalisa document.</p>"},{"location":"post-processing/2_MRI_post_processing/#recon-function","title":"Recon function","text":"<p>In our reconstruction, we utilize <code>Steva</code> function for Single-frame Least-square Regularized Reconstruction, where reularizaiton is the l1-norm of spatial gradient of the image. The reconstructed results are in the format of <code>**.mat</code>. You can find more details in the Monalisa document.</p>"},{"location":"post-processing/2_MRI_post_processing/#2-pipeline-for-processing-dicom","title":"2 Pipeline for processing Dicom","text":"<p>Whereas, the recon results from standard clinical protocols are in DICOM format, directly exported from scanner PC.</p> <ul> <li>Convert Dicom to .nii files: download the MRIcroGL software.</li> <li>Download and install the software.</li> <li>prepare the nifti folder under the Dicom folder.    <code>MRICron.exe &gt;&gt; import &gt;&gt; Dcm2nii &gt;&gt; uncompressed NifTi (.nii)</code></li> <li>Inspect the .nii files: </li> <li>Prepare spm software on the laptop.</li> <li>Open spm -&gt; display buttons</li> <li>Coregister: Estimate &amp; Reslice.   Coregister the DICOM results from the standard clinical protocol with those from LIBRE. Although the DICOM result from LIBRE will not be used for further analysis, it serves as a reference image, allowing us to transform the clinical protocol image to the same spatial domain as LIBRE for subsequent comparison.</li> </ul> <pre><code>addpath('/Users/cag/Documents/forclone/spm')\n%%\n\n% Define image paths\n\nnii_folder = '/path/to/NIFTI_NII_origin/';\n% Libre image\na_image_name = 'DICOM_BEAT_LIBREon_eye.nii';\n% Vibe image\nb_image_name = 'DICOM_t1_vibe.nii'; \n\na_image = fullfile(nii_folder, a_image_name);\nb_image = fullfile(nii_folder, b_image_name);\n\n%% Inspect the header before co-registration\nV_a_1 = spm_vol(a_image);\ndisp('V_a_1'); disp(V_a_1);\nV_b_1 = spm_vol(b_image);\ndisp('V_b_1'); disp(V_b_1);\n\n% Co_register\nco_register(b_image, a_image)\n\n% Inspect the header after co-registration\nV_a_2 = spm_vol(a_image);\ndisp('V_a_2'); disp(V_a_2);\nV_b_2 = spm_vol(b_image);\ndisp('V_b_2'); disp(V_b_2)\n\ndisp(['V_b remains the same? ', num2str(isequal(V_b_1.mat, V_b_2.mat))])\n%%\n%---------------------------------------------\n% Reslicing part\n%---------------------------------------------\nreslicing(b_image, a_image)\n</code></pre>"},{"location":"recruitment-scheduling-screening/participants-recruitment/","title":"Recruitment and screening","text":""},{"location":"recruitment-scheduling-screening/participants-recruitment/#recruitment-shortlist","title":"Recruitment shortlist","text":"<p>The recruitment was done speaking with people within the CHUV UNIL and using a recruitment announcement on EPFL site (https://myjob.epfl.ch).</p>"},{"location":"recruitment-scheduling-screening/participants-recruitment/#first-contact","title":"First contact","text":"<ul> <li>Write an email to them within the next 24 hours attaching the MRI Safety and Screening Questionnaire and the Informed Consent Form.</li> <li>Confirm the receipt of the email AND the documents.</li> <li>If the inclusion criteria for performing an MRI are met, a day is scheduled for scanning. An in-person meeting 20 minutes before scanning is required to discuss the documentation presented.</li> </ul>"},{"location":"recruitment-scheduling-screening/participants-recruitment/#in-person-meeting","title":"In-person meeting","text":"<ul> <li>Confirm whether the potential participant understood the MRI Safety &amp; Screening Questionnaire and discuss with them any questions or potential reasons that may disqualify them to participate.</li> <li>Female participants will be informed and must acknowledge that they must take a pregnancy test before the first scanning session.</li> <li>Make sure that the participant's questions about the study are all addressed and answered.</li> <li>If the candidate participant does not meet our inclusion criteria or is no longer available to participate in the experiment after having thoroughly discussed the experiment, the participation will be withdrawn.</li> <li>If participants are eligible and confirm they are willing to continue, they are asked to sign the informed consent.</li> </ul>"},{"location":"recruitment-scheduling-screening/scanner-scheduling/","title":"Scheduling","text":"<p>The acquisition day was defined based on the availability of the participant and the availability of the PrismaFit system. The PrismaFit system is then booked following these steps:</p> <ol> <li>Open the scheduling system on a browser.</li> <li>Click on the preferred slot.</li> <li>Select the adequate length for the session.</li> <li>Select Research on healthy subjects in the Type of Scan box.</li> <li>Select true in Technician Required if you are not a certified operator of the system.</li> </ol>"}]}